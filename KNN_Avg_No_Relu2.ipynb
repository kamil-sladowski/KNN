{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KNN_Avg_No_Relu2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kamilo116/KNN/blob/master/KNN_Avg_No_Relu2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7ReM7FU-xs4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import os\n",
        "import csv\n",
        "import sys\n",
        "from random import shuffle\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from matplotlib.pyplot import imshow\n",
        "%matplotlib inline\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import sampler\n",
        "import torchvision\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms as transforms\n",
        "import timeit\n",
        "\n",
        "np.random.seed(4) \n",
        "torch.manual_seed(4) \n",
        "torch.cuda.manual_seed(4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYIMZ8QyYewD",
        "colab_type": "code",
        "outputId": "aa2f54bb-dab5-4bea-eca0-51e776f00575",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "! git clone https://github.com/wang-chen/kervolution.git \n",
        "\n",
        "sys.path.append(\"kervolution/\")\n",
        "from kervolution import Kerv2d\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'kervolution'...\n",
            "remote: Enumerating objects: 6, done.\u001b[K\n",
            "remote: Counting objects:  16% (1/6)\u001b[K\rremote: Counting objects:  33% (2/6)\u001b[K\rremote: Counting objects:  50% (3/6)\u001b[K\rremote: Counting objects:  66% (4/6)\u001b[K\rremote: Counting objects:  83% (5/6)\u001b[K\rremote: Counting objects: 100% (6/6)\u001b[K\rremote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects:  16% (1/6)\u001b[K\rremote: Compressing objects:  33% (2/6)\u001b[K\rremote: Compressing objects:  50% (3/6)\u001b[K\rremote: Compressing objects:  66% (4/6)\u001b[K\rremote: Compressing objects:  83% (5/6)\u001b[K\rremote: Compressing objects: 100% (6/6)\u001b[K\rremote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 53 (delta 2), reused 0 (delta 0), pack-reused 47\u001b[K\n",
            "Unpacking objects:   1% (1/53)   \rUnpacking objects:   3% (2/53)   \rUnpacking objects:   5% (3/53)   \rUnpacking objects:   7% (4/53)   \rUnpacking objects:   9% (5/53)   \rUnpacking objects:  11% (6/53)   \rUnpacking objects:  13% (7/53)   \rUnpacking objects:  15% (8/53)   \rUnpacking objects:  16% (9/53)   \rUnpacking objects:  18% (10/53)   \rUnpacking objects:  20% (11/53)   \rUnpacking objects:  22% (12/53)   \rUnpacking objects:  24% (13/53)   \rUnpacking objects:  26% (14/53)   \rUnpacking objects:  28% (15/53)   \rUnpacking objects:  30% (16/53)   \rUnpacking objects:  32% (17/53)   \rUnpacking objects:  33% (18/53)   \rUnpacking objects:  35% (19/53)   \rUnpacking objects:  37% (20/53)   \rUnpacking objects:  39% (21/53)   \rUnpacking objects:  41% (22/53)   \rUnpacking objects:  43% (23/53)   \rUnpacking objects:  45% (24/53)   \rUnpacking objects:  47% (25/53)   \rUnpacking objects:  49% (26/53)   \rUnpacking objects:  50% (27/53)   \rUnpacking objects:  52% (28/53)   \rUnpacking objects:  54% (29/53)   \rUnpacking objects:  56% (30/53)   \rUnpacking objects:  58% (31/53)   \rUnpacking objects:  60% (32/53)   \rUnpacking objects:  62% (33/53)   \rUnpacking objects:  64% (34/53)   \rUnpacking objects:  66% (35/53)   \rUnpacking objects:  67% (36/53)   \rUnpacking objects:  69% (37/53)   \rUnpacking objects:  71% (38/53)   \rUnpacking objects:  73% (39/53)   \rUnpacking objects:  75% (40/53)   \rUnpacking objects:  77% (41/53)   \rUnpacking objects:  79% (42/53)   \rUnpacking objects:  81% (43/53)   \rUnpacking objects:  83% (44/53)   \rUnpacking objects:  84% (45/53)   \rUnpacking objects:  86% (46/53)   \rUnpacking objects:  88% (47/53)   \rUnpacking objects:  90% (48/53)   \rUnpacking objects:  92% (49/53)   \rUnpacking objects:  94% (50/53)   \rUnpacking objects:  96% (51/53)   \rUnpacking objects:  98% (52/53)   \rUnpacking objects: 100% (53/53)   \rUnpacking objects: 100% (53/53), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvbKg4VJXKY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LIMIT_IMAGES_NUM = 500\n",
        "train_test_split_size = 0.2\n",
        "image_resize = (100, 100)\n",
        "batch_size = 64\n",
        "num_workers = 0\n",
        "\n",
        "in_channels = 3\n",
        "out_channels_1 = 16\n",
        "out_channels_2 = 32\n",
        "out_channels_3 = 64\n",
        "out_channels_4 = 128\n",
        "\n",
        "kernel_size = 3\n",
        "padding_1 = 1\n",
        "num_epochs = 80\n",
        "\n",
        "learning_rate = 0.0002\n",
        "weight_decay = 0.1\n",
        "\n",
        "MALIGNANT_DATASET = '/content/drive/My Drive/Colab_data/malignant/malignant/'\n",
        "BENIGN_DATASET = '/content/drive/My Drive/Colab_data/benign/benign/'\n",
        "DATA_FOLDER = '/content/drive/My Drive/Colab_data/'\n",
        "model_backup_path = os.path.join(DATA_FOLDER, 'backup_model') \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMOHez_kHZD6",
        "colab_type": "code",
        "outputId": "ef262c5c-beed-44ab-e3e2-8144e4a7b3a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "from google.colab import drive, files\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b8P1NdXfVdX",
        "colab_type": "code",
        "outputId": "8cf58b13-13db-40eb-a55b-99310aa90590",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "benign_file_list = sorted(os.listdir(BENIGN_DATASET))\n",
        "malignant_file_list = sorted(os.listdir(MALIGNANT_DATASET))\n",
        "shuffle(benign_file_list)\n",
        "shuffle(malignant_file_list)\n",
        "benign_file_list = benign_file_list[:LIMIT_IMAGES_NUM]\n",
        "malignant_file_list = malignant_file_list[:LIMIT_IMAGES_NUM]\n",
        "\n",
        "print(f\"Number of benign {len(benign_file_list)} images\")\n",
        "print(f\"Number of malignant {len(malignant_file_list)} images\")\n",
        "\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize(image_resize),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of benign 500 images\n",
            "Number of malignant 500 images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGDQfal74BLi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "benign_dict = {filename: 0 for filename in benign_file_list}\n",
        "malignant_dict = {filename: 1 for filename in malignant_file_list}\n",
        "img_class_dict = {**benign_dict , **malignant_dict}\n",
        "labeled_data = pd.Series(img_class_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvAAOqmVfVll",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class IsicDataset(Dataset):\n",
        "    def __init__(self, data_folder, labeled_data, \n",
        "                 transform=transforms.Compose([transforms.ToTensor()])):\n",
        "        self.labeled_data = labeled_data\n",
        "        self.transform = transform\n",
        "        self.data_folder = data_folder\n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.labeled_data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        label = self.labeled_data[index]\n",
        "        if label == 0:\n",
        "          image = Image.open(os.path.join(self.data_folder, \"benign\", \"benign\", index ))\n",
        "        else:\n",
        "          image = Image.open(os.path.join(self.data_folder, \"malignant\", \"malignant\", index ))\n",
        "        image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "    @property\n",
        "    def labels(self):\n",
        "      return self.labeled_data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSuYqjLAYrWA",
        "colab_type": "code",
        "outputId": "e04eb763-a59b-4121-8cef-1de6367d9a7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        }
      },
      "source": [
        " \n",
        "dataset = IsicDataset(DATA_FOLDER, labeled_data, transform=data_transforms)\n",
        "print(dataset.labels)\n",
        "\n",
        "X_train, X_test = train_test_split(dataset.labels, test_size=train_test_split_size)\n",
        "print(\"number of training data: \",len(X_train))\n",
        "print(\"number of testing  data: \",len(X_test))\n",
        "\n",
        "train_sampler = SubsetRandomSampler(list(X_train.index))\n",
        "valid_sampler = SubsetRandomSampler(list(X_test.index))\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers)\n",
        "valid_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler, num_workers=num_workers)\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ISIC_0000577.jpeg    0\n",
            "ISIC_0002846.jpeg    0\n",
            "ISIC_0000088.jpeg    0\n",
            "ISIC_0001319.jpeg    0\n",
            "ISIC_0000725.jpeg    0\n",
            "                    ..\n",
            "ISIC_0010245.jpeg    1\n",
            "ISIC_0011743.jpeg    1\n",
            "ISIC_0024865.jpg     1\n",
            "ISIC_0000170.jpeg    1\n",
            "ISIC_0000516.jpeg    1\n",
            "Length: 1000, dtype: int64\n",
            "number of training data:  800\n",
            "number of testing  data:  200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_e8nFBR6Yug5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "avg_loss_list = []\n",
        "acc_list = []\n",
        "\n",
        "def train(model, train_loader ,loss_fn, optimizer, num_epochs=1, starting_from_epoch=1):\n",
        "    total_loss =0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Starting epoch %d / %d' % (epoch + 1, num_epochs))\n",
        "        model.train()\n",
        "\n",
        "        for t, (x, y) in enumerate(train_loader):\n",
        "            x_var = Variable(x.type(gpu_dtype))\n",
        "            y_var = Variable(y.type(gpu_dtype).long())\n",
        "            scores = model(x_var)\n",
        "            loss = loss_fn(scores, y_var)\n",
        "            total_loss += loss.data\n",
        "            \n",
        "            if (t + 1) % print_every == 0:\n",
        "                avg_loss = total_loss/print_every\n",
        "                print('t = %d, avg_loss = %.4f' % (t + 1, avg_loss) )\n",
        "                avg_loss_list.append(avg_loss)\n",
        "                total_loss = 0\n",
        "                \n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        acc = check_accuracy(model_gpu, valid_loader)\n",
        "        print('acc = %f' %(acc))\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model_gpu.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            }, model_backup_path)\n",
        "            \n",
        "def check_accuracy(model, loader):\n",
        "    print('Checking accuracy on test set')   \n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval() \n",
        "    for x, y in loader:\n",
        "        x_var = Variable(x.type(gpu_dtype))\n",
        "\n",
        "        scores = model(x_var)\n",
        "        _, preds = scores.data.cpu().max(1)\n",
        "        num_correct += (preds == y).sum()\n",
        "        num_samples += preds.size(0)\n",
        "    acc = float(num_correct) / num_samples\n",
        "    acc_list.append(acc)\n",
        "    print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
        "    return acc\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggvpMu36Yw2D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Flatten(nn.Module):\n",
        "    def forward(self, x):\n",
        "        N, C, H, W = x.size()\n",
        "        return x.view(N, -1)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgmexhBRZAK6",
        "colab_type": "code",
        "outputId": "712c768a-cd97-44fc-84b0-b6f542a500f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "gpu_dtype = torch.cuda.FloatTensor\n",
        "\n",
        "print_every = 1\n",
        "\n",
        "\n",
        "'''\n",
        "Kerv2d\n",
        "kervolution with following options:\n",
        "kernel_type: [linear, polynomial, gaussian, etc.]\n",
        "default is convolution:\n",
        "          kernel_type --> linear,\n",
        "balance, power, gamma is valid only when the kernel_type is specified\n",
        "if learnable_kernel = True,  they just be the initial value of learable parameters\n",
        "if learnable_kernel = False, they are the value of kernel_type's parameter\n",
        "the parameter [power] cannot be learned due to integer limitation\n",
        "dilation (int or tuple, optional): Spacing between kernel\n",
        "elements. Default: 1\n",
        "groups (int, optional): Number of blocked connections from input\n",
        "channels to output channels. Default: 1\n",
        "bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``\n",
        "kernel_type (str), Default: 'linear'\n",
        "learnable_kernel (bool): Learnable kernel parameters.  Default: False \n",
        "balance: 0, 1\n",
        "power: 3, 4, 5\n",
        "gamma:\n",
        "'''\n",
        "\n",
        "\n",
        "model_base = nn.Sequential( \n",
        "                nn.Kerv2d(in_channels , out_channels_1, padding=padding_1, dilation=1, groups=1, bias=True, \n",
        "                          kernel_type='polynomial', kernel_size=kernel_size, learnable_kernel=True,\n",
        "                          kernel_regularizer=True, stride=1, balance=1, power=3, gamma=1\n",
        "                          ),\n",
        "                nn.BatchNorm2d(out_channels_1),\n",
        "                nn.AvgPool2d(2, stride=2),\n",
        "                nn.Conv2d(out_channels_1 , out_channels_2, padding= padding_1, kernel_size=kernel_size, stride=1), \n",
        "                nn.BatchNorm2d(out_channels_2),\n",
        "                nn.AvgPool2d(2, stride=2),\n",
        "                nn.Conv2d(out_channels_2 , out_channels_3, padding= padding_1, kernel_size=kernel_size, stride=1),  \n",
        "                nn.BatchNorm2d(out_channels_3),\n",
        "                nn.AvgPool2d(2, stride=2),\n",
        "                nn.Conv2d(out_channels_3 , out_channels_4, padding= padding_1, kernel_size=kernel_size, stride=1),  \n",
        "                nn.BatchNorm2d(out_channels_4),\n",
        "                nn.AvgPool2d(2, stride=2),\n",
        "                nn.Dropout(0.5),\n",
        "                Flatten(),\n",
        "                nn.Linear(4608,64),\n",
        "                nn.Linear(64,2)\n",
        "            )\n",
        "model_gpu = model_base.type(gpu_dtype)\n",
        "print(model_gpu)\n",
        "loss_fn = nn.modules.loss.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_gpu.parameters(), lr = learning_rate, weight_decay=weight_decay) \n",
        "\n",
        "train(model_gpu, train_loader ,loss_fn, optimizer, num_epochs=num_epochs)\n",
        "check_accuracy(model_gpu, valid_loader)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Kerv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "  (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "  (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (8): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "  (9): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (11): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "  (12): Dropout(p=0.5, inplace=False)\n",
            "  (13): Flatten()\n",
            "  (14): Linear(in_features=4608, out_features=64, bias=True)\n",
            "  (15): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "Starting epoch 1 / 80\n",
            "t = 1, avg_loss = 0.7391\n",
            "t = 2, avg_loss = 0.7166\n",
            "t = 3, avg_loss = 0.8938\n",
            "t = 4, avg_loss = 0.9955\n",
            "t = 5, avg_loss = 0.6911\n",
            "t = 6, avg_loss = 0.9601\n",
            "t = 7, avg_loss = 0.8560\n",
            "t = 8, avg_loss = 0.9632\n",
            "t = 9, avg_loss = 0.7972\n",
            "t = 10, avg_loss = 0.8297\n",
            "t = 11, avg_loss = 0.7811\n",
            "t = 12, avg_loss = 0.7932\n",
            "t = 13, avg_loss = 0.5379\n",
            "Checking accuracy on test set\n",
            "Got 100 / 200 correct (50.00)\n",
            "acc = 0.500000\n",
            "Starting epoch 2 / 80\n",
            "t = 1, avg_loss = 0.6904\n",
            "t = 2, avg_loss = 0.6394\n",
            "t = 3, avg_loss = 0.6416\n",
            "t = 4, avg_loss = 0.7550\n",
            "t = 5, avg_loss = 0.6957\n",
            "t = 6, avg_loss = 0.7540\n",
            "t = 7, avg_loss = 0.7513\n",
            "t = 8, avg_loss = 0.9043\n",
            "t = 9, avg_loss = 0.7226\n",
            "t = 10, avg_loss = 0.7344\n",
            "t = 11, avg_loss = 0.7565\n",
            "t = 12, avg_loss = 0.6521\n",
            "t = 13, avg_loss = 0.5583\n",
            "Checking accuracy on test set\n",
            "Got 101 / 200 correct (50.50)\n",
            "acc = 0.505000\n",
            "Starting epoch 3 / 80\n",
            "t = 1, avg_loss = 0.6833\n",
            "t = 2, avg_loss = 0.7353\n",
            "t = 3, avg_loss = 0.7649\n",
            "t = 4, avg_loss = 0.6075\n",
            "t = 5, avg_loss = 0.8663\n",
            "t = 6, avg_loss = 0.8033\n",
            "t = 7, avg_loss = 0.5813\n",
            "t = 8, avg_loss = 0.7976\n",
            "t = 9, avg_loss = 0.6200\n",
            "t = 10, avg_loss = 0.5454\n",
            "t = 11, avg_loss = 0.6167\n",
            "t = 12, avg_loss = 0.7038\n",
            "t = 13, avg_loss = 0.7206\n",
            "Checking accuracy on test set\n",
            "Got 128 / 200 correct (64.00)\n",
            "acc = 0.640000\n",
            "Starting epoch 4 / 80\n",
            "t = 1, avg_loss = 0.7469\n",
            "t = 2, avg_loss = 0.6456\n",
            "t = 3, avg_loss = 0.7473\n",
            "t = 4, avg_loss = 0.6290\n",
            "t = 5, avg_loss = 0.5465\n",
            "t = 6, avg_loss = 0.5611\n",
            "t = 7, avg_loss = 0.6542\n",
            "t = 8, avg_loss = 0.6012\n",
            "t = 9, avg_loss = 0.6464\n",
            "t = 10, avg_loss = 0.6310\n",
            "t = 11, avg_loss = 0.6423\n",
            "t = 12, avg_loss = 0.6549\n",
            "t = 13, avg_loss = 0.5305\n",
            "Checking accuracy on test set\n",
            "Got 129 / 200 correct (64.50)\n",
            "acc = 0.645000\n",
            "Starting epoch 5 / 80\n",
            "t = 1, avg_loss = 0.6516\n",
            "t = 2, avg_loss = 0.5357\n",
            "t = 3, avg_loss = 0.5686\n",
            "t = 4, avg_loss = 0.7802\n",
            "t = 5, avg_loss = 0.7570\n",
            "t = 6, avg_loss = 0.6444\n",
            "t = 7, avg_loss = 0.6386\n",
            "t = 8, avg_loss = 0.7335\n",
            "t = 9, avg_loss = 0.7264\n",
            "t = 10, avg_loss = 0.6338\n",
            "t = 11, avg_loss = 0.6073\n",
            "t = 12, avg_loss = 0.6769\n",
            "t = 13, avg_loss = 0.5985\n",
            "Checking accuracy on test set\n",
            "Got 127 / 200 correct (63.50)\n",
            "acc = 0.635000\n",
            "Starting epoch 6 / 80\n",
            "t = 1, avg_loss = 0.5475\n",
            "t = 2, avg_loss = 0.5904\n",
            "t = 3, avg_loss = 0.4820\n",
            "t = 4, avg_loss = 0.7940\n",
            "t = 5, avg_loss = 0.6987\n",
            "t = 6, avg_loss = 0.5267\n",
            "t = 7, avg_loss = 0.5205\n",
            "t = 8, avg_loss = 0.5889\n",
            "t = 9, avg_loss = 0.6735\n",
            "t = 10, avg_loss = 0.6259\n",
            "t = 11, avg_loss = 0.5343\n",
            "t = 12, avg_loss = 0.7125\n",
            "t = 13, avg_loss = 0.7017\n",
            "Checking accuracy on test set\n",
            "Got 146 / 200 correct (73.00)\n",
            "acc = 0.730000\n",
            "Starting epoch 7 / 80\n",
            "t = 1, avg_loss = 0.6523\n",
            "t = 2, avg_loss = 0.5635\n",
            "t = 3, avg_loss = 0.5248\n",
            "t = 4, avg_loss = 0.5078\n",
            "t = 5, avg_loss = 0.6484\n",
            "t = 6, avg_loss = 0.7033\n",
            "t = 7, avg_loss = 0.6582\n",
            "t = 8, avg_loss = 0.7439\n",
            "t = 9, avg_loss = 0.6386\n",
            "t = 10, avg_loss = 0.7354\n",
            "t = 11, avg_loss = 0.7212\n",
            "t = 12, avg_loss = 0.6309\n",
            "t = 13, avg_loss = 0.8082\n",
            "Checking accuracy on test set\n",
            "Got 143 / 200 correct (71.50)\n",
            "acc = 0.715000\n",
            "Starting epoch 8 / 80\n",
            "t = 1, avg_loss = 0.5816\n",
            "t = 2, avg_loss = 0.6386\n",
            "t = 3, avg_loss = 0.6835\n",
            "t = 4, avg_loss = 0.4620\n",
            "t = 5, avg_loss = 0.5093\n",
            "t = 6, avg_loss = 0.6590\n",
            "t = 7, avg_loss = 0.5982\n",
            "t = 8, avg_loss = 0.5410\n",
            "t = 9, avg_loss = 0.6879\n",
            "t = 10, avg_loss = 0.5546\n",
            "t = 11, avg_loss = 0.6340\n",
            "t = 12, avg_loss = 0.6562\n",
            "t = 13, avg_loss = 0.7582\n",
            "Checking accuracy on test set\n",
            "Got 129 / 200 correct (64.50)\n",
            "acc = 0.645000\n",
            "Starting epoch 9 / 80\n",
            "t = 1, avg_loss = 0.4951\n",
            "t = 2, avg_loss = 0.6048\n",
            "t = 3, avg_loss = 0.5169\n",
            "t = 4, avg_loss = 0.7248\n",
            "t = 5, avg_loss = 0.6365\n",
            "t = 6, avg_loss = 0.6030\n",
            "t = 7, avg_loss = 0.6459\n",
            "t = 8, avg_loss = 0.5749\n",
            "t = 9, avg_loss = 0.6235\n",
            "t = 10, avg_loss = 0.6676\n",
            "t = 11, avg_loss = 0.6641\n",
            "t = 12, avg_loss = 0.5318\n",
            "t = 13, avg_loss = 0.6435\n",
            "Checking accuracy on test set\n",
            "Got 135 / 200 correct (67.50)\n",
            "acc = 0.675000\n",
            "Starting epoch 10 / 80\n",
            "t = 1, avg_loss = 0.5872\n",
            "t = 2, avg_loss = 0.5041\n",
            "t = 3, avg_loss = 0.6183\n",
            "t = 4, avg_loss = 0.4789\n",
            "t = 5, avg_loss = 0.6997\n",
            "t = 6, avg_loss = 0.8319\n",
            "t = 7, avg_loss = 0.7061\n",
            "t = 8, avg_loss = 0.6340\n",
            "t = 9, avg_loss = 0.6479\n",
            "t = 10, avg_loss = 0.5588\n",
            "t = 11, avg_loss = 0.5699\n",
            "t = 12, avg_loss = 0.5955\n",
            "t = 13, avg_loss = 0.6178\n",
            "Checking accuracy on test set\n",
            "Got 149 / 200 correct (74.50)\n",
            "acc = 0.745000\n",
            "Starting epoch 11 / 80\n",
            "t = 1, avg_loss = 0.5727\n",
            "t = 2, avg_loss = 0.4645\n",
            "t = 3, avg_loss = 0.6201\n",
            "t = 4, avg_loss = 0.5154\n",
            "t = 5, avg_loss = 0.4738\n",
            "t = 6, avg_loss = 0.8905\n",
            "t = 7, avg_loss = 0.6476\n",
            "t = 8, avg_loss = 0.6797\n",
            "t = 9, avg_loss = 0.6526\n",
            "t = 10, avg_loss = 0.5669\n",
            "t = 11, avg_loss = 0.5814\n",
            "t = 12, avg_loss = 0.6754\n",
            "t = 13, avg_loss = 0.5752\n",
            "Checking accuracy on test set\n",
            "Got 125 / 200 correct (62.50)\n",
            "acc = 0.625000\n",
            "Starting epoch 12 / 80\n",
            "t = 1, avg_loss = 0.6874\n",
            "t = 2, avg_loss = 0.6611\n",
            "t = 3, avg_loss = 0.6279\n",
            "t = 4, avg_loss = 0.5838\n",
            "t = 5, avg_loss = 0.5852\n",
            "t = 6, avg_loss = 0.5318\n",
            "t = 7, avg_loss = 0.4979\n",
            "t = 8, avg_loss = 0.7684\n",
            "t = 9, avg_loss = 0.5236\n",
            "t = 10, avg_loss = 0.5579\n",
            "t = 11, avg_loss = 0.6792\n",
            "t = 12, avg_loss = 0.5472\n",
            "t = 13, avg_loss = 0.4400\n",
            "Checking accuracy on test set\n",
            "Got 127 / 200 correct (63.50)\n",
            "acc = 0.635000\n",
            "Starting epoch 13 / 80\n",
            "t = 1, avg_loss = 0.6553\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzuX5gLEeJfE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def retry_from_backup()\n",
        "  model_gpu = model_base.type(gpu_dtype)\n",
        "  print(model_gpu)\n",
        "  loss_fn = nn.modules.loss.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(model_gpu.parameters(), lr = learning_rate, weight_decay=weight_decay) \n",
        "\n",
        "\n",
        "\n",
        "  checkpoint = torch.load(model_backup_path)\n",
        "  model_gpu.load_state_dict(checkpoint['model_state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "  epoch = checkpoint['epoch']\n",
        "  loss = checkpoint['loss']\n",
        "  print(\"starting from epoch: \" + str(epoch))\n",
        "  print(\"starting from loss: \" + str(loss))\n",
        "  print(checkpoint['model_state_dict'])\n",
        "  print(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "  train(model_gpu, train_loader ,loss_fn, optimizer, num_epochs=num_epochs, starting_from_epoch=epoch)\n",
        "  check_accuracy(model_gpu, valid_loader)\n",
        "#retry_from_backup()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDn-2g-TLY3k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot([print_every*batch_size*(i+1) for i in range((len(avg_loss_list)))],avg_loss_list)\n",
        "print(\"Loss:\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGer6hJv7-zR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot([i+1 for i in range((len(acc_list)))],acc_list)\n",
        "print(\"Accurancy:\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBTReWHxz4Ml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%tensorboard --logdir {logs_base_dir}"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}