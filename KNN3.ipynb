{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KNN3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNt7K9wOG1piSeaT3mk8Gre",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kamilo116/KNN/blob/master/KNN3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7ReM7FU-xs4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import os\n",
        "import csv\n",
        "import sys\n",
        "from random import shuffle\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from matplotlib.pyplot import imshow\n",
        "%matplotlib inline\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import sampler\n",
        "import torchvision\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "import timeit\n",
        "\n",
        "np.random.seed(4) \n",
        "torch.manual_seed(4) \n",
        "torch.cuda.manual_seed(4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMOHez_kHZD6",
        "colab_type": "code",
        "outputId": "08cc25d0-3c45-4d30-d6a8-a325eb12c87d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive, files\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYIMZ8QyYewD",
        "colab_type": "code",
        "outputId": "328d655b-3975-4abc-c1e5-f158bff488e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "! git clone https://github.com/wang-chen/kervolution.git \n",
        "\n",
        "sys.path.append(\"kervolution/\")\n",
        "from kervolution import Kerv2d\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'kervolution' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b8P1NdXfVdX",
        "colab_type": "code",
        "outputId": "ab684240-244f-4d39-9954-876baa9b9403",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "MALIGNANT_DATASET = '/content/drive/My Drive/Colab_data/malignant/malignant/'\n",
        "BENIGN_DATASET = '/content/drive/My Drive/Colab_data/benign/benign/'\n",
        "DATA_FOLDER = '/content/drive/My Drive/Colab_data/'\n",
        "LIMIT_IMAGES_NUM = 600\n",
        "\n",
        "benign_file_list = sorted(os.listdir(BENIGN_DATASET))[:LIMIT_IMAGES_NUM]\n",
        "malignant_file_list = sorted(os.listdir(MALIGNANT_DATASET))[:LIMIT_IMAGES_NUM]\n",
        "shuffle(benign_file_list)\n",
        "shuffle(malignant_file_list)\n",
        "\n",
        "print(f\"Number of benign {len(benign_file_list)} images\")\n",
        "print(f\"Number of malignant {len(malignant_file_list)} images\")\n",
        "\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize((100, 100)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of benign 600 images\n",
            "Number of malignant 600 images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGDQfal74BLi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "benign_dict = {filename: 0 for filename in benign_file_list}\n",
        "malignant_dict = {filename: 1 for filename in malignant_file_list}\n",
        "img_class_dict = {**benign_dict , **malignant_dict}\n",
        "labeled_data = pd.Series(img_class_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvAAOqmVfVll",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class IsicDataset(Dataset):\n",
        "    def __init__(self, data_folder, labeled_data, \n",
        "                 transform=transforms.Compose([transforms.ToTensor()])):\n",
        "        self.labeled_data = labeled_data\n",
        "        self.transform = transform\n",
        "        self.data_folder = data_folder\n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.labeled_data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        label = self.labeled_data[index]\n",
        "        if label == 0:\n",
        "          image = Image.open(os.path.join(self.data_folder, \"benign\", \"benign\", index ))\n",
        "        else:\n",
        "          image = Image.open(os.path.join(self.data_folder, \"malignant\", \"malignant\", index ))\n",
        "        image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "    @property\n",
        "    def labels(self):\n",
        "      return self.labeled_data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSuYqjLAYrWA",
        "colab_type": "code",
        "outputId": "5404030c-cbc2-473a-f158-43b45c6295f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        " \n",
        "dataset = IsicDataset(DATA_FOLDER, labeled_data, transform=data_transforms)\n",
        "print(dataset.labels)\n",
        "\n",
        "X_train, X_test = train_test_split(dataset.labels, test_size=0.2)\n",
        "print(\"number of training data: \",len(X_train))\n",
        "print(\"number of testing  data: \",len(X_test))\n",
        "\n",
        "train_sampler = SubsetRandomSampler(list(X_train.index))\n",
        "valid_sampler = SubsetRandomSampler(list(X_test.index))\n",
        "batch_size = 32\n",
        "num_workers = 0\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers)\n",
        "valid_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler, num_workers=num_workers)\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ISIC_0000673.jpeg    0\n",
            "ISIC_0000684.jpeg    0\n",
            "ISIC_0000089.jpeg    0\n",
            "ISIC_0000232.jpeg    0\n",
            "ISIC_0000580.jpeg    0\n",
            "                    ..\n",
            "ISIC_0000550.jpeg    1\n",
            "ISIC_0010630.jpeg    1\n",
            "ISIC_0010883.jpeg    1\n",
            "ISIC_0010644.jpeg    1\n",
            "ISIC_0000002.jpeg    1\n",
            "Length: 1200, dtype: int64\n",
            "number of training data:  960\n",
            "number of testing  data:  240\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_e8nFBR6Yug5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "avg_loss_list = []\n",
        "acc_list = []\n",
        "\n",
        "def train(model, train_loader ,loss_fn, optimizer, num_epochs = 1):\n",
        "    total_loss =0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Starting epoch %d / %d' % (epoch + 1, num_epochs))\n",
        "        model.train()\n",
        "\n",
        "        for t, (x, y) in enumerate(train_loader):\n",
        "            x_var = Variable(x.type(gpu_dtype))\n",
        "            y_var = Variable(y.type(gpu_dtype).long())\n",
        "            scores = model(x_var)\n",
        "            loss = loss_fn(scores, y_var)\n",
        "            total_loss += loss.data\n",
        "            \n",
        "            if (t + 1) % print_every == 0:\n",
        "                avg_loss = total_loss/print_every\n",
        "                print('t = %d, avg_loss = %.4f' % (t + 1, avg_loss) )\n",
        "                avg_loss_list.append(avg_loss)\n",
        "                total_loss = 0\n",
        "                \n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        acc = check_accuracy(fixed_model_gpu, valid_loader)\n",
        "        print('acc = %f' %(acc))\n",
        "            \n",
        "def check_accuracy(model, loader):\n",
        "    print('Checking accuracy on test set')   \n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval() \n",
        "    for x, y in loader:\n",
        "        x_var = Variable(x.type(gpu_dtype))\n",
        "\n",
        "        scores = model(x_var)\n",
        "        _, preds = scores.data.cpu().max(1)\n",
        "        num_correct += (preds == y).sum()\n",
        "        num_samples += preds.size(0)\n",
        "    acc = float(num_correct) / num_samples\n",
        "    acc_list.append(acc)\n",
        "    print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
        "    return acc\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggvpMu36Yw2D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Flatten(nn.Module):\n",
        "    def forward(self, x):\n",
        "        N, C, H, W = x.size()\n",
        "        return x.view(N, -1)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgmexhBRZAK6",
        "colab_type": "code",
        "outputId": "bf6c4585-8d34-4f1d-8420-610c36c362ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "\n",
        "print_every = 1\n",
        "gpu_dtype = torch.cuda.FloatTensor\n",
        "\n",
        "out_0 = 90\n",
        "out_1 = 64\n",
        "out_2 = 48\n",
        "out_3 = 32\n",
        "out_4 = 24\n",
        "#out_5 = 16\n",
        "\n",
        "k_size_1 = 3\n",
        "padding_1 = 1\n",
        "in_channels = 3\n",
        "\n",
        "num_epochs = 50\n",
        "\n",
        "'''\n",
        "Kerv2d\n",
        "kervolution with following options:\n",
        "kernel_type: [linear, polynomial, gaussian, etc.]\n",
        "default is convolution:\n",
        "          kernel_type --> linear,\n",
        "balance, power, gamma is valid only when the kernel_type is specified\n",
        "if learnable_kernel = True,  they just be the initial value of learable parameters\n",
        "if learnable_kernel = False, they are the value of kernel_type's parameter\n",
        "the parameter [power] cannot be learned due to integer limitation\n",
        "balance: 0, 1\n",
        "power: 3, 4, 5\n",
        "gamma:\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "fixed_model_base = nn.Sequential( \n",
        "                nn.Kerv2d(in_channels , out_0, padding= padding_1, kernel_type='polynomial', kernel_size=k_size_1, learnable_kernel=True,\n",
        "                          kernel_regularizer=True, stride=1, balance=0, power=3, gamma=1), \n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.BatchNorm2d(out_0),\n",
        "                nn.Conv2d(out_0 , out_1, padding= padding_1, kernel_size=k_size_1, stride=1), \n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.BatchNorm2d(out_1),\n",
        "                nn.MaxPool2d(2, stride=2),\n",
        "                nn.Conv2d(out_1 , out_2, padding= padding_1, kernel_size=k_size_1, stride=1), \n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.BatchNorm2d(out_2),\n",
        "                nn.MaxPool2d(2, stride=2),\n",
        "                nn.Conv2d(out_2 , out_3, padding= padding_1, kernel_size=k_size_1, stride=1),  \n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.BatchNorm2d(out_3),\n",
        "                nn.MaxPool2d(2, stride=2),\n",
        "                nn.Conv2d(out_3 , out_4, padding= padding_1, kernel_size=k_size_1, stride=1), \n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.BatchNorm2d(out_4),\n",
        "                #nn.Conv2d(out_4 , out_5, padding= padding_1, kernel_size=k_size_1, stride=1), \n",
        "                #nn.ReLU(inplace=True),\n",
        "                #nn.BatchNorm2d(out_5),\n",
        "                nn.MaxPool2d(2, stride=2),\n",
        "                nn.Dropout(0.5),\n",
        "                Flatten(),\n",
        "                nn.Linear(864,64),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Linear(64,10),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Linear(10,2)\n",
        "            )\n",
        "fixed_model_gpu = fixed_model_base.type(gpu_dtype)\n",
        "print(fixed_model_gpu)\n",
        "loss_fn = nn.modules.loss.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(fixed_model_gpu.parameters(), lr = 0.00122, weight_decay=0.01) \n",
        "\n",
        "train(fixed_model_gpu, train_loader ,loss_fn, optimizer, num_epochs=num_epochs)\n",
        "check_accuracy(fixed_model_gpu, valid_loader)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR! Session/line number was not unique in database. History logging moved to new session 60\n",
            "Sequential(\n",
            "  (0): Kerv2d(3, 90, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (1): ReLU(inplace=True)\n",
            "  (2): BatchNorm2d(90, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (3): Conv2d(90, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (4): ReLU(inplace=True)\n",
            "  (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (7): Conv2d(64, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (8): ReLU(inplace=True)\n",
            "  (9): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (11): Conv2d(48, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (12): ReLU(inplace=True)\n",
            "  (13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (15): Conv2d(32, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (16): ReLU(inplace=True)\n",
            "  (17): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (19): Dropout(p=0.5, inplace=False)\n",
            "  (20): Flatten()\n",
            "  (21): Linear(in_features=864, out_features=64, bias=True)\n",
            "  (22): ReLU(inplace=True)\n",
            "  (23): Linear(in_features=64, out_features=10, bias=True)\n",
            "  (24): ReLU(inplace=True)\n",
            "  (25): Linear(in_features=10, out_features=2, bias=True)\n",
            ")\n",
            "Starting epoch 1 / 50\n",
            "t = 1, avg_loss = 0.6752\n",
            "t = 2, avg_loss = 0.6606\n",
            "t = 3, avg_loss = 0.7141\n",
            "t = 4, avg_loss = 0.6965\n",
            "t = 5, avg_loss = 0.6081\n",
            "t = 6, avg_loss = 0.7736\n",
            "t = 7, avg_loss = 0.7138\n",
            "t = 8, avg_loss = 0.6624\n",
            "t = 9, avg_loss = 0.6378\n",
            "t = 10, avg_loss = 0.6796\n",
            "t = 11, avg_loss = 0.6465\n",
            "t = 12, avg_loss = 0.6584\n",
            "t = 13, avg_loss = 0.7583\n",
            "t = 14, avg_loss = 0.6244\n",
            "t = 15, avg_loss = 0.6940\n",
            "t = 16, avg_loss = 0.6085\n",
            "t = 17, avg_loss = 0.6254\n",
            "t = 18, avg_loss = 0.7075\n",
            "t = 19, avg_loss = 0.7310\n",
            "t = 20, avg_loss = 0.6686\n",
            "t = 21, avg_loss = 0.6470\n",
            "t = 22, avg_loss = 0.6053\n",
            "t = 23, avg_loss = 0.6136\n",
            "t = 24, avg_loss = 0.6206\n",
            "t = 25, avg_loss = 0.5877\n",
            "t = 26, avg_loss = 0.6081\n",
            "t = 27, avg_loss = 0.5601\n",
            "t = 28, avg_loss = 0.5790\n",
            "t = 29, avg_loss = 0.6360\n",
            "t = 30, avg_loss = 0.6215\n",
            "Checking accuracy on test set\n",
            "Got 134 / 240 correct (55.83)\n",
            "acc = 0.558333\n",
            "Starting epoch 2 / 50\n",
            "t = 1, avg_loss = 0.5641\n",
            "t = 2, avg_loss = 0.5435\n",
            "t = 3, avg_loss = 0.5474\n",
            "t = 4, avg_loss = 0.6419\n",
            "t = 5, avg_loss = 0.6219\n",
            "t = 6, avg_loss = 0.5943\n",
            "t = 7, avg_loss = 0.6589\n",
            "t = 8, avg_loss = 0.5842\n",
            "t = 9, avg_loss = 0.6109\n",
            "t = 10, avg_loss = 0.5736\n",
            "t = 11, avg_loss = 0.5930\n",
            "t = 12, avg_loss = 0.5500\n",
            "t = 13, avg_loss = 0.5393\n",
            "t = 14, avg_loss = 0.5710\n",
            "t = 15, avg_loss = 0.7051\n",
            "t = 16, avg_loss = 0.5092\n",
            "t = 17, avg_loss = 0.6019\n",
            "t = 18, avg_loss = 0.4641\n",
            "t = 19, avg_loss = 0.5589\n",
            "t = 20, avg_loss = 0.4703\n",
            "t = 21, avg_loss = 0.5724\n",
            "t = 22, avg_loss = 0.5488\n",
            "t = 23, avg_loss = 0.4734\n",
            "t = 24, avg_loss = 0.4171\n",
            "t = 25, avg_loss = 0.4871\n",
            "t = 26, avg_loss = 0.4429\n",
            "t = 27, avg_loss = 0.4516\n",
            "t = 28, avg_loss = 0.5495\n",
            "t = 29, avg_loss = 0.5750\n",
            "t = 30, avg_loss = 0.5338\n",
            "Checking accuracy on test set\n",
            "t = 1, avg_loss = 0.4491\n",
            "t = 2, avg_loss = 0.4659\n",
            "t = 3, avg_loss = 0.6422\n",
            "t = 4, avg_loss = 0.5258\n",
            "t = 5, avg_loss = 0.3865\n",
            "t = 6, avg_loss = 0.4230\n",
            "t = 7, avg_loss = 0.5936\n",
            "t = 8, avg_loss = 0.4839\n",
            "t = 9, avg_loss = 0.5090\n",
            "t = 10, avg_loss = 0.5786\n",
            "t = 11, avg_loss = 0.4529\n",
            "t = 12, avg_loss = 0.5452\n",
            "t = 13, avg_loss = 0.8121\n",
            "t = 14, avg_loss = 0.5354\n",
            "t = 15, avg_loss = 0.8261\n",
            "t = 16, avg_loss = 0.5857\n",
            "t = 17, avg_loss = 0.4277\n",
            "t = 18, avg_loss = 0.6066\n",
            "t = 19, avg_loss = 0.6888\n",
            "t = 20, avg_loss = 0.5840\n",
            "t = 21, avg_loss = 0.5482\n",
            "t = 22, avg_loss = 0.5617\n",
            "t = 23, avg_loss = 0.5874\n",
            "t = 24, avg_loss = 0.5779\n",
            "t = 25, avg_loss = 0.4987\n",
            "t = 26, avg_loss = 0.7727\n",
            "t = 27, avg_loss = 0.5139\n",
            "t = 28, avg_loss = 0.7123\n",
            "t = 29, avg_loss = 0.5260\n",
            "t = 30, avg_loss = 0.4426\n",
            "Checking accuracy on test set\n",
            "Got 189 / 240 correct (78.75)\n",
            "acc = 0.787500\n",
            "Starting epoch 4 / 50\n",
            "t = 1, avg_loss = 0.5881\n",
            "t = 2, avg_loss = 0.7196\n",
            "t = 3, avg_loss = 0.4619\n",
            "t = 4, avg_loss = 0.4716\n",
            "t = 5, avg_loss = 0.3670\n",
            "t = 6, avg_loss = 0.5128\n",
            "t = 7, avg_loss = 0.5540\n",
            "t = 8, avg_loss = 0.7014\n",
            "t = 9, avg_loss = 0.5371\n",
            "t = 10, avg_loss = 0.4918\n",
            "t = 11, avg_loss = 0.5067\n",
            "t = 12, avg_loss = 0.5909\n",
            "t = 13, avg_loss = 0.4319\n",
            "t = 14, avg_loss = 0.4984\n",
            "t = 15, avg_loss = 0.5530\n",
            "t = 16, avg_loss = 0.5496\n",
            "t = 17, avg_loss = 0.5971\n",
            "t = 18, avg_loss = 0.4844\n",
            "t = 19, avg_loss = 0.4286\n",
            "t = 20, avg_loss = 0.5181\n",
            "t = 21, avg_loss = 0.4516\n",
            "t = 22, avg_loss = 0.5776\n",
            "t = 23, avg_loss = 0.6343\n",
            "t = 24, avg_loss = 0.4737\n",
            "t = 25, avg_loss = 0.5035\n",
            "t = 26, avg_loss = 0.5015\n",
            "t = 27, avg_loss = 0.4464\n",
            "t = 28, avg_loss = 0.4691\n",
            "t = 29, avg_loss = 0.4685\n",
            "t = 30, avg_loss = 0.6174\n",
            "Checking accuracy on test set\n",
            "Got 158 / 240 correct (65.83)\n",
            "acc = 0.658333\n",
            "Starting epoch 5 / 50\n",
            "t = 1, avg_loss = 0.4054\n",
            "t = 2, avg_loss = 0.4188\n",
            "t = 3, avg_loss = 0.5544\n",
            "t = 4, avg_loss = 0.5152\n",
            "t = 5, avg_loss = 0.4472\n",
            "t = 6, avg_loss = 0.3954\n",
            "t = 7, avg_loss = 0.5081\n",
            "t = 8, avg_loss = 0.5090\n",
            "t = 9, avg_loss = 0.4963\n",
            "t = 10, avg_loss = 0.4407\n",
            "t = 11, avg_loss = 0.4770\n",
            "t = 12, avg_loss = 0.4765\n",
            "t = 13, avg_loss = 0.3422\n",
            "t = 14, avg_loss = 0.5683\n",
            "t = 15, avg_loss = 0.4063\n",
            "t = 16, avg_loss = 0.4503\n",
            "t = 17, avg_loss = 0.3840\n",
            "t = 18, avg_loss = 0.5746\n",
            "t = 19, avg_loss = 0.4429\n",
            "t = 20, avg_loss = 0.5304\n",
            "t = 21, avg_loss = 0.4112\n",
            "t = 22, avg_loss = 0.3334\n",
            "t = 23, avg_loss = 0.5228\n",
            "t = 24, avg_loss = 0.4640\n",
            "t = 25, avg_loss = 0.5051\n",
            "t = 26, avg_loss = 0.5348\n",
            "t = 27, avg_loss = 0.5963\n",
            "t = 28, avg_loss = 0.4380\n",
            "t = 29, avg_loss = 0.5621\n",
            "t = 30, avg_loss = 0.5192\n",
            "Checking accuracy on test set\n",
            "Got 153 / 240 correct (63.75)\n",
            "acc = 0.637500\n",
            "Starting epoch 6 / 50\n",
            "t = 1, avg_loss = 0.4008\n",
            "t = 2, avg_loss = 0.3179\n",
            "t = 3, avg_loss = 0.3478\n",
            "t = 4, avg_loss = 0.2916\n",
            "t = 5, avg_loss = 0.4390\n",
            "t = 6, avg_loss = 0.5936\n",
            "t = 7, avg_loss = 0.4264\n",
            "t = 8, avg_loss = 0.4681\n",
            "t = 9, avg_loss = 0.4104\n",
            "t = 10, avg_loss = 0.4811\n",
            "t = 11, avg_loss = 0.3859\n",
            "t = 12, avg_loss = 0.5453\n",
            "t = 13, avg_loss = 0.3507\n",
            "t = 14, avg_loss = 0.3809\n",
            "t = 15, avg_loss = 0.4067\n",
            "t = 16, avg_loss = 0.3159\n",
            "t = 17, avg_loss = 0.4129\n",
            "t = 18, avg_loss = 0.4052\n",
            "t = 19, avg_loss = 0.5754\n",
            "t = 20, avg_loss = 0.5601\n",
            "t = 21, avg_loss = 0.6217\n",
            "t = 22, avg_loss = 0.5310\n",
            "t = 23, avg_loss = 0.5462\n",
            "t = 24, avg_loss = 0.4494\n",
            "t = 25, avg_loss = 0.3497\n",
            "t = 26, avg_loss = 0.7478\n",
            "t = 27, avg_loss = 0.4792\n",
            "t = 28, avg_loss = 0.4411\n",
            "t = 29, avg_loss = 0.4261\n",
            "t = 30, avg_loss = 0.6478\n",
            "Checking accuracy on test set\n",
            "Got 196 / 240 correct (81.67)\n",
            "acc = 0.816667\n",
            "Starting epoch 7 / 50\n",
            "t = 1, avg_loss = 0.4075\n",
            "t = 2, avg_loss = 0.5905\n",
            "t = 3, avg_loss = 0.4725\n",
            "t = 4, avg_loss = 0.4103\n",
            "t = 5, avg_loss = 0.4734\n",
            "t = 6, avg_loss = 0.3969\n",
            "t = 7, avg_loss = 0.4490\n",
            "t = 8, avg_loss = 0.6218\n",
            "t = 9, avg_loss = 0.5602\n",
            "t = 10, avg_loss = 0.4854\n",
            "t = 11, avg_loss = 0.5485\n",
            "t = 12, avg_loss = 0.5240\n",
            "t = 13, avg_loss = 0.5049\n",
            "t = 14, avg_loss = 0.3552\n",
            "t = 15, avg_loss = 0.2863\n",
            "t = 16, avg_loss = 0.4925\n",
            "t = 17, avg_loss = 0.3335\n",
            "t = 18, avg_loss = 0.3682\n",
            "t = 19, avg_loss = 0.4293\n",
            "t = 20, avg_loss = 0.4763\n",
            "t = 21, avg_loss = 0.4751\n",
            "t = 22, avg_loss = 0.5269\n",
            "t = 23, avg_loss = 0.4023\n",
            "t = 24, avg_loss = 0.7247\n",
            "t = 25, avg_loss = 0.6440\n",
            "t = 26, avg_loss = 0.3908\n",
            "t = 27, avg_loss = 0.4455\n",
            "t = 28, avg_loss = 0.3160\n",
            "t = 29, avg_loss = 0.3433\n",
            "t = 30, avg_loss = 0.4243\n",
            "Checking accuracy on test set\n",
            "Got 181 / 240 correct (75.42)\n",
            "acc = 0.754167\n",
            "Starting epoch 8 / 50\n",
            "t = 1, avg_loss = 0.2740\n",
            "t = 2, avg_loss = 0.4759\n",
            "t = 3, avg_loss = 0.3980\n",
            "t = 4, avg_loss = 0.4486\n",
            "t = 5, avg_loss = 0.5302\n",
            "t = 6, avg_loss = 0.4134\n",
            "t = 7, avg_loss = 0.2884\n",
            "t = 8, avg_loss = 0.5710\n",
            "t = 9, avg_loss = 0.3784\n",
            "t = 10, avg_loss = 0.4174\n",
            "t = 11, avg_loss = 0.7748\n",
            "t = 12, avg_loss = 0.4466\n",
            "t = 13, avg_loss = 0.3180\n",
            "t = 14, avg_loss = 0.4908\n",
            "t = 15, avg_loss = 0.3872\n",
            "t = 16, avg_loss = 0.3736\n",
            "t = 17, avg_loss = 0.3669\n",
            "t = 18, avg_loss = 0.3689\n",
            "t = 19, avg_loss = 0.5703\n",
            "t = 20, avg_loss = 0.5240\n",
            "t = 21, avg_loss = 0.4544\n",
            "t = 22, avg_loss = 0.3902\n",
            "t = 23, avg_loss = 0.4674\n",
            "t = 24, avg_loss = 0.3695\n",
            "t = 25, avg_loss = 0.3714\n",
            "t = 26, avg_loss = 0.4042\n",
            "t = 27, avg_loss = 0.4188\n",
            "t = 28, avg_loss = 0.3648\n",
            "t = 29, avg_loss = 0.3842\n",
            "t = 30, avg_loss = 0.4608\n",
            "Checking accuracy on test set\n",
            "Got 192 / 240 correct (80.00)\n",
            "acc = 0.800000\n",
            "Starting epoch 9 / 50\n",
            "t = 1, avg_loss = 0.4100\n",
            "t = 2, avg_loss = 0.2629\n",
            "t = 3, avg_loss = 0.3410\n",
            "t = 4, avg_loss = 0.3996\n",
            "t = 5, avg_loss = 0.4419\n",
            "t = 6, avg_loss = 0.3378\n",
            "t = 7, avg_loss = 0.4219\n",
            "t = 8, avg_loss = 0.3654\n",
            "t = 9, avg_loss = 0.3657\n",
            "t = 10, avg_loss = 0.2029\n",
            "t = 11, avg_loss = 0.2913\n",
            "t = 12, avg_loss = 0.6739\n",
            "t = 13, avg_loss = 0.3682\n",
            "t = 14, avg_loss = 0.5187\n",
            "t = 15, avg_loss = 0.2520\n",
            "t = 16, avg_loss = 0.3527\n",
            "t = 17, avg_loss = 0.4059\n",
            "t = 18, avg_loss = 0.2282\n",
            "t = 19, avg_loss = 0.3426\n",
            "t = 20, avg_loss = 0.2897\n",
            "t = 21, avg_loss = 0.5978\n",
            "t = 22, avg_loss = 0.5123\n",
            "t = 23, avg_loss = 0.3287\n",
            "t = 24, avg_loss = 0.6508\n",
            "t = 25, avg_loss = 0.4689\n",
            "t = 26, avg_loss = 0.3647\n",
            "t = 27, avg_loss = 0.7008\n",
            "t = 28, avg_loss = 0.5989\n",
            "t = 29, avg_loss = 0.4061\n",
            "t = 30, avg_loss = 0.2664\n",
            "Checking accuracy on test set\n",
            "Got 168 / 240 correct (70.00)\n",
            "acc = 0.700000\n",
            "Starting epoch 10 / 50\n",
            "t = 1, avg_loss = 0.3263\n",
            "t = 2, avg_loss = 0.4027\n",
            "t = 3, avg_loss = 0.4580\n",
            "t = 4, avg_loss = 0.3948\n",
            "t = 5, avg_loss = 0.3985\n",
            "t = 6, avg_loss = 0.4342\n",
            "t = 7, avg_loss = 0.4004\n",
            "t = 8, avg_loss = 0.4466\n",
            "t = 9, avg_loss = 0.4406\n",
            "t = 10, avg_loss = 0.4906\n",
            "t = 11, avg_loss = 0.3756\n",
            "t = 12, avg_loss = 0.3530\n",
            "t = 13, avg_loss = 0.4573\n",
            "t = 14, avg_loss = 0.4865\n",
            "t = 15, avg_loss = 0.3895\n",
            "t = 16, avg_loss = 0.4620\n",
            "t = 17, avg_loss = 0.3782\n",
            "t = 18, avg_loss = 0.4284\n",
            "t = 19, avg_loss = 0.3273\n",
            "t = 20, avg_loss = 0.3899\n",
            "t = 21, avg_loss = 0.4406\n",
            "t = 22, avg_loss = 0.3287\n",
            "t = 23, avg_loss = 0.3939\n",
            "t = 24, avg_loss = 0.4384\n",
            "t = 25, avg_loss = 0.4024\n",
            "t = 26, avg_loss = 0.4322\n",
            "t = 27, avg_loss = 0.3046\n",
            "t = 28, avg_loss = 0.4105\n",
            "t = 29, avg_loss = 0.4029\n",
            "t = 30, avg_loss = 0.3869\n",
            "Checking accuracy on test set\n",
            "Got 185 / 240 correct (77.08)\n",
            "acc = 0.770833\n",
            "Starting epoch 11 / 50\n",
            "t = 1, avg_loss = 0.2702\n",
            "t = 2, avg_loss = 0.3801\n",
            "t = 3, avg_loss = 0.2455\n",
            "t = 4, avg_loss = 0.4376\n",
            "t = 5, avg_loss = 0.4601\n",
            "t = 6, avg_loss = 0.2898\n",
            "t = 7, avg_loss = 0.3230\n",
            "t = 8, avg_loss = 0.3633\n",
            "t = 9, avg_loss = 0.6357\n",
            "t = 10, avg_loss = 0.4491\n",
            "t = 11, avg_loss = 0.3870\n",
            "t = 12, avg_loss = 0.5213\n",
            "t = 13, avg_loss = 0.3169\n",
            "t = 14, avg_loss = 0.3362\n",
            "t = 15, avg_loss = 0.3298\n",
            "t = 16, avg_loss = 0.3510\n",
            "t = 17, avg_loss = 0.4192\n",
            "t = 18, avg_loss = 0.6491\n",
            "t = 19, avg_loss = 0.4022\n",
            "t = 20, avg_loss = 0.3724\n",
            "t = 21, avg_loss = 0.4596\n",
            "t = 22, avg_loss = 0.3696\n",
            "t = 23, avg_loss = 0.4100\n",
            "t = 24, avg_loss = 0.3812\n",
            "t = 25, avg_loss = 0.3756\n",
            "t = 26, avg_loss = 0.6196\n",
            "t = 27, avg_loss = 0.4906\n",
            "t = 28, avg_loss = 0.3875\n",
            "t = 29, avg_loss = 0.3110\n",
            "t = 30, avg_loss = 0.2911\n",
            "Checking accuracy on test set\n",
            "Got 170 / 240 correct (70.83)\n",
            "acc = 0.708333\n",
            "Starting epoch 12 / 50\n",
            "t = 1, avg_loss = 0.3105\n",
            "t = 2, avg_loss = 0.3862\n",
            "t = 3, avg_loss = 0.6353\n",
            "t = 4, avg_loss = 0.5760\n",
            "t = 5, avg_loss = 0.3928\n",
            "t = 6, avg_loss = 0.6733\n",
            "t = 7, avg_loss = 0.4213\n",
            "t = 8, avg_loss = 0.3979\n",
            "t = 9, avg_loss = 0.3196\n",
            "t = 10, avg_loss = 0.3446\n",
            "t = 11, avg_loss = 0.4999\n",
            "t = 12, avg_loss = 0.3209\n",
            "t = 13, avg_loss = 0.4098\n",
            "t = 14, avg_loss = 0.3856\n",
            "t = 15, avg_loss = 0.3422\n",
            "t = 16, avg_loss = 0.4243\n",
            "t = 17, avg_loss = 0.3829\n",
            "t = 18, avg_loss = 0.3744\n",
            "t = 19, avg_loss = 0.3671\n",
            "t = 20, avg_loss = 0.3206\n",
            "t = 21, avg_loss = 0.4979\n",
            "t = 22, avg_loss = 0.3662\n",
            "t = 23, avg_loss = 0.4301\n",
            "t = 24, avg_loss = 0.4388\n",
            "t = 25, avg_loss = 0.3509\n",
            "t = 26, avg_loss = 0.2275\n",
            "t = 27, avg_loss = 0.4255\n",
            "t = 28, avg_loss = 0.3943\n",
            "t = 29, avg_loss = 0.5874\n",
            "t = 30, avg_loss = 0.3166\n",
            "Checking accuracy on test set\n",
            "Got 173 / 240 correct (72.08)\n",
            "acc = 0.720833\n",
            "Starting epoch 13 / 50\n",
            "t = 1, avg_loss = 0.2905\n",
            "t = 2, avg_loss = 0.3385\n",
            "t = 3, avg_loss = 0.3808\n",
            "t = 4, avg_loss = 0.3633\n",
            "t = 5, avg_loss = 0.4974\n",
            "t = 6, avg_loss = 0.4831\n",
            "t = 7, avg_loss = 0.5334\n",
            "t = 8, avg_loss = 0.3206\n",
            "t = 9, avg_loss = 0.3830\n",
            "t = 10, avg_loss = 0.3601\n",
            "t = 11, avg_loss = 0.8377\n",
            "t = 12, avg_loss = 0.3783\n",
            "t = 13, avg_loss = 0.3394\n",
            "t = 14, avg_loss = 0.6100\n",
            "t = 15, avg_loss = 0.4373\n",
            "t = 16, avg_loss = 0.3854\n",
            "t = 17, avg_loss = 0.3513\n",
            "t = 18, avg_loss = 0.3608\n",
            "t = 19, avg_loss = 0.4788\n",
            "t = 20, avg_loss = 0.5087\n",
            "t = 21, avg_loss = 0.5126\n",
            "t = 22, avg_loss = 0.4158\n",
            "t = 23, avg_loss = 0.4595\n",
            "t = 24, avg_loss = 0.3270\n",
            "t = 25, avg_loss = 0.4852\n",
            "t = 26, avg_loss = 0.3861\n",
            "t = 27, avg_loss = 0.5008\n",
            "t = 28, avg_loss = 0.3706\n",
            "t = 29, avg_loss = 0.2685\n",
            "t = 30, avg_loss = 0.3873\n",
            "Checking accuracy on test set\n",
            "Got 191 / 240 correct (79.58)\n",
            "acc = 0.795833\n",
            "Starting epoch 14 / 50\n",
            "t = 1, avg_loss = 0.2970\n",
            "t = 2, avg_loss = 0.3885\n",
            "t = 3, avg_loss = 0.2595\n",
            "t = 4, avg_loss = 0.3301\n",
            "t = 5, avg_loss = 0.3097\n",
            "t = 6, avg_loss = 0.3436\n",
            "t = 7, avg_loss = 0.3441\n",
            "t = 8, avg_loss = 0.2877\n",
            "t = 9, avg_loss = 0.4279\n",
            "t = 10, avg_loss = 0.3036\n",
            "t = 11, avg_loss = 0.4717\n",
            "t = 12, avg_loss = 0.5931\n",
            "t = 13, avg_loss = 0.4951\n",
            "t = 14, avg_loss = 0.3032\n",
            "t = 15, avg_loss = 0.3174\n",
            "t = 16, avg_loss = 0.2875\n",
            "t = 17, avg_loss = 0.4055\n",
            "t = 18, avg_loss = 0.3058\n",
            "t = 19, avg_loss = 0.3649\n",
            "t = 20, avg_loss = 0.3653\n",
            "t = 21, avg_loss = 0.3705\n",
            "t = 22, avg_loss = 0.3687\n",
            "t = 23, avg_loss = 0.3386\n",
            "t = 24, avg_loss = 0.3430\n",
            "t = 25, avg_loss = 0.4716\n",
            "t = 26, avg_loss = 0.6075\n",
            "t = 27, avg_loss = 0.3890\n",
            "t = 28, avg_loss = 0.5585\n",
            "t = 29, avg_loss = 0.2404\n",
            "t = 30, avg_loss = 0.3812\n",
            "Checking accuracy on test set\n",
            "Got 194 / 240 correct (80.83)\n",
            "acc = 0.808333\n",
            "Starting epoch 15 / 50\n",
            "t = 1, avg_loss = 0.5125\n",
            "t = 2, avg_loss = 0.2671\n",
            "t = 3, avg_loss = 0.4605\n",
            "t = 4, avg_loss = 0.2771\n",
            "t = 5, avg_loss = 0.4021\n",
            "t = 6, avg_loss = 0.3239\n",
            "t = 7, avg_loss = 0.3728\n",
            "t = 8, avg_loss = 0.2708\n",
            "t = 9, avg_loss = 0.2793\n",
            "t = 10, avg_loss = 0.3207\n",
            "t = 11, avg_loss = 0.4112\n",
            "t = 12, avg_loss = 0.3834\n",
            "t = 13, avg_loss = 0.4446\n",
            "t = 14, avg_loss = 0.4669\n",
            "t = 15, avg_loss = 0.4731\n",
            "t = 16, avg_loss = 0.3886\n",
            "t = 17, avg_loss = 0.4187\n",
            "t = 18, avg_loss = 0.3759\n",
            "t = 19, avg_loss = 0.3531\n",
            "t = 20, avg_loss = 0.4995\n",
            "t = 21, avg_loss = 0.2831\n",
            "t = 22, avg_loss = 0.4537\n",
            "t = 23, avg_loss = 0.3731\n",
            "t = 24, avg_loss = 0.3273\n",
            "t = 25, avg_loss = 0.6066\n",
            "t = 26, avg_loss = 0.5078\n",
            "t = 27, avg_loss = 0.5345\n",
            "t = 28, avg_loss = 0.5709\n",
            "t = 29, avg_loss = 0.4574\n",
            "t = 30, avg_loss = 0.2295\n",
            "Checking accuracy on test set\n",
            "Got 148 / 240 correct (61.67)\n",
            "acc = 0.616667\n",
            "Starting epoch 16 / 50\n",
            "t = 1, avg_loss = 0.4209\n",
            "t = 2, avg_loss = 0.3145\n",
            "t = 3, avg_loss = 0.3616\n",
            "t = 4, avg_loss = 0.3747\n",
            "t = 5, avg_loss = 0.3789\n",
            "t = 6, avg_loss = 0.3307\n",
            "t = 7, avg_loss = 0.3367\n",
            "t = 8, avg_loss = 0.3104\n",
            "t = 9, avg_loss = 0.2995\n",
            "t = 10, avg_loss = 0.4242\n",
            "t = 11, avg_loss = 0.2770\n",
            "t = 12, avg_loss = 0.2620\n",
            "t = 13, avg_loss = 0.3474\n",
            "t = 14, avg_loss = 0.3146\n",
            "t = 15, avg_loss = 0.4187\n",
            "t = 16, avg_loss = 0.5028\n",
            "t = 17, avg_loss = 0.5117\n",
            "t = 18, avg_loss = 0.4314\n",
            "t = 19, avg_loss = 0.3512\n",
            "t = 20, avg_loss = 0.4997\n",
            "t = 21, avg_loss = 0.4039\n",
            "t = 22, avg_loss = 0.3405\n",
            "t = 23, avg_loss = 0.5851\n",
            "t = 24, avg_loss = 0.5297\n",
            "t = 25, avg_loss = 0.4769\n",
            "t = 26, avg_loss = 0.4103\n",
            "t = 27, avg_loss = 0.3277\n",
            "t = 28, avg_loss = 0.3517\n",
            "t = 29, avg_loss = 0.4543\n",
            "t = 30, avg_loss = 0.5389\n",
            "Checking accuracy on test set\n",
            "Got 199 / 240 correct (82.92)\n",
            "acc = 0.829167\n",
            "Starting epoch 17 / 50\n",
            "t = 1, avg_loss = 0.3568\n",
            "t = 2, avg_loss = 0.3099\n",
            "t = 3, avg_loss = 0.4539\n",
            "t = 4, avg_loss = 0.4209\n",
            "t = 5, avg_loss = 0.3871\n",
            "t = 6, avg_loss = 0.4173\n",
            "t = 7, avg_loss = 0.4524\n",
            "t = 8, avg_loss = 0.2285\n",
            "t = 9, avg_loss = 0.6306\n",
            "t = 10, avg_loss = 0.4291\n",
            "t = 11, avg_loss = 0.3896\n",
            "t = 12, avg_loss = 0.3778\n",
            "t = 13, avg_loss = 0.2686\n",
            "t = 14, avg_loss = 0.3405\n",
            "t = 15, avg_loss = 0.3518\n",
            "t = 16, avg_loss = 0.3196\n",
            "t = 17, avg_loss = 0.4150\n",
            "t = 18, avg_loss = 0.3447\n",
            "t = 19, avg_loss = 0.5284\n",
            "t = 20, avg_loss = 0.3273\n",
            "t = 21, avg_loss = 0.3652\n",
            "t = 22, avg_loss = 0.3282\n",
            "t = 23, avg_loss = 0.4358\n",
            "t = 24, avg_loss = 0.2033\n",
            "t = 25, avg_loss = 0.4869\n",
            "t = 26, avg_loss = 0.3530\n",
            "t = 27, avg_loss = 0.2452\n",
            "t = 28, avg_loss = 0.3828\n",
            "t = 29, avg_loss = 0.3462\n",
            "t = 30, avg_loss = 0.2420\n",
            "Checking accuracy on test set\n",
            "Got 171 / 240 correct (71.25)\n",
            "acc = 0.712500\n",
            "Starting epoch 18 / 50\n",
            "t = 1, avg_loss = 0.4700\n",
            "t = 2, avg_loss = 0.3324\n",
            "t = 3, avg_loss = 0.4607\n",
            "t = 4, avg_loss = 0.4696\n",
            "t = 5, avg_loss = 0.3146\n",
            "t = 6, avg_loss = 0.2278\n",
            "t = 7, avg_loss = 0.4300\n",
            "t = 8, avg_loss = 0.4440\n",
            "t = 9, avg_loss = 0.4112\n",
            "t = 10, avg_loss = 0.3068\n",
            "t = 11, avg_loss = 0.3825\n",
            "t = 12, avg_loss = 0.3699\n",
            "t = 13, avg_loss = 0.4447\n",
            "t = 14, avg_loss = 0.2134\n",
            "t = 15, avg_loss = 0.2381\n",
            "t = 16, avg_loss = 0.4446\n",
            "t = 17, avg_loss = 0.4540\n",
            "t = 18, avg_loss = 0.4867\n",
            "t = 19, avg_loss = 0.4514\n",
            "t = 20, avg_loss = 0.3220\n",
            "t = 21, avg_loss = 0.3268\n",
            "t = 22, avg_loss = 0.3507\n",
            "t = 23, avg_loss = 0.5306\n",
            "t = 24, avg_loss = 0.4544\n",
            "t = 25, avg_loss = 0.3226\n",
            "t = 26, avg_loss = 0.3753\n",
            "t = 27, avg_loss = 0.3868\n",
            "t = 28, avg_loss = 0.2571\n",
            "t = 29, avg_loss = 0.4466\n",
            "t = 30, avg_loss = 0.4755\n",
            "Checking accuracy on test set\n",
            "Got 193 / 240 correct (80.42)\n",
            "acc = 0.804167\n",
            "Starting epoch 19 / 50\n",
            "t = 1, avg_loss = 0.4031\n",
            "t = 2, avg_loss = 0.4837\n",
            "t = 3, avg_loss = 0.2993\n",
            "t = 4, avg_loss = 0.3326\n",
            "t = 5, avg_loss = 0.3649\n",
            "t = 6, avg_loss = 0.3947\n",
            "t = 7, avg_loss = 0.5571\n",
            "t = 8, avg_loss = 0.3255\n",
            "t = 9, avg_loss = 0.3715\n",
            "t = 10, avg_loss = 0.2842\n",
            "t = 11, avg_loss = 0.5324\n",
            "t = 12, avg_loss = 0.3316\n",
            "t = 13, avg_loss = 0.2228\n",
            "t = 14, avg_loss = 0.3005\n",
            "t = 15, avg_loss = 0.3102\n",
            "t = 16, avg_loss = 0.3848\n",
            "t = 17, avg_loss = 0.2866\n",
            "t = 18, avg_loss = 0.3889\n",
            "t = 19, avg_loss = 0.4982\n",
            "t = 20, avg_loss = 0.5024\n",
            "t = 21, avg_loss = 0.4337\n",
            "t = 22, avg_loss = 0.3008\n",
            "t = 23, avg_loss = 0.3771\n",
            "t = 24, avg_loss = 0.4014\n",
            "t = 25, avg_loss = 0.3393\n",
            "t = 26, avg_loss = 0.4109\n",
            "t = 27, avg_loss = 0.2595\n",
            "t = 28, avg_loss = 0.3156\n",
            "t = 29, avg_loss = 0.2601\n",
            "t = 30, avg_loss = 0.3129\n",
            "Checking accuracy on test set\n",
            "Got 203 / 240 correct (84.58)\n",
            "acc = 0.845833\n",
            "Starting epoch 20 / 50\n",
            "t = 1, avg_loss = 0.3854\n",
            "t = 2, avg_loss = 0.3879\n",
            "t = 3, avg_loss = 0.4002\n",
            "t = 4, avg_loss = 0.4338\n",
            "t = 5, avg_loss = 0.4354\n",
            "t = 6, avg_loss = 0.3456\n",
            "t = 7, avg_loss = 0.3315\n",
            "t = 8, avg_loss = 0.3483\n",
            "t = 9, avg_loss = 0.4228\n",
            "t = 10, avg_loss = 0.4666\n",
            "t = 11, avg_loss = 0.1945\n",
            "t = 12, avg_loss = 0.3381\n",
            "t = 13, avg_loss = 0.2935\n",
            "t = 14, avg_loss = 0.5147\n",
            "t = 15, avg_loss = 0.2571\n",
            "t = 16, avg_loss = 0.3716\n",
            "t = 17, avg_loss = 0.4904\n",
            "t = 18, avg_loss = 0.4893\n",
            "t = 19, avg_loss = 0.4650\n",
            "t = 20, avg_loss = 0.4144\n",
            "t = 21, avg_loss = 0.2274\n",
            "t = 22, avg_loss = 0.4067\n",
            "t = 23, avg_loss = 0.2447\n",
            "t = 24, avg_loss = 0.2431\n",
            "t = 25, avg_loss = 0.3470\n",
            "t = 26, avg_loss = 0.2990\n",
            "t = 27, avg_loss = 0.3021\n",
            "t = 28, avg_loss = 0.3358\n",
            "t = 29, avg_loss = 0.4094\n",
            "t = 30, avg_loss = 0.4512\n",
            "Checking accuracy on test set\n",
            "Got 204 / 240 correct (85.00)\n",
            "acc = 0.850000\n",
            "Starting epoch 21 / 50\n",
            "t = 1, avg_loss = 0.6292\n",
            "t = 2, avg_loss = 0.4719\n",
            "t = 3, avg_loss = 0.3671\n",
            "t = 4, avg_loss = 0.3831\n",
            "t = 5, avg_loss = 0.3692\n",
            "t = 6, avg_loss = 0.2426\n",
            "t = 7, avg_loss = 0.3464\n",
            "t = 8, avg_loss = 0.3424\n",
            "t = 9, avg_loss = 0.5344\n",
            "t = 10, avg_loss = 0.3116\n",
            "t = 11, avg_loss = 0.6684\n",
            "t = 12, avg_loss = 0.3492\n",
            "t = 13, avg_loss = 0.3494\n",
            "t = 14, avg_loss = 0.2272\n",
            "t = 15, avg_loss = 0.3527\n",
            "t = 16, avg_loss = 0.3445\n",
            "t = 17, avg_loss = 0.3746\n",
            "t = 18, avg_loss = 0.3523\n",
            "t = 19, avg_loss = 0.3788\n",
            "t = 20, avg_loss = 0.3991\n",
            "t = 21, avg_loss = 0.2512\n",
            "t = 22, avg_loss = 0.3820\n",
            "t = 23, avg_loss = 0.3039\n",
            "t = 24, avg_loss = 0.4544\n",
            "t = 25, avg_loss = 0.4181\n",
            "t = 26, avg_loss = 0.3929\n",
            "t = 27, avg_loss = 0.4373\n",
            "t = 28, avg_loss = 0.2614\n",
            "t = 29, avg_loss = 0.3365\n",
            "t = 30, avg_loss = 0.3264\n",
            "Checking accuracy on test set\n",
            "Got 202 / 240 correct (84.17)\n",
            "acc = 0.841667\n",
            "Starting epoch 22 / 50\n",
            "t = 1, avg_loss = 0.3525\n",
            "t = 2, avg_loss = 0.4211\n",
            "t = 3, avg_loss = 0.2564\n",
            "t = 4, avg_loss = 0.4323\n",
            "t = 5, avg_loss = 0.3453\n",
            "t = 6, avg_loss = 0.3912\n",
            "t = 7, avg_loss = 0.3302\n",
            "t = 8, avg_loss = 0.2534\n",
            "t = 9, avg_loss = 0.5905\n",
            "t = 10, avg_loss = 0.4050\n",
            "t = 11, avg_loss = 0.3512\n",
            "t = 12, avg_loss = 0.3155\n",
            "t = 13, avg_loss = 0.3812\n",
            "t = 14, avg_loss = 0.3716\n",
            "t = 15, avg_loss = 0.3793\n",
            "t = 16, avg_loss = 0.4262\n",
            "t = 17, avg_loss = 0.3603\n",
            "t = 18, avg_loss = 0.3754\n",
            "t = 19, avg_loss = 0.3579\n",
            "t = 20, avg_loss = 0.3441\n",
            "t = 21, avg_loss = 0.2994\n",
            "t = 22, avg_loss = 0.3362\n",
            "t = 23, avg_loss = 0.3030\n",
            "t = 24, avg_loss = 0.3193\n",
            "t = 25, avg_loss = 0.3558\n",
            "t = 26, avg_loss = 0.5295\n",
            "t = 27, avg_loss = 0.3265\n",
            "t = 28, avg_loss = 0.2584\n",
            "t = 29, avg_loss = 0.4792\n",
            "t = 30, avg_loss = 0.2748\n",
            "Checking accuracy on test set\n",
            "Got 185 / 240 correct (77.08)\n",
            "acc = 0.770833\n",
            "Starting epoch 23 / 50\n",
            "t = 1, avg_loss = 0.1770\n",
            "t = 2, avg_loss = 0.2525\n",
            "t = 3, avg_loss = 0.2055\n",
            "t = 4, avg_loss = 0.2947\n",
            "t = 5, avg_loss = 0.3090\n",
            "t = 6, avg_loss = 0.3008\n",
            "t = 7, avg_loss = 0.4499\n",
            "t = 8, avg_loss = 0.2550\n",
            "t = 9, avg_loss = 0.5104\n",
            "t = 10, avg_loss = 0.3055\n",
            "t = 11, avg_loss = 0.3519\n",
            "t = 12, avg_loss = 0.4229\n",
            "t = 13, avg_loss = 0.4186\n",
            "t = 14, avg_loss = 0.2264\n",
            "t = 15, avg_loss = 0.3123\n",
            "t = 16, avg_loss = 0.2599\n",
            "t = 17, avg_loss = 0.2855\n",
            "t = 18, avg_loss = 0.2655\n",
            "t = 19, avg_loss = 0.2762\n",
            "t = 20, avg_loss = 0.2399\n",
            "t = 21, avg_loss = 0.4011\n",
            "t = 22, avg_loss = 0.2607\n",
            "t = 23, avg_loss = 0.3576\n",
            "t = 24, avg_loss = 0.3930\n",
            "t = 25, avg_loss = 0.3809\n",
            "t = 26, avg_loss = 0.5562\n",
            "t = 27, avg_loss = 0.2717\n",
            "t = 28, avg_loss = 0.4478\n",
            "t = 29, avg_loss = 0.4757\n",
            "t = 30, avg_loss = 0.3887\n",
            "Checking accuracy on test set\n",
            "Got 180 / 240 correct (75.00)\n",
            "acc = 0.750000\n",
            "Starting epoch 24 / 50\n",
            "t = 1, avg_loss = 0.3362\n",
            "t = 2, avg_loss = 0.3467\n",
            "t = 3, avg_loss = 0.4802\n",
            "t = 4, avg_loss = 0.3197\n",
            "t = 5, avg_loss = 0.4533\n",
            "t = 6, avg_loss = 0.3554\n",
            "t = 7, avg_loss = 0.4384\n",
            "t = 8, avg_loss = 0.4051\n",
            "t = 9, avg_loss = 0.4210\n",
            "t = 10, avg_loss = 0.3604\n",
            "t = 11, avg_loss = 0.2713\n",
            "t = 12, avg_loss = 0.3137\n",
            "t = 13, avg_loss = 0.2864\n",
            "t = 14, avg_loss = 0.4221\n",
            "t = 15, avg_loss = 0.2897\n",
            "t = 16, avg_loss = 0.3876\n",
            "t = 17, avg_loss = 0.2600\n",
            "t = 18, avg_loss = 0.2965\n",
            "t = 19, avg_loss = 0.4617\n",
            "t = 20, avg_loss = 0.3420\n",
            "t = 21, avg_loss = 0.4266\n",
            "t = 22, avg_loss = 0.1900\n",
            "t = 23, avg_loss = 0.3907\n",
            "t = 24, avg_loss = 0.3883\n",
            "t = 25, avg_loss = 0.2674\n",
            "t = 26, avg_loss = 0.3694\n",
            "t = 27, avg_loss = 0.4919\n",
            "t = 28, avg_loss = 0.2491\n",
            "t = 29, avg_loss = 0.5113\n",
            "t = 30, avg_loss = 0.2804\n",
            "Checking accuracy on test set\n",
            "Got 198 / 240 correct (82.50)\n",
            "acc = 0.825000\n",
            "Starting epoch 25 / 50\n",
            "t = 1, avg_loss = 0.3442\n",
            "t = 2, avg_loss = 0.2622\n",
            "t = 3, avg_loss = 0.3668\n",
            "t = 4, avg_loss = 0.3345\n",
            "t = 5, avg_loss = 0.3004\n",
            "t = 6, avg_loss = 0.2560\n",
            "t = 7, avg_loss = 0.2696\n",
            "t = 8, avg_loss = 0.4565\n",
            "t = 9, avg_loss = 0.4063\n",
            "t = 10, avg_loss = 0.1891\n",
            "t = 11, avg_loss = 0.2776\n",
            "t = 12, avg_loss = 0.2969\n",
            "t = 13, avg_loss = 0.5254\n",
            "t = 14, avg_loss = 0.4572\n",
            "t = 15, avg_loss = 0.4471\n",
            "t = 16, avg_loss = 0.3334\n",
            "t = 17, avg_loss = 0.2809\n",
            "t = 18, avg_loss = 0.4379\n",
            "t = 19, avg_loss = 0.3951\n",
            "t = 20, avg_loss = 0.3721\n",
            "t = 21, avg_loss = 0.4410\n",
            "t = 22, avg_loss = 0.3986\n",
            "t = 23, avg_loss = 0.3954\n",
            "t = 24, avg_loss = 0.2301\n",
            "t = 25, avg_loss = 0.3449\n",
            "t = 26, avg_loss = 0.4714\n",
            "t = 27, avg_loss = 0.2845\n",
            "t = 28, avg_loss = 0.3573\n",
            "t = 29, avg_loss = 0.2852\n",
            "t = 30, avg_loss = 0.5206\n",
            "Checking accuracy on test set\n",
            "Got 191 / 240 correct (79.58)\n",
            "acc = 0.795833\n",
            "Starting epoch 26 / 50\n",
            "t = 1, avg_loss = 0.3009\n",
            "t = 2, avg_loss = 0.3646\n",
            "t = 3, avg_loss = 0.3546\n",
            "t = 4, avg_loss = 0.4417\n",
            "t = 5, avg_loss = 0.4003\n",
            "t = 6, avg_loss = 0.3229\n",
            "t = 7, avg_loss = 0.4895\n",
            "t = 8, avg_loss = 0.3726\n",
            "t = 9, avg_loss = 0.3901\n",
            "t = 10, avg_loss = 0.3773\n",
            "t = 11, avg_loss = 0.2481\n",
            "t = 12, avg_loss = 0.2452\n",
            "t = 13, avg_loss = 0.4304\n",
            "t = 14, avg_loss = 0.3185\n",
            "t = 15, avg_loss = 0.3591\n",
            "t = 16, avg_loss = 0.3145\n",
            "t = 17, avg_loss = 0.4035\n",
            "t = 18, avg_loss = 0.2939\n",
            "t = 19, avg_loss = 0.3982\n",
            "t = 20, avg_loss = 0.4119\n",
            "t = 21, avg_loss = 0.4147\n",
            "t = 22, avg_loss = 0.3996\n",
            "t = 23, avg_loss = 0.4477\n",
            "t = 24, avg_loss = 0.2914\n",
            "t = 25, avg_loss = 0.3159\n",
            "t = 26, avg_loss = 0.3811\n",
            "t = 27, avg_loss = 0.2272\n",
            "t = 28, avg_loss = 0.4756\n",
            "t = 29, avg_loss = 0.2390\n",
            "t = 30, avg_loss = 0.5688\n",
            "Checking accuracy on test set\n",
            "Got 193 / 240 correct (80.42)\n",
            "acc = 0.804167\n",
            "Starting epoch 27 / 50\n",
            "t = 1, avg_loss = 0.1752\n",
            "t = 2, avg_loss = 0.3282\n",
            "t = 3, avg_loss = 0.1957\n",
            "t = 4, avg_loss = 0.4300\n",
            "t = 5, avg_loss = 0.3963\n",
            "t = 6, avg_loss = 0.7522\n",
            "t = 7, avg_loss = 0.2597\n",
            "t = 8, avg_loss = 0.3634\n",
            "t = 9, avg_loss = 0.3223\n",
            "t = 10, avg_loss = 0.4333\n",
            "t = 11, avg_loss = 0.5796\n",
            "t = 12, avg_loss = 0.3382\n",
            "t = 13, avg_loss = 0.3466\n",
            "t = 14, avg_loss = 0.4713\n",
            "t = 15, avg_loss = 0.5159\n",
            "t = 16, avg_loss = 0.4045\n",
            "t = 17, avg_loss = 0.2507\n",
            "t = 18, avg_loss = 0.4547\n",
            "t = 19, avg_loss = 0.4129\n",
            "t = 20, avg_loss = 0.3331\n",
            "t = 21, avg_loss = 0.2754\n",
            "t = 22, avg_loss = 0.3004\n",
            "t = 23, avg_loss = 0.4133\n",
            "t = 24, avg_loss = 0.4687\n",
            "t = 25, avg_loss = 0.3825\n",
            "t = 26, avg_loss = 0.4009\n",
            "t = 27, avg_loss = 0.4412\n",
            "t = 28, avg_loss = 0.2937\n",
            "t = 29, avg_loss = 0.3643\n",
            "t = 30, avg_loss = 0.2822\n",
            "Checking accuracy on test set\n",
            "Got 199 / 240 correct (82.92)\n",
            "acc = 0.829167\n",
            "Starting epoch 28 / 50\n",
            "t = 1, avg_loss = 0.3123\n",
            "t = 2, avg_loss = 0.3710\n",
            "t = 3, avg_loss = 0.2836\n",
            "t = 4, avg_loss = 0.3292\n",
            "t = 5, avg_loss = 0.4272\n",
            "t = 6, avg_loss = 0.2483\n",
            "t = 7, avg_loss = 0.3706\n",
            "t = 8, avg_loss = 0.3093\n",
            "t = 9, avg_loss = 0.2759\n",
            "t = 10, avg_loss = 0.5005\n",
            "t = 11, avg_loss = 0.2889\n",
            "t = 12, avg_loss = 0.4727\n",
            "t = 13, avg_loss = 0.3520\n",
            "t = 14, avg_loss = 0.3292\n",
            "t = 15, avg_loss = 0.1744\n",
            "t = 16, avg_loss = 0.3026\n",
            "t = 17, avg_loss = 0.4526\n",
            "t = 18, avg_loss = 0.4064\n",
            "t = 19, avg_loss = 0.2572\n",
            "t = 20, avg_loss = 0.5990\n",
            "t = 21, avg_loss = 0.2590\n",
            "t = 22, avg_loss = 0.3753\n",
            "t = 23, avg_loss = 0.3685\n",
            "t = 24, avg_loss = 0.2094\n",
            "t = 25, avg_loss = 0.3078\n",
            "t = 26, avg_loss = 0.4314\n",
            "t = 27, avg_loss = 0.2936\n",
            "t = 28, avg_loss = 0.4292\n",
            "t = 29, avg_loss = 0.6463\n",
            "t = 30, avg_loss = 0.3646\n",
            "Checking accuracy on test set\n",
            "Got 198 / 240 correct (82.50)\n",
            "acc = 0.825000\n",
            "Starting epoch 29 / 50\n",
            "t = 1, avg_loss = 0.2714\n",
            "t = 2, avg_loss = 0.3081\n",
            "t = 3, avg_loss = 0.2532\n",
            "t = 4, avg_loss = 0.3371\n",
            "t = 5, avg_loss = 0.4490\n",
            "t = 6, avg_loss = 0.3282\n",
            "t = 7, avg_loss = 0.3396\n",
            "t = 8, avg_loss = 0.5049\n",
            "t = 9, avg_loss = 0.5319\n",
            "t = 10, avg_loss = 0.2850\n",
            "t = 11, avg_loss = 0.4670\n",
            "t = 12, avg_loss = 0.3281\n",
            "t = 13, avg_loss = 0.3241\n",
            "t = 14, avg_loss = 0.3618\n",
            "t = 15, avg_loss = 0.2926\n",
            "t = 16, avg_loss = 0.2884\n",
            "t = 17, avg_loss = 0.2553\n",
            "t = 18, avg_loss = 0.3143\n",
            "t = 19, avg_loss = 0.3507\n",
            "t = 20, avg_loss = 0.8447\n",
            "t = 21, avg_loss = 0.6255\n",
            "t = 22, avg_loss = 0.2995\n",
            "t = 23, avg_loss = 0.2683\n",
            "t = 24, avg_loss = 0.3467\n",
            "t = 25, avg_loss = 0.4347\n",
            "t = 26, avg_loss = 0.3197\n",
            "t = 27, avg_loss = 0.2411\n",
            "t = 28, avg_loss = 0.4226\n",
            "t = 29, avg_loss = 0.5457\n",
            "t = 30, avg_loss = 0.4307\n",
            "Checking accuracy on test set\n",
            "Got 183 / 240 correct (76.25)\n",
            "acc = 0.762500\n",
            "Starting epoch 30 / 50\n",
            "t = 1, avg_loss = 0.3333\n",
            "t = 2, avg_loss = 0.3211\n",
            "t = 3, avg_loss = 0.4543\n",
            "t = 4, avg_loss = 0.4342\n",
            "t = 5, avg_loss = 0.3535\n",
            "t = 6, avg_loss = 0.3479\n",
            "t = 7, avg_loss = 0.3132\n",
            "t = 8, avg_loss = 0.2815\n",
            "t = 9, avg_loss = 0.3874\n",
            "t = 10, avg_loss = 0.3485\n",
            "t = 11, avg_loss = 0.4755\n",
            "t = 12, avg_loss = 0.2436\n",
            "t = 13, avg_loss = 0.4085\n",
            "t = 14, avg_loss = 0.3313\n",
            "t = 15, avg_loss = 0.3718\n",
            "t = 16, avg_loss = 0.2620\n",
            "t = 17, avg_loss = 0.3527\n",
            "t = 18, avg_loss = 0.3203\n",
            "t = 19, avg_loss = 0.1745\n",
            "t = 20, avg_loss = 0.3022\n",
            "t = 21, avg_loss = 0.3649\n",
            "t = 22, avg_loss = 0.3389\n",
            "t = 23, avg_loss = 0.4183\n",
            "t = 24, avg_loss = 0.1996\n",
            "t = 25, avg_loss = 0.2561\n",
            "t = 26, avg_loss = 0.3922\n",
            "t = 27, avg_loss = 0.2170\n",
            "t = 28, avg_loss = 0.4853\n",
            "t = 29, avg_loss = 0.4008\n",
            "t = 30, avg_loss = 0.2387\n",
            "Checking accuracy on test set\n",
            "Got 206 / 240 correct (85.83)\n",
            "acc = 0.858333\n",
            "Starting epoch 31 / 50\n",
            "t = 1, avg_loss = 0.2024\n",
            "t = 2, avg_loss = 0.3205\n",
            "t = 3, avg_loss = 0.3291\n",
            "t = 4, avg_loss = 0.1965\n",
            "t = 5, avg_loss = 0.1840\n",
            "t = 6, avg_loss = 0.2747\n",
            "t = 7, avg_loss = 0.3047\n",
            "t = 8, avg_loss = 0.2529\n",
            "t = 9, avg_loss = 0.4906\n",
            "t = 10, avg_loss = 0.2862\n",
            "t = 11, avg_loss = 0.2466\n",
            "t = 12, avg_loss = 0.2544\n",
            "t = 13, avg_loss = 0.2373\n",
            "t = 14, avg_loss = 0.5769\n",
            "t = 15, avg_loss = 0.3374\n",
            "t = 16, avg_loss = 0.3379\n",
            "t = 17, avg_loss = 0.5128\n",
            "t = 18, avg_loss = 0.3290\n",
            "t = 19, avg_loss = 0.3008\n",
            "t = 20, avg_loss = 0.2666\n",
            "t = 21, avg_loss = 0.4688\n",
            "t = 22, avg_loss = 0.5662\n",
            "t = 23, avg_loss = 0.2534\n",
            "t = 24, avg_loss = 0.4827\n",
            "t = 25, avg_loss = 0.2332\n",
            "t = 26, avg_loss = 0.3671\n",
            "t = 27, avg_loss = 0.4474\n",
            "t = 28, avg_loss = 0.3239\n",
            "t = 29, avg_loss = 0.3506\n",
            "t = 30, avg_loss = 0.3516\n",
            "Checking accuracy on test set\n",
            "Got 191 / 240 correct (79.58)\n",
            "acc = 0.795833\n",
            "Starting epoch 32 / 50\n",
            "t = 1, avg_loss = 0.2453\n",
            "t = 2, avg_loss = 0.3636\n",
            "t = 3, avg_loss = 0.4055\n",
            "t = 4, avg_loss = 0.3334\n",
            "t = 5, avg_loss = 0.2210\n",
            "t = 6, avg_loss = 0.2311\n",
            "t = 7, avg_loss = 0.4196\n",
            "t = 8, avg_loss = 0.3039\n",
            "t = 9, avg_loss = 0.3761\n",
            "t = 10, avg_loss = 0.3913\n",
            "t = 11, avg_loss = 0.2701\n",
            "t = 12, avg_loss = 0.3817\n",
            "t = 13, avg_loss = 0.3331\n",
            "t = 14, avg_loss = 0.6030\n",
            "t = 15, avg_loss = 0.3939\n",
            "t = 16, avg_loss = 0.3299\n",
            "t = 17, avg_loss = 0.3231\n",
            "t = 18, avg_loss = 0.4264\n",
            "t = 19, avg_loss = 0.3092\n",
            "t = 20, avg_loss = 0.3662\n",
            "t = 21, avg_loss = 0.2337\n",
            "t = 22, avg_loss = 0.2416\n",
            "t = 23, avg_loss = 0.4613\n",
            "t = 24, avg_loss = 0.3418\n",
            "t = 25, avg_loss = 0.1966\n",
            "t = 26, avg_loss = 0.3230\n",
            "t = 27, avg_loss = 0.4000\n",
            "t = 28, avg_loss = 0.3616\n",
            "t = 29, avg_loss = 0.4841\n",
            "t = 30, avg_loss = 0.3000\n",
            "Checking accuracy on test set\n",
            "Got 208 / 240 correct (86.67)\n",
            "acc = 0.866667\n",
            "Starting epoch 33 / 50\n",
            "t = 1, avg_loss = 0.2207\n",
            "t = 2, avg_loss = 0.4816\n",
            "t = 3, avg_loss = 0.2658\n",
            "t = 4, avg_loss = 0.2809\n",
            "t = 5, avg_loss = 0.3132\n",
            "t = 6, avg_loss = 0.1858\n",
            "t = 7, avg_loss = 0.2856\n",
            "t = 8, avg_loss = 0.2278\n",
            "t = 9, avg_loss = 0.4109\n",
            "t = 10, avg_loss = 0.2065\n",
            "t = 11, avg_loss = 0.5677\n",
            "t = 12, avg_loss = 0.2575\n",
            "t = 13, avg_loss = 0.2529\n",
            "t = 14, avg_loss = 0.3261\n",
            "t = 15, avg_loss = 0.4001\n",
            "t = 16, avg_loss = 0.3542\n",
            "t = 17, avg_loss = 0.4525\n",
            "t = 18, avg_loss = 0.5219\n",
            "t = 19, avg_loss = 0.3936\n",
            "t = 20, avg_loss = 0.3309\n",
            "t = 21, avg_loss = 0.2617\n",
            "t = 22, avg_loss = 0.2194\n",
            "t = 23, avg_loss = 0.2881\n",
            "t = 24, avg_loss = 0.4859\n",
            "t = 25, avg_loss = 0.3082\n",
            "t = 26, avg_loss = 0.2354\n",
            "t = 27, avg_loss = 0.4435\n",
            "t = 28, avg_loss = 0.5138\n",
            "t = 29, avg_loss = 0.3696\n",
            "t = 30, avg_loss = 0.1619\n",
            "Checking accuracy on test set\n",
            "Got 202 / 240 correct (84.17)\n",
            "acc = 0.841667\n",
            "Starting epoch 34 / 50\n",
            "t = 1, avg_loss = 0.4272\n",
            "t = 2, avg_loss = 0.2070\n",
            "t = 3, avg_loss = 0.3570\n",
            "t = 4, avg_loss = 0.1974\n",
            "t = 5, avg_loss = 0.2429\n",
            "t = 6, avg_loss = 0.3475\n",
            "t = 7, avg_loss = 0.2679\n",
            "t = 8, avg_loss = 0.3056\n",
            "t = 9, avg_loss = 0.3053\n",
            "t = 10, avg_loss = 0.6546\n",
            "t = 11, avg_loss = 0.3792\n",
            "t = 12, avg_loss = 0.3815\n",
            "t = 13, avg_loss = 0.3647\n",
            "t = 14, avg_loss = 0.3727\n",
            "t = 15, avg_loss = 0.3016\n",
            "t = 16, avg_loss = 0.3865\n",
            "t = 17, avg_loss = 0.3354\n",
            "t = 18, avg_loss = 0.2640\n",
            "t = 19, avg_loss = 0.2576\n",
            "t = 20, avg_loss = 0.3788\n",
            "t = 21, avg_loss = 0.3685\n",
            "t = 22, avg_loss = 0.2679\n",
            "t = 23, avg_loss = 0.3518\n",
            "t = 24, avg_loss = 0.4068\n",
            "t = 25, avg_loss = 0.3253\n",
            "t = 26, avg_loss = 0.2423\n",
            "t = 27, avg_loss = 0.3068\n",
            "t = 28, avg_loss = 0.2347\n",
            "t = 29, avg_loss = 0.3591\n",
            "t = 30, avg_loss = 0.4276\n",
            "Checking accuracy on test set\n",
            "Got 208 / 240 correct (86.67)\n",
            "acc = 0.866667\n",
            "Starting epoch 35 / 50\n",
            "t = 1, avg_loss = 0.2776\n",
            "t = 2, avg_loss = 0.3816\n",
            "t = 3, avg_loss = 0.4801\n",
            "t = 4, avg_loss = 0.1367\n",
            "t = 5, avg_loss = 0.3312\n",
            "t = 6, avg_loss = 0.2653\n",
            "t = 7, avg_loss = 0.2260\n",
            "t = 8, avg_loss = 0.4169\n",
            "t = 9, avg_loss = 0.4648\n",
            "t = 10, avg_loss = 0.4567\n",
            "t = 11, avg_loss = 0.3723\n",
            "t = 12, avg_loss = 0.3168\n",
            "t = 13, avg_loss = 0.4212\n",
            "t = 14, avg_loss = 0.2354\n",
            "t = 15, avg_loss = 0.3611\n",
            "t = 16, avg_loss = 0.4162\n",
            "t = 17, avg_loss = 0.4502\n",
            "t = 18, avg_loss = 0.3145\n",
            "t = 19, avg_loss = 0.3279\n",
            "t = 20, avg_loss = 0.3435\n",
            "t = 21, avg_loss = 0.2866\n",
            "t = 22, avg_loss = 0.3532\n",
            "t = 23, avg_loss = 0.3117\n",
            "t = 24, avg_loss = 0.4044\n",
            "t = 25, avg_loss = 0.3919\n",
            "t = 26, avg_loss = 0.3607\n",
            "t = 27, avg_loss = 0.3372\n",
            "t = 28, avg_loss = 0.3528\n",
            "t = 29, avg_loss = 0.3565\n",
            "t = 30, avg_loss = 0.4296\n",
            "Checking accuracy on test set\n",
            "Got 207 / 240 correct (86.25)\n",
            "acc = 0.862500\n",
            "Starting epoch 36 / 50\n",
            "t = 1, avg_loss = 0.1910\n",
            "t = 2, avg_loss = 0.2185\n",
            "t = 3, avg_loss = 0.2255\n",
            "t = 4, avg_loss = 0.2705\n",
            "t = 5, avg_loss = 0.3931\n",
            "t = 6, avg_loss = 0.2491\n",
            "t = 7, avg_loss = 0.1682\n",
            "t = 8, avg_loss = 0.3474\n",
            "t = 9, avg_loss = 0.2844\n",
            "t = 10, avg_loss = 0.4379\n",
            "t = 11, avg_loss = 0.4862\n",
            "t = 12, avg_loss = 0.4231\n",
            "t = 13, avg_loss = 0.4605\n",
            "t = 14, avg_loss = 0.2929\n",
            "t = 15, avg_loss = 0.1578\n",
            "t = 16, avg_loss = 0.3754\n",
            "t = 17, avg_loss = 0.3472\n",
            "t = 18, avg_loss = 0.5073\n",
            "t = 19, avg_loss = 0.1994\n",
            "t = 20, avg_loss = 0.3059\n",
            "t = 21, avg_loss = 0.2655\n",
            "t = 22, avg_loss = 0.3128\n",
            "t = 23, avg_loss = 0.3163\n",
            "t = 24, avg_loss = 0.2670\n",
            "t = 25, avg_loss = 0.5023\n",
            "t = 26, avg_loss = 0.4055\n",
            "t = 27, avg_loss = 0.3634\n",
            "t = 28, avg_loss = 0.2259\n",
            "t = 29, avg_loss = 0.1977\n",
            "t = 30, avg_loss = 0.4904\n",
            "Checking accuracy on test set\n",
            "Got 188 / 240 correct (78.33)\n",
            "acc = 0.783333\n",
            "Starting epoch 37 / 50\n",
            "t = 1, avg_loss = 0.2334\n",
            "t = 2, avg_loss = 0.2725\n",
            "t = 3, avg_loss = 0.3605\n",
            "t = 4, avg_loss = 0.2426\n",
            "t = 5, avg_loss = 0.2394\n",
            "t = 6, avg_loss = 0.2959\n",
            "t = 7, avg_loss = 0.3008\n",
            "t = 8, avg_loss = 0.3318\n",
            "t = 9, avg_loss = 0.2476\n",
            "t = 10, avg_loss = 0.3287\n",
            "t = 11, avg_loss = 0.3150\n",
            "t = 12, avg_loss = 0.4546\n",
            "t = 13, avg_loss = 0.3248\n",
            "t = 14, avg_loss = 0.2034\n",
            "t = 15, avg_loss = 0.4450\n",
            "t = 16, avg_loss = 0.3564\n",
            "t = 17, avg_loss = 0.3617\n",
            "t = 18, avg_loss = 0.5442\n",
            "t = 19, avg_loss = 0.2423\n",
            "t = 20, avg_loss = 0.4405\n",
            "t = 21, avg_loss = 0.2979\n",
            "t = 22, avg_loss = 0.3886\n",
            "t = 23, avg_loss = 0.4308\n",
            "t = 24, avg_loss = 0.3559\n",
            "t = 25, avg_loss = 0.2803\n",
            "t = 26, avg_loss = 0.3490\n",
            "t = 27, avg_loss = 0.2965\n",
            "t = 28, avg_loss = 0.2927\n",
            "t = 29, avg_loss = 0.5164\n",
            "t = 30, avg_loss = 0.3880\n",
            "Checking accuracy on test set\n",
            "Got 201 / 240 correct (83.75)\n",
            "acc = 0.837500\n",
            "Starting epoch 38 / 50\n",
            "t = 1, avg_loss = 0.2958\n",
            "t = 2, avg_loss = 0.3464\n",
            "t = 3, avg_loss = 0.3561\n",
            "t = 4, avg_loss = 0.2668\n",
            "t = 5, avg_loss = 0.3773\n",
            "t = 6, avg_loss = 0.2794\n",
            "t = 7, avg_loss = 0.4613\n",
            "t = 8, avg_loss = 0.2969\n",
            "t = 9, avg_loss = 0.3799\n",
            "t = 10, avg_loss = 0.2291\n",
            "t = 11, avg_loss = 0.2996\n",
            "t = 12, avg_loss = 0.3010\n",
            "t = 13, avg_loss = 0.2568\n",
            "t = 14, avg_loss = 0.2985\n",
            "t = 15, avg_loss = 0.3018\n",
            "t = 16, avg_loss = 0.2494\n",
            "t = 17, avg_loss = 0.3648\n",
            "t = 18, avg_loss = 0.2318\n",
            "t = 19, avg_loss = 0.3214\n",
            "t = 20, avg_loss = 0.1499\n",
            "t = 21, avg_loss = 0.2237\n",
            "t = 22, avg_loss = 0.1990\n",
            "t = 23, avg_loss = 0.3120\n",
            "t = 24, avg_loss = 0.4538\n",
            "t = 25, avg_loss = 0.4570\n",
            "t = 26, avg_loss = 0.2240\n",
            "t = 27, avg_loss = 0.4054\n",
            "t = 28, avg_loss = 0.1890\n",
            "t = 29, avg_loss = 0.2742\n",
            "t = 30, avg_loss = 0.3580\n",
            "Checking accuracy on test set\n",
            "Got 209 / 240 correct (87.08)\n",
            "acc = 0.870833\n",
            "Starting epoch 39 / 50\n",
            "t = 1, avg_loss = 0.2027\n",
            "t = 2, avg_loss = 0.2377\n",
            "t = 3, avg_loss = 0.2868\n",
            "t = 4, avg_loss = 0.4027\n",
            "t = 5, avg_loss = 0.2988\n",
            "t = 6, avg_loss = 0.4332\n",
            "t = 7, avg_loss = 0.2282\n",
            "t = 8, avg_loss = 0.1981\n",
            "t = 9, avg_loss = 0.3761\n",
            "t = 10, avg_loss = 0.4153\n",
            "t = 11, avg_loss = 0.1976\n",
            "t = 12, avg_loss = 0.6091\n",
            "t = 13, avg_loss = 0.3197\n",
            "t = 14, avg_loss = 0.3437\n",
            "t = 15, avg_loss = 0.1967\n",
            "t = 16, avg_loss = 0.3103\n",
            "t = 17, avg_loss = 0.4226\n",
            "t = 18, avg_loss = 0.2322\n",
            "t = 19, avg_loss = 0.3349\n",
            "t = 20, avg_loss = 0.4831\n",
            "t = 21, avg_loss = 0.2501\n",
            "t = 22, avg_loss = 0.2552\n",
            "t = 23, avg_loss = 0.3135\n",
            "t = 24, avg_loss = 0.3922\n",
            "t = 25, avg_loss = 0.2747\n",
            "t = 26, avg_loss = 0.4787\n",
            "t = 27, avg_loss = 0.4143\n",
            "t = 28, avg_loss = 0.4443\n",
            "t = 29, avg_loss = 0.2566\n",
            "t = 30, avg_loss = 0.4468\n",
            "Checking accuracy on test set\n",
            "Got 200 / 240 correct (83.33)\n",
            "acc = 0.833333\n",
            "Starting epoch 40 / 50\n",
            "t = 1, avg_loss = 0.2693\n",
            "t = 2, avg_loss = 0.3574\n",
            "t = 3, avg_loss = 0.2732\n",
            "t = 4, avg_loss = 0.3948\n",
            "t = 5, avg_loss = 0.3132\n",
            "t = 6, avg_loss = 0.5234\n",
            "t = 7, avg_loss = 0.3660\n",
            "t = 8, avg_loss = 0.3041\n",
            "t = 9, avg_loss = 0.3067\n",
            "t = 10, avg_loss = 0.3283\n",
            "t = 11, avg_loss = 0.2717\n",
            "t = 12, avg_loss = 0.4058\n",
            "t = 13, avg_loss = 0.4555\n",
            "t = 14, avg_loss = 0.4374\n",
            "t = 15, avg_loss = 0.2887\n",
            "t = 16, avg_loss = 0.3635\n",
            "t = 17, avg_loss = 0.2925\n",
            "t = 18, avg_loss = 0.3641\n",
            "t = 19, avg_loss = 0.4047\n",
            "t = 20, avg_loss = 0.2980\n",
            "t = 21, avg_loss = 0.2825\n",
            "t = 22, avg_loss = 0.3469\n",
            "t = 23, avg_loss = 0.4188\n",
            "t = 24, avg_loss = 0.3835\n",
            "t = 25, avg_loss = 0.3188\n",
            "t = 26, avg_loss = 0.2725\n",
            "t = 27, avg_loss = 0.3309\n",
            "t = 28, avg_loss = 0.5573\n",
            "t = 29, avg_loss = 0.3487\n",
            "t = 30, avg_loss = 0.4866\n",
            "Checking accuracy on test set\n",
            "Got 206 / 240 correct (85.83)\n",
            "acc = 0.858333\n",
            "Starting epoch 41 / 50\n",
            "t = 1, avg_loss = 0.2699\n",
            "t = 2, avg_loss = 0.3380\n",
            "t = 3, avg_loss = 0.3558\n",
            "t = 4, avg_loss = 0.2960\n",
            "t = 5, avg_loss = 0.2227\n",
            "t = 6, avg_loss = 0.4064\n",
            "t = 7, avg_loss = 0.3876\n",
            "t = 8, avg_loss = 0.4902\n",
            "t = 9, avg_loss = 0.2944\n",
            "t = 10, avg_loss = 0.3215\n",
            "t = 11, avg_loss = 0.3850\n",
            "t = 12, avg_loss = 0.3584\n",
            "t = 13, avg_loss = 0.5037\n",
            "t = 14, avg_loss = 0.3118\n",
            "t = 15, avg_loss = 0.3708\n",
            "t = 16, avg_loss = 0.2221\n",
            "t = 17, avg_loss = 0.3054\n",
            "t = 18, avg_loss = 0.3755\n",
            "t = 19, avg_loss = 0.4306\n",
            "t = 20, avg_loss = 0.2828\n",
            "t = 21, avg_loss = 0.4787\n",
            "t = 22, avg_loss = 0.2704\n",
            "t = 23, avg_loss = 0.4749\n",
            "t = 24, avg_loss = 0.3877\n",
            "t = 25, avg_loss = 0.4557\n",
            "t = 26, avg_loss = 0.2116\n",
            "t = 27, avg_loss = 0.4353\n",
            "t = 28, avg_loss = 0.3144\n",
            "t = 29, avg_loss = 0.3148\n",
            "t = 30, avg_loss = 0.2665\n",
            "Checking accuracy on test set\n",
            "Got 192 / 240 correct (80.00)\n",
            "acc = 0.800000\n",
            "Starting epoch 42 / 50\n",
            "t = 1, avg_loss = 0.3816\n",
            "t = 2, avg_loss = 0.2433\n",
            "t = 3, avg_loss = 0.1960\n",
            "t = 4, avg_loss = 0.2799\n",
            "t = 5, avg_loss = 0.2148\n",
            "t = 6, avg_loss = 0.3250\n",
            "t = 7, avg_loss = 0.3532\n",
            "t = 8, avg_loss = 0.2719\n",
            "t = 9, avg_loss = 0.4663\n",
            "t = 10, avg_loss = 0.3014\n",
            "t = 11, avg_loss = 0.2391\n",
            "t = 12, avg_loss = 0.2944\n",
            "t = 13, avg_loss = 0.3721\n",
            "t = 14, avg_loss = 0.3094\n",
            "t = 15, avg_loss = 0.3839\n",
            "t = 16, avg_loss = 0.3519\n",
            "t = 17, avg_loss = 0.2521\n",
            "t = 18, avg_loss = 0.3651\n",
            "t = 19, avg_loss = 0.4229\n",
            "t = 20, avg_loss = 0.2769\n",
            "t = 21, avg_loss = 0.4923\n",
            "t = 22, avg_loss = 0.2662\n",
            "t = 23, avg_loss = 0.3661\n",
            "t = 24, avg_loss = 0.2947\n",
            "t = 25, avg_loss = 0.2232\n",
            "t = 26, avg_loss = 0.2937\n",
            "t = 27, avg_loss = 0.3381\n",
            "t = 28, avg_loss = 0.2926\n",
            "t = 29, avg_loss = 0.2469\n",
            "t = 30, avg_loss = 0.2991\n",
            "Checking accuracy on test set\n",
            "Got 185 / 240 correct (77.08)\n",
            "acc = 0.770833\n",
            "Starting epoch 43 / 50\n",
            "t = 1, avg_loss = 0.3265\n",
            "t = 2, avg_loss = 0.1398\n",
            "t = 3, avg_loss = 0.2384\n",
            "t = 4, avg_loss = 0.3311\n",
            "t = 5, avg_loss = 0.2143\n",
            "t = 6, avg_loss = 0.3766\n",
            "t = 7, avg_loss = 0.4101\n",
            "t = 8, avg_loss = 0.2981\n",
            "t = 9, avg_loss = 0.3039\n",
            "t = 10, avg_loss = 0.3086\n",
            "t = 11, avg_loss = 0.1558\n",
            "t = 12, avg_loss = 0.2929\n",
            "t = 13, avg_loss = 0.2313\n",
            "t = 14, avg_loss = 0.3015\n",
            "t = 15, avg_loss = 0.3753\n",
            "t = 16, avg_loss = 0.3408\n",
            "t = 17, avg_loss = 0.4302\n",
            "t = 18, avg_loss = 0.2617\n",
            "t = 19, avg_loss = 0.4766\n",
            "t = 20, avg_loss = 0.2953\n",
            "t = 21, avg_loss = 0.3732\n",
            "t = 22, avg_loss = 0.2445\n",
            "t = 23, avg_loss = 0.4030\n",
            "t = 24, avg_loss = 0.3587\n",
            "t = 25, avg_loss = 0.3707\n",
            "t = 26, avg_loss = 0.2880\n",
            "t = 27, avg_loss = 0.4967\n",
            "t = 28, avg_loss = 0.2414\n",
            "t = 29, avg_loss = 0.3816\n",
            "t = 30, avg_loss = 0.4511\n",
            "Checking accuracy on test set\n",
            "Got 182 / 240 correct (75.83)\n",
            "acc = 0.758333\n",
            "Starting epoch 44 / 50\n",
            "t = 1, avg_loss = 0.3266\n",
            "t = 2, avg_loss = 0.2980\n",
            "t = 3, avg_loss = 0.3476\n",
            "t = 4, avg_loss = 0.4321\n",
            "t = 5, avg_loss = 0.3214\n",
            "t = 6, avg_loss = 0.3101\n",
            "t = 7, avg_loss = 0.3606\n",
            "t = 8, avg_loss = 0.3016\n",
            "t = 9, avg_loss = 0.2297\n",
            "t = 10, avg_loss = 0.2591\n",
            "t = 11, avg_loss = 0.2300\n",
            "t = 12, avg_loss = 0.2110\n",
            "t = 13, avg_loss = 0.2419\n",
            "t = 14, avg_loss = 0.3182\n",
            "t = 15, avg_loss = 0.4676\n",
            "t = 16, avg_loss = 0.2810\n",
            "t = 17, avg_loss = 0.1819\n",
            "t = 18, avg_loss = 0.1840\n",
            "t = 19, avg_loss = 0.3183\n",
            "t = 20, avg_loss = 0.2382\n",
            "t = 21, avg_loss = 0.2263\n",
            "t = 22, avg_loss = 0.3641\n",
            "t = 23, avg_loss = 0.2571\n",
            "t = 24, avg_loss = 0.2915\n",
            "t = 25, avg_loss = 0.8050\n",
            "t = 26, avg_loss = 0.1245\n",
            "t = 27, avg_loss = 0.2945\n",
            "t = 28, avg_loss = 0.2611\n",
            "t = 29, avg_loss = 0.4366\n",
            "t = 30, avg_loss = 0.3040\n",
            "Checking accuracy on test set\n",
            "Got 202 / 240 correct (84.17)\n",
            "acc = 0.841667\n",
            "Starting epoch 45 / 50\n",
            "t = 1, avg_loss = 0.4410\n",
            "t = 2, avg_loss = 0.2434\n",
            "t = 3, avg_loss = 0.1984\n",
            "t = 4, avg_loss = 0.1947\n",
            "t = 5, avg_loss = 0.2502\n",
            "t = 6, avg_loss = 0.2726\n",
            "t = 7, avg_loss = 0.2120\n",
            "t = 8, avg_loss = 0.2502\n",
            "t = 9, avg_loss = 0.2873\n",
            "t = 10, avg_loss = 0.3032\n",
            "t = 11, avg_loss = 0.4075\n",
            "t = 12, avg_loss = 0.4372\n",
            "t = 13, avg_loss = 0.4068\n",
            "t = 14, avg_loss = 0.2284\n",
            "t = 15, avg_loss = 0.4017\n",
            "t = 16, avg_loss = 0.3213\n",
            "t = 17, avg_loss = 0.3877\n",
            "t = 18, avg_loss = 0.3059\n",
            "t = 19, avg_loss = 0.4103\n",
            "t = 20, avg_loss = 0.3387\n",
            "t = 21, avg_loss = 0.2406\n",
            "t = 22, avg_loss = 0.4333\n",
            "t = 23, avg_loss = 0.2356\n",
            "t = 24, avg_loss = 0.2575\n",
            "t = 25, avg_loss = 0.3701\n",
            "t = 26, avg_loss = 0.2483\n",
            "t = 27, avg_loss = 0.2967\n",
            "t = 28, avg_loss = 0.3347\n",
            "t = 29, avg_loss = 0.3818\n",
            "t = 30, avg_loss = 0.5290\n",
            "Checking accuracy on test set\n",
            "Got 206 / 240 correct (85.83)\n",
            "acc = 0.858333\n",
            "Starting epoch 46 / 50\n",
            "t = 1, avg_loss = 0.4483\n",
            "t = 2, avg_loss = 0.3220\n",
            "t = 3, avg_loss = 0.4146\n",
            "t = 4, avg_loss = 0.4086\n",
            "t = 5, avg_loss = 0.3244\n",
            "t = 6, avg_loss = 0.2847\n",
            "t = 7, avg_loss = 0.3978\n",
            "t = 8, avg_loss = 0.3183\n",
            "t = 9, avg_loss = 0.3313\n",
            "t = 10, avg_loss = 0.3076\n",
            "t = 11, avg_loss = 0.4153\n",
            "t = 12, avg_loss = 0.2922\n",
            "t = 13, avg_loss = 0.3805\n",
            "t = 14, avg_loss = 0.2844\n",
            "t = 15, avg_loss = 0.3108\n",
            "t = 16, avg_loss = 0.2205\n",
            "t = 17, avg_loss = 0.2449\n",
            "t = 18, avg_loss = 0.3218\n",
            "t = 19, avg_loss = 0.2659\n",
            "t = 20, avg_loss = 0.4688\n",
            "t = 21, avg_loss = 0.3711\n",
            "t = 22, avg_loss = 0.1836\n",
            "t = 23, avg_loss = 0.3487\n",
            "t = 24, avg_loss = 0.6521\n",
            "t = 25, avg_loss = 0.2371\n",
            "t = 26, avg_loss = 0.1869\n",
            "t = 27, avg_loss = 0.2104\n",
            "t = 28, avg_loss = 0.2853\n",
            "t = 29, avg_loss = 0.3081\n",
            "t = 30, avg_loss = 0.3602\n",
            "Checking accuracy on test set\n",
            "Got 189 / 240 correct (78.75)\n",
            "acc = 0.787500\n",
            "Starting epoch 47 / 50\n",
            "t = 1, avg_loss = 0.3547\n",
            "t = 2, avg_loss = 0.3662\n",
            "t = 3, avg_loss = 0.3672\n",
            "t = 4, avg_loss = 0.3669\n",
            "t = 5, avg_loss = 0.2803\n",
            "t = 6, avg_loss = 0.1917\n",
            "t = 7, avg_loss = 0.3947\n",
            "t = 8, avg_loss = 0.3198\n",
            "t = 9, avg_loss = 0.3077\n",
            "t = 10, avg_loss = 0.3163\n",
            "t = 11, avg_loss = 0.2925\n",
            "t = 12, avg_loss = 0.2769\n",
            "t = 13, avg_loss = 0.3384\n",
            "t = 14, avg_loss = 0.2000\n",
            "t = 15, avg_loss = 0.3678\n",
            "t = 16, avg_loss = 0.6427\n",
            "t = 17, avg_loss = 0.3496\n",
            "t = 18, avg_loss = 0.2377\n",
            "t = 19, avg_loss = 0.3210\n",
            "t = 20, avg_loss = 0.3388\n",
            "t = 21, avg_loss = 0.2008\n",
            "t = 22, avg_loss = 0.4085\n",
            "t = 23, avg_loss = 0.3471\n",
            "t = 24, avg_loss = 0.2911\n",
            "t = 25, avg_loss = 0.1521\n",
            "t = 26, avg_loss = 0.4145\n",
            "t = 27, avg_loss = 0.2389\n",
            "t = 28, avg_loss = 0.1969\n",
            "t = 29, avg_loss = 0.3793\n",
            "t = 30, avg_loss = 0.3915\n",
            "Checking accuracy on test set\n",
            "Got 197 / 240 correct (82.08)\n",
            "acc = 0.820833\n",
            "Starting epoch 48 / 50\n",
            "t = 1, avg_loss = 0.1247\n",
            "t = 2, avg_loss = 0.3798\n",
            "t = 3, avg_loss = 0.3147\n",
            "t = 4, avg_loss = 0.4456\n",
            "t = 5, avg_loss = 0.5828\n",
            "t = 6, avg_loss = 0.3120\n",
            "t = 7, avg_loss = 0.2419\n",
            "t = 8, avg_loss = 0.2270\n",
            "t = 9, avg_loss = 0.2685\n",
            "t = 10, avg_loss = 0.3481\n",
            "t = 11, avg_loss = 0.3104\n",
            "t = 12, avg_loss = 0.3808\n",
            "t = 13, avg_loss = 0.2254\n",
            "t = 14, avg_loss = 0.1937\n",
            "t = 15, avg_loss = 0.3771\n",
            "t = 16, avg_loss = 0.3171\n",
            "t = 17, avg_loss = 0.3299\n",
            "t = 18, avg_loss = 0.2315\n",
            "t = 19, avg_loss = 0.1763\n",
            "t = 20, avg_loss = 0.3721\n",
            "t = 21, avg_loss = 0.2092\n",
            "t = 22, avg_loss = 0.3962\n",
            "t = 23, avg_loss = 0.1537\n",
            "t = 24, avg_loss = 0.4596\n",
            "t = 25, avg_loss = 0.4220\n",
            "t = 26, avg_loss = 0.3386\n",
            "t = 27, avg_loss = 0.3595\n",
            "t = 28, avg_loss = 0.3416\n",
            "t = 29, avg_loss = 0.2133\n",
            "t = 30, avg_loss = 0.3529\n",
            "Checking accuracy on test set\n",
            "Got 173 / 240 correct (72.08)\n",
            "acc = 0.720833\n",
            "Starting epoch 49 / 50\n",
            "t = 1, avg_loss = 0.2961\n",
            "t = 2, avg_loss = 0.2740\n",
            "t = 3, avg_loss = 0.4118\n",
            "t = 4, avg_loss = 0.4293\n",
            "t = 5, avg_loss = 0.2312\n",
            "t = 6, avg_loss = 0.3381\n",
            "t = 7, avg_loss = 0.3361\n",
            "t = 8, avg_loss = 0.2894\n",
            "t = 9, avg_loss = 0.2603\n",
            "t = 10, avg_loss = 0.3337\n",
            "t = 11, avg_loss = 0.4294\n",
            "t = 12, avg_loss = 0.3563\n",
            "t = 13, avg_loss = 0.2618\n",
            "t = 14, avg_loss = 0.3551\n",
            "t = 15, avg_loss = 0.2416\n",
            "t = 16, avg_loss = 0.4204\n",
            "t = 17, avg_loss = 0.3636\n",
            "t = 18, avg_loss = 0.1742\n",
            "t = 19, avg_loss = 0.3370\n",
            "t = 20, avg_loss = 0.3322\n",
            "t = 21, avg_loss = 0.5033\n",
            "t = 22, avg_loss = 0.2835\n",
            "t = 23, avg_loss = 0.2608\n",
            "t = 24, avg_loss = 0.3920\n",
            "t = 25, avg_loss = 0.4673\n",
            "t = 26, avg_loss = 0.2792\n",
            "t = 27, avg_loss = 0.2632\n",
            "t = 28, avg_loss = 0.2980\n",
            "t = 29, avg_loss = 0.3663\n",
            "t = 30, avg_loss = 0.2724\n",
            "Checking accuracy on test set\n",
            "Got 207 / 240 correct (86.25)\n",
            "acc = 0.862500\n",
            "Starting epoch 50 / 50\n",
            "t = 1, avg_loss = 0.3201\n",
            "t = 2, avg_loss = 0.4112\n",
            "t = 3, avg_loss = 0.5119\n",
            "t = 4, avg_loss = 0.2923\n",
            "t = 5, avg_loss = 0.2202\n",
            "t = 6, avg_loss = 0.2254\n",
            "t = 7, avg_loss = 0.3495\n",
            "t = 8, avg_loss = 0.3103\n",
            "t = 9, avg_loss = 0.4803\n",
            "t = 10, avg_loss = 0.3762\n",
            "t = 11, avg_loss = 0.3691\n",
            "t = 12, avg_loss = 0.2965\n",
            "t = 13, avg_loss = 0.2701\n",
            "t = 14, avg_loss = 0.2514\n",
            "t = 15, avg_loss = 0.3290\n",
            "t = 16, avg_loss = 0.2823\n",
            "t = 17, avg_loss = 0.3259\n",
            "t = 18, avg_loss = 0.2564\n",
            "t = 19, avg_loss = 0.3273\n",
            "t = 20, avg_loss = 0.3128\n",
            "t = 21, avg_loss = 0.3014\n",
            "t = 22, avg_loss = 0.3459\n",
            "t = 23, avg_loss = 0.3363\n",
            "t = 24, avg_loss = 0.2336\n",
            "t = 25, avg_loss = 0.2083\n",
            "t = 26, avg_loss = 0.4190\n",
            "t = 27, avg_loss = 0.5553\n",
            "t = 28, avg_loss = 0.3365\n",
            "t = 29, avg_loss = 0.2590\n",
            "t = 30, avg_loss = 0.4739\n",
            "Checking accuracy on test set\n",
            "Got 203 / 240 correct (84.58)\n",
            "acc = 0.845833\n",
            "Checking accuracy on test set\n",
            "Got 203 / 240 correct (84.58)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8458333333333333"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDn-2g-TLY3k",
        "colab_type": "code",
        "outputId": "4f1bb8a1-76c0-4326-996b-769d8d761bca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot([print_every*batch_size*(i+1) for i in range((len(avg_loss_list)))],avg_loss_list)\n",
        "print(\"Loss:\")\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD4CAYAAAAKA1qZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUw+ZAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGer6hJv7-zR",
        "colab_type": "code",
        "outputId": "b18be421-370b-4251-d79a-bf463ecf476f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "plt.plot([i+1 for i in range((len(acc_list)))],acc_list)\n",
        "print(\"Accurancy:\")\n",
        "plt.show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accurancy:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQM8wDKNz/n/VYLzv5g2QDwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBTReWHxz4Ml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}