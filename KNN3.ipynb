{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KNN3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPsDMhDcKJru0RiJwjYw3Mb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kamilo116/KNN/blob/master/KNN3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7ReM7FU-xs4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import os\n",
        "import csv\n",
        "import sys\n",
        "from random import shuffle\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from matplotlib.pyplot import imshow\n",
        "%matplotlib inline\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import sampler\n",
        "import torchvision\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "import timeit\n",
        "\n",
        "np.random.seed(4) \n",
        "torch.manual_seed(4) \n",
        "torch.cuda.manual_seed(4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbkMzE2e7DTW",
        "colab_type": "code",
        "outputId": "95344577-8b2c-4057-c5b2-50150b74b224",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMOHez_kHZD6",
        "colab_type": "code",
        "outputId": "dc13b317-70d2-49aa-db5b-156277b4ce82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive, files\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYIMZ8QyYewD",
        "colab_type": "code",
        "outputId": "25f42f5e-7aa4-42df-ce0a-646720bb1089",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "! git clone https://github.com/wang-chen/kervolution.git \n",
        "\n",
        "sys.path.append(\"kervolution/\")\n",
        "from kervolution import Kerv2d\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'kervolution'...\n",
            "remote: Enumerating objects: 6, done.\u001b[K\n",
            "remote: Counting objects:  16% (1/6)\u001b[K\rremote: Counting objects:  33% (2/6)\u001b[K\rremote: Counting objects:  50% (3/6)\u001b[K\rremote: Counting objects:  66% (4/6)\u001b[K\rremote: Counting objects:  83% (5/6)\u001b[K\rremote: Counting objects: 100% (6/6)\u001b[K\rremote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects:  16% (1/6)\u001b[K\rremote: Compressing objects:  33% (2/6)\u001b[K\rremote: Compressing objects:  50% (3/6)\u001b[K\rremote: Compressing objects:  66% (4/6)\u001b[K\rremote: Compressing objects:  83% (5/6)\u001b[K\rremote: Compressing objects: 100% (6/6)\u001b[K\rremote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "Unpacking objects:   1% (1/53)   \rUnpacking objects:   3% (2/53)   \rUnpacking objects:   5% (3/53)   \rUnpacking objects:   7% (4/53)   \rUnpacking objects:   9% (5/53)   \rUnpacking objects:  11% (6/53)   \rUnpacking objects:  13% (7/53)   \rUnpacking objects:  15% (8/53)   \rUnpacking objects:  16% (9/53)   \rUnpacking objects:  18% (10/53)   \rUnpacking objects:  20% (11/53)   \rUnpacking objects:  22% (12/53)   \rUnpacking objects:  24% (13/53)   \rUnpacking objects:  26% (14/53)   \rUnpacking objects:  28% (15/53)   \rUnpacking objects:  30% (16/53)   \rUnpacking objects:  32% (17/53)   \rUnpacking objects:  33% (18/53)   \rremote: Total 53 (delta 2), reused 0 (delta 0), pack-reused 47\u001b[K\n",
            "Unpacking objects:  35% (19/53)   \rUnpacking objects:  37% (20/53)   \rUnpacking objects:  39% (21/53)   \rUnpacking objects:  41% (22/53)   \rUnpacking objects:  43% (23/53)   \rUnpacking objects:  45% (24/53)   \rUnpacking objects:  47% (25/53)   \rUnpacking objects:  49% (26/53)   \rUnpacking objects:  50% (27/53)   \rUnpacking objects:  52% (28/53)   \rUnpacking objects:  54% (29/53)   \rUnpacking objects:  56% (30/53)   \rUnpacking objects:  58% (31/53)   \rUnpacking objects:  60% (32/53)   \rUnpacking objects:  62% (33/53)   \rUnpacking objects:  64% (34/53)   \rUnpacking objects:  66% (35/53)   \rUnpacking objects:  67% (36/53)   \rUnpacking objects:  69% (37/53)   \rUnpacking objects:  71% (38/53)   \rUnpacking objects:  73% (39/53)   \rUnpacking objects:  75% (40/53)   \rUnpacking objects:  77% (41/53)   \rUnpacking objects:  79% (42/53)   \rUnpacking objects:  81% (43/53)   \rUnpacking objects:  83% (44/53)   \rUnpacking objects:  84% (45/53)   \rUnpacking objects:  86% (46/53)   \rUnpacking objects:  88% (47/53)   \rUnpacking objects:  90% (48/53)   \rUnpacking objects:  92% (49/53)   \rUnpacking objects:  94% (50/53)   \rUnpacking objects:  96% (51/53)   \rUnpacking objects:  98% (52/53)   \rUnpacking objects: 100% (53/53)   \rUnpacking objects: 100% (53/53), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b8P1NdXfVdX",
        "colab_type": "code",
        "outputId": "089dc4f7-3a50-444b-8d50-0d45eca795ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "MALIGNANT_DATASET = '/content/drive/My Drive/Colab_data/malignant/malignant/'\n",
        "BENIGN_DATASET = '/content/drive/My Drive/Colab_data/benign/benign/'\n",
        "DATA_FOLDER = '/content/drive/My Drive/Colab_data/'\n",
        "LIMIT_IMAGES_NUM = 600\n",
        "\n",
        "benign_file_list = sorted(os.listdir(BENIGN_DATASET))[:LIMIT_IMAGES_NUM]\n",
        "malignant_file_list = sorted(os.listdir(MALIGNANT_DATASET))[:LIMIT_IMAGES_NUM]\n",
        "shuffle(benign_file_list)\n",
        "shuffle(malignant_file_list)\n",
        "\n",
        "print(f\"Number of benign {len(benign_file_list)} images\")\n",
        "print(f\"Number of malignant {len(malignant_file_list)} images\")\n",
        "\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize((100, 100)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of benign 600 images\n",
            "Number of malignant 600 images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGDQfal74BLi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "benign_dict = {filename: 0 for filename in benign_file_list}\n",
        "malignant_dict = {filename: 1 for filename in malignant_file_list}\n",
        "img_class_dict = {**benign_dict , **malignant_dict}\n",
        "labeled_data = pd.Series(img_class_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvAAOqmVfVll",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class IsicDataset(Dataset):\n",
        "    def __init__(self, data_folder, labeled_data, \n",
        "                 transform=transforms.Compose([transforms.ToTensor()])):\n",
        "        self.labeled_data = labeled_data\n",
        "        self.transform = transform\n",
        "        self.data_folder = data_folder\n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.labeled_data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        label = self.labeled_data[index]\n",
        "        if label == 0:\n",
        "          image = Image.open(os.path.join(self.data_folder, \"benign\", \"benign\", index ))\n",
        "        else:\n",
        "          image = Image.open(os.path.join(self.data_folder, \"malignant\", \"malignant\", index ))\n",
        "        image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "    @property\n",
        "    def labels(self):\n",
        "      return self.labeled_data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSuYqjLAYrWA",
        "colab_type": "code",
        "outputId": "62f28281-bb53-437b-fc82-fe90bc564afc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        " \n",
        "dataset = IsicDataset(DATA_FOLDER, labeled_data, transform=data_transforms)\n",
        "print(dataset.labels)\n",
        "\n",
        "X_train, X_test = train_test_split(dataset.labels, test_size=0.2)\n",
        "print(\"number of training data: \",len(X_train))\n",
        "print(\"number of testing  data: \",len(X_test))\n",
        "\n",
        "train_sampler = SubsetRandomSampler(list(X_train.index))\n",
        "valid_sampler = SubsetRandomSampler(list(X_test.index))\n",
        "batch_size = 32\n",
        "num_workers = 0\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers)\n",
        "valid_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler, num_workers=num_workers)\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ISIC_0000266.jpeg    0\n",
            "ISIC_0000187.jpeg    0\n",
            "ISIC_0000208.jpeg    0\n",
            "ISIC_0000229.jpeg    0\n",
            "ISIC_0000082.jpeg    0\n",
            "                    ..\n",
            "ISIC_0010537.jpeg    1\n",
            "ISIC_0000277.jpeg    1\n",
            "ISIC_0000445.jpeg    1\n",
            "ISIC_0011369.jpeg    1\n",
            "ISIC_0000166.jpeg    1\n",
            "Length: 1200, dtype: int64\n",
            "number of training data:  960\n",
            "number of testing  data:  240\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_e8nFBR6Yug5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "avg_loss_list = []\n",
        "acc_list = []\n",
        "\n",
        "def train(model, train_loader ,loss_fn, optimizer, num_epochs = 1):\n",
        "    total_loss =0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Starting epoch %d / %d' % (epoch + 1, num_epochs))\n",
        "        model.train()\n",
        "\n",
        "        for t, (x, y) in enumerate(train_loader):\n",
        "            x_var = Variable(x.type(gpu_dtype))\n",
        "            y_var = Variable(y.type(gpu_dtype).long())\n",
        "            scores = model(x_var)\n",
        "            loss = loss_fn(scores, y_var)\n",
        "            total_loss += loss.data\n",
        "            \n",
        "            if (t + 1) % print_every == 0:\n",
        "                avg_loss = total_loss/print_every\n",
        "                print('t = %d, avg_loss = %.4f' % (t + 1, avg_loss) )\n",
        "                avg_loss_list.append(avg_loss)\n",
        "                total_loss = 0\n",
        "                \n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        acc = check_accuracy(fixed_model_gpu, valid_loader)\n",
        "        print('acc = %f' %(acc))\n",
        "            \n",
        "def check_accuracy(model, loader):\n",
        "    print('Checking accuracy on test set')   \n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval() \n",
        "    for x, y in loader:\n",
        "        x_var = Variable(x.type(gpu_dtype))\n",
        "\n",
        "        scores = model(x_var)\n",
        "        _, preds = scores.data.cpu().max(1)\n",
        "        num_correct += (preds == y).sum()\n",
        "        num_samples += preds.size(0)\n",
        "    acc = float(num_correct) / num_samples\n",
        "    acc_list.append(acc)\n",
        "    print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
        "    return acc\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggvpMu36Yw2D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Flatten(nn.Module):\n",
        "    def forward(self, x):\n",
        "        N, C, H, W = x.size()\n",
        "        return x.view(N, -1)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgmexhBRZAK6",
        "colab_type": "code",
        "outputId": "a0a6d834-cc70-422b-f119-eac1e42c3fb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "\n",
        "print_every = 1\n",
        "gpu_dtype = torch.cuda.FloatTensor\n",
        "\n",
        "out_0 = 90\n",
        "out_1 = 64\n",
        "out_2 = 48\n",
        "out_3 = 32\n",
        "out_4 = 24\n",
        "out_5 = 16\n",
        "\n",
        "k_size_1 = 3\n",
        "padding_1 = 1\n",
        "in_channels = 3\n",
        "\n",
        "num_epochs = 50\n",
        "\n",
        "'''\n",
        "Kerv2d\n",
        "kervolution with following options:\n",
        "kernel_type: [linear, polynomial, gaussian, etc.]\n",
        "default is convolution:\n",
        "          kernel_type --> linear,\n",
        "balance, power, gamma is valid only when the kernel_type is specified\n",
        "if learnable_kernel = True,  they just be the initial value of learable parameters\n",
        "if learnable_kernel = False, they are the value of kernel_type's parameter\n",
        "the parameter [power] cannot be learned due to integer limitation\n",
        "balance: 0, 1\n",
        "power: 3, 4, 5\n",
        "gamma:\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "fixed_model_base = nn.Sequential( \n",
        "                nn.Kerv2d(in_channels , out_0, padding= padding_1, kernel_type='gaussian', kernel_size=k_size_1, learnable_kernel=True,\n",
        "                          kernel_regularizer=True, stride=1), \n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.BatchNorm2d(out_0),\n",
        "                nn.Conv2d(out_0 , out_1, padding= padding_1, kernel_size=k_size_1, stride=1), \n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.BatchNorm2d(out_1),\n",
        "                nn.MaxPool2d(2, stride=2),\n",
        "                nn.Conv2d(out_1 , out_2, padding= padding_1, kernel_size=k_size_1, stride=1), \n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.BatchNorm2d(out_2),\n",
        "                nn.MaxPool2d(2, stride=2),\n",
        "                nn.Conv2d(out_2 , out_3, padding= padding_1, kernel_size=k_size_1, stride=1),  \n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.BatchNorm2d(out_3),\n",
        "                nn.MaxPool2d(2, stride=2),\n",
        "                nn.Conv2d(out_3 , out_4, padding= padding_1, kernel_size=k_size_1, stride=1), \n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.BatchNorm2d(out_4),\n",
        "                #nn.Conv2d(out_4 , out_5, padding= padding_1, kernel_size=k_size_1, stride=1), \n",
        "                #nn.ReLU(inplace=True),\n",
        "                #nn.BatchNorm2d(out_5),\n",
        "                nn.MaxPool2d(2, stride=2),\n",
        "                nn.Dropout(0.5),\n",
        "                Flatten(),\n",
        "                nn.Linear(864,64),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Linear(64,10),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Linear(10,2)\n",
        "            )\n",
        "fixed_model_gpu = fixed_model_base.type(gpu_dtype)\n",
        "print(fixed_model_gpu)\n",
        "loss_fn = nn.modules.loss.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(fixed_model_gpu.parameters(), lr = 0.0011, weight_decay=0.05) \n",
        "\n",
        "train(fixed_model_gpu, train_loader ,loss_fn, optimizer, num_epochs=num_epochs)\n",
        "check_accuracy(fixed_model_gpu, valid_loader)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Kerv2d(3, 90, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (1): ReLU(inplace=True)\n",
            "  (2): BatchNorm2d(90, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (3): Conv2d(90, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (4): ReLU(inplace=True)\n",
            "  (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (7): Conv2d(64, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (8): ReLU(inplace=True)\n",
            "  (9): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (11): Conv2d(48, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (12): ReLU(inplace=True)\n",
            "  (13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (15): Conv2d(32, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (16): ReLU(inplace=True)\n",
            "  (17): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (19): Dropout(p=0.5, inplace=False)\n",
            "  (20): Flatten()\n",
            "  (21): Linear(in_features=864, out_features=64, bias=True)\n",
            "  (22): ReLU(inplace=True)\n",
            "  (23): Linear(in_features=64, out_features=10, bias=True)\n",
            "  (24): ReLU(inplace=True)\n",
            "  (25): Linear(in_features=10, out_features=2, bias=True)\n",
            ")\n",
            "Starting epoch 1 / 50\n",
            "t = 1, avg_loss = 0.7472\n",
            "t = 2, avg_loss = 0.7028\n",
            "t = 3, avg_loss = 0.6311\n",
            "t = 4, avg_loss = 0.6560\n",
            "t = 5, avg_loss = 0.7151\n",
            "t = 6, avg_loss = 0.6784\n",
            "t = 7, avg_loss = 0.7156\n",
            "t = 8, avg_loss = 0.5981\n",
            "t = 9, avg_loss = 0.6554\n",
            "t = 10, avg_loss = 0.5819\n",
            "t = 11, avg_loss = 0.6768\n",
            "t = 12, avg_loss = 0.6214\n",
            "t = 13, avg_loss = 0.6349\n",
            "t = 14, avg_loss = 0.5663\n",
            "t = 15, avg_loss = 0.6709\n",
            "t = 16, avg_loss = 0.5969\n",
            "t = 17, avg_loss = 0.5789\n",
            "t = 18, avg_loss = 0.5997\n",
            "t = 19, avg_loss = 0.5670\n",
            "t = 20, avg_loss = 0.5848\n",
            "t = 21, avg_loss = 0.6657\n",
            "t = 22, avg_loss = 0.6453\n",
            "t = 23, avg_loss = 0.5371\n",
            "t = 24, avg_loss = 0.5135\n",
            "t = 25, avg_loss = 0.6192\n",
            "t = 26, avg_loss = 0.6035\n",
            "t = 27, avg_loss = 0.6503\n",
            "t = 28, avg_loss = 0.6410\n",
            "t = 29, avg_loss = 0.4831\n",
            "t = 30, avg_loss = 0.8074\n",
            "Checking accuracy on test set\n",
            "Got 118 / 240 correct (49.17)\n",
            "acc = 0.491667\n",
            "Starting epoch 2 / 50\n",
            "t = 1, avg_loss = 0.5854\n",
            "t = 2, avg_loss = 0.6850\n",
            "t = 3, avg_loss = 0.5344\n",
            "t = 4, avg_loss = 0.5476\n",
            "t = 5, avg_loss = 0.5860\n",
            "t = 6, avg_loss = 0.6562\n",
            "t = 7, avg_loss = 0.6412\n",
            "t = 8, avg_loss = 0.4830\n",
            "t = 9, avg_loss = 0.6156\n",
            "t = 10, avg_loss = 0.6006\n",
            "t = 11, avg_loss = 0.6046\n",
            "t = 12, avg_loss = 0.5193\n",
            "t = 13, avg_loss = 0.6690\n",
            "t = 14, avg_loss = 0.6621\n",
            "t = 15, avg_loss = 0.5704\n",
            "t = 16, avg_loss = 0.5540\n",
            "t = 17, avg_loss = 0.5205\n",
            "t = 18, avg_loss = 0.5343\n",
            "t = 19, avg_loss = 0.5361\n",
            "t = 20, avg_loss = 0.5047\n",
            "t = 21, avg_loss = 0.4421\n",
            "t = 22, avg_loss = 0.6055\n",
            "t = 23, avg_loss = 0.7103\n",
            "t = 24, avg_loss = 0.6159\n",
            "t = 25, avg_loss = 0.5333\n",
            "t = 26, avg_loss = 0.5549\n",
            "t = 27, avg_loss = 0.6498\n",
            "t = 28, avg_loss = 0.5779\n",
            "t = 29, avg_loss = 0.6130\n",
            "t = 30, avg_loss = 0.4641\n",
            "Checking accuracy on test set\n",
            "Got 147 / 240 correct (61.25)\n",
            "acc = 0.612500\n",
            "Starting epoch 3 / 50\n",
            "t = 1, avg_loss = 0.6782\n",
            "t = 2, avg_loss = 0.6023\n",
            "t = 3, avg_loss = 0.4862\n",
            "t = 4, avg_loss = 0.6282\n",
            "t = 5, avg_loss = 0.5740\n",
            "t = 6, avg_loss = 0.5512\n",
            "t = 7, avg_loss = 0.6002\n",
            "t = 8, avg_loss = 0.5718\n",
            "t = 9, avg_loss = 0.6055\n",
            "t = 10, avg_loss = 0.5905\n",
            "t = 11, avg_loss = 0.5866\n",
            "t = 12, avg_loss = 0.5587\n",
            "t = 13, avg_loss = 0.6437\n",
            "t = 14, avg_loss = 0.6613\n",
            "t = 15, avg_loss = 0.5995\n",
            "t = 16, avg_loss = 0.4755\n",
            "t = 17, avg_loss = 0.6118\n",
            "t = 18, avg_loss = 0.5605\n",
            "t = 19, avg_loss = 0.5148\n",
            "t = 20, avg_loss = 0.4443\n",
            "t = 21, avg_loss = 0.6231\n",
            "t = 22, avg_loss = 0.6171\n",
            "t = 23, avg_loss = 0.5966\n",
            "t = 24, avg_loss = 0.6050\n",
            "t = 25, avg_loss = 0.6171\n",
            "t = 26, avg_loss = 0.5045\n",
            "t = 27, avg_loss = 0.4625\n",
            "t = 28, avg_loss = 0.6116\n",
            "t = 29, avg_loss = 0.6928\n",
            "t = 30, avg_loss = 0.5082\n",
            "Checking accuracy on test set\n",
            "Got 118 / 240 correct (49.17)\n",
            "acc = 0.491667\n",
            "Starting epoch 4 / 50\n",
            "t = 1, avg_loss = 0.5353\n",
            "t = 2, avg_loss = 0.6333\n",
            "t = 3, avg_loss = 0.6171\n",
            "t = 4, avg_loss = 0.5972\n",
            "t = 5, avg_loss = 0.5737\n",
            "t = 6, avg_loss = 0.6092\n",
            "t = 7, avg_loss = 0.6653\n",
            "t = 8, avg_loss = 0.5612\n",
            "t = 9, avg_loss = 0.5406\n",
            "t = 10, avg_loss = 0.5478\n",
            "t = 11, avg_loss = 0.6413\n",
            "t = 12, avg_loss = 0.5154\n",
            "t = 13, avg_loss = 0.4929\n",
            "t = 14, avg_loss = 0.5640\n",
            "t = 15, avg_loss = 0.5703\n",
            "t = 16, avg_loss = 0.4844\n",
            "t = 17, avg_loss = 0.5661\n",
            "t = 18, avg_loss = 0.4014\n",
            "t = 19, avg_loss = 0.5969\n",
            "t = 20, avg_loss = 0.6597\n",
            "t = 21, avg_loss = 0.6614\n",
            "t = 22, avg_loss = 0.5061\n",
            "t = 23, avg_loss = 0.4898\n",
            "t = 24, avg_loss = 0.5781\n",
            "t = 25, avg_loss = 0.6338\n",
            "t = 26, avg_loss = 0.5676\n",
            "t = 27, avg_loss = 0.5758\n",
            "t = 28, avg_loss = 0.5731\n",
            "t = 29, avg_loss = 0.6448\n",
            "t = 30, avg_loss = 0.5858\n",
            "Checking accuracy on test set\n",
            "Got 124 / 240 correct (51.67)\n",
            "acc = 0.516667\n",
            "Starting epoch 5 / 50\n",
            "t = 1, avg_loss = 0.5171\n",
            "t = 2, avg_loss = 0.5523\n",
            "t = 3, avg_loss = 0.5206\n",
            "t = 4, avg_loss = 0.5157\n",
            "t = 5, avg_loss = 0.5075\n",
            "t = 6, avg_loss = 0.7426\n",
            "t = 7, avg_loss = 0.4898\n",
            "t = 8, avg_loss = 0.5476\n",
            "t = 9, avg_loss = 0.6135\n",
            "t = 10, avg_loss = 0.4530\n",
            "t = 11, avg_loss = 0.4730\n",
            "t = 12, avg_loss = 0.4848\n",
            "t = 13, avg_loss = 0.5995\n",
            "t = 14, avg_loss = 0.5126\n",
            "t = 15, avg_loss = 0.5217\n",
            "t = 16, avg_loss = 0.5199\n",
            "t = 17, avg_loss = 0.5025\n",
            "t = 18, avg_loss = 0.6303\n",
            "t = 19, avg_loss = 0.5545\n",
            "t = 20, avg_loss = 0.5431\n",
            "t = 21, avg_loss = 0.5214\n",
            "t = 22, avg_loss = 0.6307\n",
            "t = 23, avg_loss = 0.4941\n",
            "t = 24, avg_loss = 0.5967\n",
            "t = 25, avg_loss = 0.4754\n",
            "t = 26, avg_loss = 0.6769\n",
            "t = 27, avg_loss = 0.5078\n",
            "t = 28, avg_loss = 0.4380\n",
            "t = 29, avg_loss = 0.5197\n",
            "t = 30, avg_loss = 0.4849\n",
            "Checking accuracy on test set\n",
            "Got 160 / 240 correct (66.67)\n",
            "acc = 0.666667\n",
            "Starting epoch 6 / 50\n",
            "t = 1, avg_loss = 0.5012\n",
            "t = 2, avg_loss = 0.5302\n",
            "t = 3, avg_loss = 0.6301\n",
            "t = 4, avg_loss = 0.4994\n",
            "t = 5, avg_loss = 0.4807\n",
            "t = 6, avg_loss = 0.6084\n",
            "t = 7, avg_loss = 0.4638\n",
            "t = 8, avg_loss = 0.5561\n",
            "t = 9, avg_loss = 0.5372\n",
            "t = 10, avg_loss = 0.5367\n",
            "t = 11, avg_loss = 0.4253\n",
            "t = 12, avg_loss = 0.5495\n",
            "t = 13, avg_loss = 0.5705\n",
            "t = 14, avg_loss = 0.6197\n",
            "t = 15, avg_loss = 0.5799\n",
            "t = 16, avg_loss = 0.6022\n",
            "t = 17, avg_loss = 0.5237\n",
            "t = 18, avg_loss = 0.4551\n",
            "t = 19, avg_loss = 0.7031\n",
            "t = 20, avg_loss = 0.5111\n",
            "t = 21, avg_loss = 0.5455\n",
            "t = 22, avg_loss = 0.5177\n",
            "t = 23, avg_loss = 0.6434\n",
            "t = 24, avg_loss = 0.6505\n",
            "t = 25, avg_loss = 0.8472\n",
            "t = 26, avg_loss = 0.6214\n",
            "t = 27, avg_loss = 0.7070\n",
            "t = 28, avg_loss = 0.5806\n",
            "t = 29, avg_loss = 0.6198\n",
            "t = 30, avg_loss = 0.5626\n",
            "Checking accuracy on test set\n",
            "Got 143 / 240 correct (59.58)\n",
            "acc = 0.595833\n",
            "Starting epoch 7 / 50\n",
            "t = 1, avg_loss = 0.6180\n",
            "t = 2, avg_loss = 0.5420\n",
            "t = 3, avg_loss = 0.6185\n",
            "t = 4, avg_loss = 0.5604\n",
            "t = 5, avg_loss = 0.6067\n",
            "t = 6, avg_loss = 0.5231\n",
            "t = 7, avg_loss = 0.5860\n",
            "t = 8, avg_loss = 0.5760\n",
            "t = 9, avg_loss = 0.5469\n",
            "t = 10, avg_loss = 0.4805\n",
            "t = 11, avg_loss = 0.5581\n",
            "t = 12, avg_loss = 0.6065\n",
            "t = 13, avg_loss = 0.6549\n",
            "t = 14, avg_loss = 0.5121\n",
            "t = 15, avg_loss = 0.5933\n",
            "t = 16, avg_loss = 0.6589\n",
            "t = 17, avg_loss = 0.6026\n",
            "t = 18, avg_loss = 0.5465\n",
            "t = 19, avg_loss = 0.5402\n",
            "t = 20, avg_loss = 0.5158\n",
            "t = 21, avg_loss = 0.5131\n",
            "t = 22, avg_loss = 0.4703\n",
            "t = 23, avg_loss = 0.5905\n",
            "t = 24, avg_loss = 0.6673\n",
            "t = 25, avg_loss = 0.5177\n",
            "t = 26, avg_loss = 0.4727\n",
            "t = 27, avg_loss = 0.5836\n",
            "t = 28, avg_loss = 0.4670\n",
            "t = 29, avg_loss = 0.4597\n",
            "t = 30, avg_loss = 0.4957\n",
            "Checking accuracy on test set\n",
            "Got 117 / 240 correct (48.75)\n",
            "acc = 0.487500\n",
            "Starting epoch 8 / 50\n",
            "t = 1, avg_loss = 0.4942\n",
            "t = 2, avg_loss = 0.4438\n",
            "t = 3, avg_loss = 0.4953\n",
            "t = 4, avg_loss = 0.5735\n",
            "t = 5, avg_loss = 0.5743\n",
            "t = 6, avg_loss = 0.5518\n",
            "t = 7, avg_loss = 0.5556\n",
            "t = 8, avg_loss = 0.6727\n",
            "t = 9, avg_loss = 0.5456\n",
            "t = 10, avg_loss = 0.4848\n",
            "t = 11, avg_loss = 0.5974\n",
            "t = 12, avg_loss = 0.5548\n",
            "t = 13, avg_loss = 0.5201\n",
            "t = 14, avg_loss = 0.6053\n",
            "t = 15, avg_loss = 0.5360\n",
            "t = 16, avg_loss = 0.4739\n",
            "t = 17, avg_loss = 0.4247\n",
            "t = 18, avg_loss = 0.5226\n",
            "t = 19, avg_loss = 0.6053\n",
            "t = 20, avg_loss = 0.5807\n",
            "t = 21, avg_loss = 0.4963\n",
            "t = 22, avg_loss = 0.7143\n",
            "t = 23, avg_loss = 0.5168\n",
            "t = 24, avg_loss = 0.6032\n",
            "t = 25, avg_loss = 0.5174\n",
            "t = 26, avg_loss = 0.5287\n",
            "t = 27, avg_loss = 0.5001\n",
            "t = 28, avg_loss = 0.5664\n",
            "t = 29, avg_loss = 0.5429\n",
            "t = 30, avg_loss = 0.6594\n",
            "Checking accuracy on test set\n",
            "Got 169 / 240 correct (70.42)\n",
            "acc = 0.704167\n",
            "Starting epoch 9 / 50\n",
            "t = 1, avg_loss = 0.4922\n",
            "t = 2, avg_loss = 0.4693\n",
            "t = 3, avg_loss = 0.5666\n",
            "t = 4, avg_loss = 0.5021\n",
            "t = 5, avg_loss = 0.5085\n",
            "t = 6, avg_loss = 0.5219\n",
            "t = 7, avg_loss = 0.6837\n",
            "t = 8, avg_loss = 0.5613\n",
            "t = 9, avg_loss = 0.5691\n",
            "t = 10, avg_loss = 0.4999\n",
            "t = 11, avg_loss = 0.5026\n",
            "t = 12, avg_loss = 0.5128\n",
            "t = 13, avg_loss = 0.5386\n",
            "t = 14, avg_loss = 0.6558\n",
            "t = 15, avg_loss = 0.5353\n",
            "t = 16, avg_loss = 0.6393\n",
            "t = 17, avg_loss = 0.5650\n",
            "t = 18, avg_loss = 0.4951\n",
            "t = 19, avg_loss = 0.5092\n",
            "t = 20, avg_loss = 0.4886\n",
            "t = 21, avg_loss = 0.5594\n",
            "t = 22, avg_loss = 0.4952\n",
            "t = 23, avg_loss = 0.4503\n",
            "t = 24, avg_loss = 0.4987\n",
            "t = 25, avg_loss = 0.4893\n",
            "t = 26, avg_loss = 0.5596\n",
            "t = 27, avg_loss = 0.5965\n",
            "t = 28, avg_loss = 0.4527\n",
            "t = 29, avg_loss = 0.5749\n",
            "t = 30, avg_loss = 0.6143\n",
            "Checking accuracy on test set\n",
            "Got 118 / 240 correct (49.17)\n",
            "acc = 0.491667\n",
            "Starting epoch 10 / 50\n",
            "t = 1, avg_loss = 0.6454\n",
            "t = 2, avg_loss = 0.5018\n",
            "t = 3, avg_loss = 0.5041\n",
            "t = 4, avg_loss = 0.5067\n",
            "t = 5, avg_loss = 0.5895\n",
            "t = 6, avg_loss = 0.6159\n",
            "t = 7, avg_loss = 0.4962\n",
            "t = 8, avg_loss = 0.5455\n",
            "t = 9, avg_loss = 0.5534\n",
            "t = 10, avg_loss = 0.6267\n",
            "t = 11, avg_loss = 0.6028\n",
            "t = 12, avg_loss = 0.6019\n",
            "t = 13, avg_loss = 0.4080\n",
            "t = 14, avg_loss = 0.5277\n",
            "t = 15, avg_loss = 0.4581\n",
            "t = 16, avg_loss = 0.5554\n",
            "t = 17, avg_loss = 0.5328\n",
            "t = 18, avg_loss = 0.5439\n",
            "t = 19, avg_loss = 0.4988\n",
            "t = 20, avg_loss = 0.5258\n",
            "t = 21, avg_loss = 0.5656\n",
            "t = 22, avg_loss = 0.5488\n",
            "t = 23, avg_loss = 0.5677\n",
            "t = 24, avg_loss = 0.5163\n",
            "t = 25, avg_loss = 0.6388\n",
            "t = 26, avg_loss = 0.4832\n",
            "t = 27, avg_loss = 0.5663\n",
            "t = 28, avg_loss = 0.5494\n",
            "t = 29, avg_loss = 0.5020\n",
            "t = 30, avg_loss = 0.5115\n",
            "Checking accuracy on test set\n",
            "Got 164 / 240 correct (68.33)\n",
            "acc = 0.683333\n",
            "Starting epoch 11 / 50\n",
            "t = 1, avg_loss = 0.5561\n",
            "t = 2, avg_loss = 0.6127\n",
            "t = 3, avg_loss = 0.6330\n",
            "t = 4, avg_loss = 0.4598\n",
            "t = 5, avg_loss = 0.4593\n",
            "t = 6, avg_loss = 0.4457\n",
            "t = 7, avg_loss = 0.5913\n",
            "t = 8, avg_loss = 0.5845\n",
            "t = 9, avg_loss = 0.5749\n",
            "t = 10, avg_loss = 0.6779\n",
            "t = 11, avg_loss = 0.5494\n",
            "t = 12, avg_loss = 0.5189\n",
            "t = 13, avg_loss = 0.4289\n",
            "t = 14, avg_loss = 0.4764\n",
            "t = 15, avg_loss = 0.5430\n",
            "t = 16, avg_loss = 0.5547\n",
            "t = 17, avg_loss = 0.5600\n",
            "t = 18, avg_loss = 0.5542\n",
            "t = 19, avg_loss = 0.5889\n",
            "t = 20, avg_loss = 0.6212\n",
            "t = 21, avg_loss = 0.4489\n",
            "t = 22, avg_loss = 0.5660\n",
            "t = 23, avg_loss = 0.4932\n",
            "t = 24, avg_loss = 0.5121\n",
            "t = 25, avg_loss = 0.5570\n",
            "t = 26, avg_loss = 0.4744\n",
            "t = 27, avg_loss = 0.5272\n",
            "t = 28, avg_loss = 0.4783\n",
            "t = 29, avg_loss = 0.5665\n",
            "t = 30, avg_loss = 0.5769\n",
            "Checking accuracy on test set\n",
            "Got 119 / 240 correct (49.58)\n",
            "acc = 0.495833\n",
            "Starting epoch 12 / 50\n",
            "t = 1, avg_loss = 0.4435\n",
            "t = 2, avg_loss = 0.6896\n",
            "t = 3, avg_loss = 0.4553\n",
            "t = 4, avg_loss = 0.5193\n",
            "t = 5, avg_loss = 0.5918\n",
            "t = 6, avg_loss = 0.5701\n",
            "t = 7, avg_loss = 0.4431\n",
            "t = 8, avg_loss = 0.5214\n",
            "t = 9, avg_loss = 0.6554\n",
            "t = 10, avg_loss = 0.6854\n",
            "t = 11, avg_loss = 0.4142\n",
            "t = 12, avg_loss = 0.4389\n",
            "t = 13, avg_loss = 0.5250\n",
            "t = 14, avg_loss = 0.4266\n",
            "t = 15, avg_loss = 0.7799\n",
            "t = 16, avg_loss = 0.5257\n",
            "t = 17, avg_loss = 0.4602\n",
            "t = 18, avg_loss = 0.5932\n",
            "t = 19, avg_loss = 0.5580\n",
            "t = 20, avg_loss = 0.6720\n",
            "t = 21, avg_loss = 0.5359\n",
            "t = 22, avg_loss = 0.5227\n",
            "t = 23, avg_loss = 0.4759\n",
            "t = 24, avg_loss = 0.6315\n",
            "t = 25, avg_loss = 0.6107\n",
            "t = 26, avg_loss = 0.6210\n",
            "t = 27, avg_loss = 0.6526\n",
            "t = 28, avg_loss = 0.4589\n",
            "t = 29, avg_loss = 0.5662\n",
            "t = 30, avg_loss = 0.4990\n",
            "Checking accuracy on test set\n",
            "Got 126 / 240 correct (52.50)\n",
            "acc = 0.525000\n",
            "Starting epoch 13 / 50\n",
            "t = 1, avg_loss = 0.6931\n",
            "t = 2, avg_loss = 0.7791\n",
            "t = 3, avg_loss = 0.6074\n",
            "t = 4, avg_loss = 0.5893\n",
            "t = 5, avg_loss = 0.5149\n",
            "t = 6, avg_loss = 0.5357\n",
            "t = 7, avg_loss = 0.4874\n",
            "t = 8, avg_loss = 0.5003\n",
            "t = 9, avg_loss = 0.6109\n",
            "t = 10, avg_loss = 0.5423\n",
            "t = 11, avg_loss = 0.5373\n",
            "t = 12, avg_loss = 0.4638\n",
            "t = 13, avg_loss = 0.5802\n",
            "t = 14, avg_loss = 0.5078\n",
            "t = 15, avg_loss = 0.4669\n",
            "t = 16, avg_loss = 0.4778\n",
            "t = 17, avg_loss = 0.5511\n",
            "t = 18, avg_loss = 0.5422\n",
            "t = 19, avg_loss = 0.5617\n",
            "t = 20, avg_loss = 0.6094\n",
            "t = 21, avg_loss = 0.4989\n",
            "t = 22, avg_loss = 0.5186\n",
            "t = 23, avg_loss = 0.5468\n",
            "t = 24, avg_loss = 0.5006\n",
            "t = 25, avg_loss = 0.5003\n",
            "t = 26, avg_loss = 0.5865\n",
            "t = 27, avg_loss = 0.6216\n",
            "t = 28, avg_loss = 0.6225\n",
            "t = 29, avg_loss = 0.6080\n",
            "t = 30, avg_loss = 0.4458\n",
            "Checking accuracy on test set\n",
            "Got 167 / 240 correct (69.58)\n",
            "acc = 0.695833\n",
            "Starting epoch 14 / 50\n",
            "t = 1, avg_loss = 0.6862\n",
            "t = 2, avg_loss = 0.4853\n",
            "t = 3, avg_loss = 0.4736\n",
            "t = 4, avg_loss = 0.4909\n",
            "t = 5, avg_loss = 0.5577\n",
            "t = 6, avg_loss = 0.5156\n",
            "t = 7, avg_loss = 0.7065\n",
            "t = 8, avg_loss = 0.5309\n",
            "t = 9, avg_loss = 0.4202\n",
            "t = 10, avg_loss = 0.3530\n",
            "t = 11, avg_loss = 0.5582\n",
            "t = 12, avg_loss = 0.5763\n",
            "t = 13, avg_loss = 0.4204\n",
            "t = 14, avg_loss = 0.4441\n",
            "t = 15, avg_loss = 0.5512\n",
            "t = 16, avg_loss = 0.6484\n",
            "t = 17, avg_loss = 0.5251\n",
            "t = 18, avg_loss = 0.4707\n",
            "t = 19, avg_loss = 0.5512\n",
            "t = 20, avg_loss = 0.4978\n",
            "t = 21, avg_loss = 0.5046\n",
            "t = 22, avg_loss = 0.4547\n",
            "t = 23, avg_loss = 0.5222\n",
            "t = 24, avg_loss = 0.4771\n",
            "t = 25, avg_loss = 0.6442\n",
            "t = 26, avg_loss = 0.4766\n",
            "t = 27, avg_loss = 0.4880\n",
            "t = 28, avg_loss = 0.5221\n",
            "t = 29, avg_loss = 0.4955\n",
            "t = 30, avg_loss = 0.6025\n",
            "Checking accuracy on test set\n",
            "Got 133 / 240 correct (55.42)\n",
            "acc = 0.554167\n",
            "Starting epoch 15 / 50\n",
            "t = 1, avg_loss = 0.6030\n",
            "t = 2, avg_loss = 0.6233\n",
            "t = 3, avg_loss = 0.5608\n",
            "t = 4, avg_loss = 0.4465\n",
            "t = 5, avg_loss = 0.4931\n",
            "t = 6, avg_loss = 0.5534\n",
            "t = 7, avg_loss = 0.6475\n",
            "t = 8, avg_loss = 0.5624\n",
            "t = 9, avg_loss = 0.4904\n",
            "t = 10, avg_loss = 0.5945\n",
            "t = 11, avg_loss = 0.4610\n",
            "t = 12, avg_loss = 0.4831\n",
            "t = 13, avg_loss = 0.5933\n",
            "t = 14, avg_loss = 0.5057\n",
            "t = 15, avg_loss = 0.4155\n",
            "t = 16, avg_loss = 0.4958\n",
            "t = 17, avg_loss = 0.4994\n",
            "t = 18, avg_loss = 0.4369\n",
            "t = 19, avg_loss = 0.3618\n",
            "t = 20, avg_loss = 0.6329\n",
            "t = 21, avg_loss = 0.4928\n",
            "t = 22, avg_loss = 0.4029\n",
            "t = 23, avg_loss = 0.6291\n",
            "t = 24, avg_loss = 0.4779\n",
            "t = 25, avg_loss = 0.4299\n",
            "t = 26, avg_loss = 0.5259\n",
            "t = 27, avg_loss = 0.6016\n",
            "t = 28, avg_loss = 0.6134\n",
            "t = 29, avg_loss = 0.5771\n",
            "t = 30, avg_loss = 0.4899\n",
            "Checking accuracy on test set\n",
            "Got 122 / 240 correct (50.83)\n",
            "acc = 0.508333\n",
            "Starting epoch 16 / 50\n",
            "t = 1, avg_loss = 0.4397\n",
            "t = 2, avg_loss = 0.4999\n",
            "t = 3, avg_loss = 0.4824\n",
            "t = 4, avg_loss = 0.4823\n",
            "t = 5, avg_loss = 0.4350\n",
            "t = 6, avg_loss = 0.4113\n",
            "t = 7, avg_loss = 0.5492\n",
            "t = 8, avg_loss = 0.3980\n",
            "t = 9, avg_loss = 0.5272\n",
            "t = 10, avg_loss = 0.6710\n",
            "t = 11, avg_loss = 0.4469\n",
            "t = 12, avg_loss = 0.5179\n",
            "t = 13, avg_loss = 0.6189\n",
            "t = 14, avg_loss = 0.6073\n",
            "t = 15, avg_loss = 0.5564\n",
            "t = 16, avg_loss = 0.5302\n",
            "t = 17, avg_loss = 0.5693\n",
            "t = 18, avg_loss = 0.5691\n",
            "t = 19, avg_loss = 0.5525\n",
            "t = 20, avg_loss = 0.5466\n",
            "t = 21, avg_loss = 0.5328\n",
            "t = 22, avg_loss = 0.5482\n",
            "t = 23, avg_loss = 0.6413\n",
            "t = 24, avg_loss = 0.5699\n",
            "t = 25, avg_loss = 0.5132\n",
            "t = 26, avg_loss = 0.5147\n",
            "t = 27, avg_loss = 0.6022\n",
            "t = 28, avg_loss = 0.6448\n",
            "t = 29, avg_loss = 0.4824\n",
            "t = 30, avg_loss = 0.5723\n",
            "Checking accuracy on test set\n",
            "Got 172 / 240 correct (71.67)\n",
            "acc = 0.716667\n",
            "Starting epoch 17 / 50\n",
            "t = 1, avg_loss = 0.5558\n",
            "t = 2, avg_loss = 0.5261\n",
            "t = 3, avg_loss = 0.5167\n",
            "t = 4, avg_loss = 0.5752\n",
            "t = 5, avg_loss = 0.4402\n",
            "t = 6, avg_loss = 0.4338\n",
            "t = 7, avg_loss = 0.4584\n",
            "t = 8, avg_loss = 0.4076\n",
            "t = 9, avg_loss = 0.5776\n",
            "t = 10, avg_loss = 0.5144\n",
            "t = 11, avg_loss = 0.5400\n",
            "t = 12, avg_loss = 0.5563\n",
            "t = 13, avg_loss = 0.5927\n",
            "t = 14, avg_loss = 0.6379\n",
            "t = 15, avg_loss = 0.5848\n",
            "t = 16, avg_loss = 0.4899\n",
            "t = 17, avg_loss = 0.6550\n",
            "t = 18, avg_loss = 0.4948\n",
            "t = 19, avg_loss = 0.5567\n",
            "t = 20, avg_loss = 0.4880\n",
            "t = 21, avg_loss = 0.6776\n",
            "t = 22, avg_loss = 0.4034\n",
            "t = 23, avg_loss = 0.4315\n",
            "t = 24, avg_loss = 0.5945\n",
            "t = 25, avg_loss = 0.3962\n",
            "t = 26, avg_loss = 0.4676\n",
            "t = 27, avg_loss = 0.4882\n",
            "t = 28, avg_loss = 0.5434\n",
            "t = 29, avg_loss = 0.4367\n",
            "t = 30, avg_loss = 0.5479\n",
            "Checking accuracy on test set\n",
            "Got 141 / 240 correct (58.75)\n",
            "acc = 0.587500\n",
            "Starting epoch 18 / 50\n",
            "t = 1, avg_loss = 0.5959\n",
            "t = 2, avg_loss = 0.4424\n",
            "t = 3, avg_loss = 0.4592\n",
            "t = 4, avg_loss = 0.5313\n",
            "t = 5, avg_loss = 0.6411\n",
            "t = 6, avg_loss = 0.4422\n",
            "t = 7, avg_loss = 0.5926\n",
            "t = 8, avg_loss = 0.5307\n",
            "t = 9, avg_loss = 0.5685\n",
            "t = 10, avg_loss = 0.5100\n",
            "t = 11, avg_loss = 0.4801\n",
            "t = 12, avg_loss = 0.6045\n",
            "t = 13, avg_loss = 0.4578\n",
            "t = 14, avg_loss = 0.4589\n",
            "t = 15, avg_loss = 0.5087\n",
            "t = 16, avg_loss = 0.5415\n",
            "t = 17, avg_loss = 0.5323\n",
            "t = 18, avg_loss = 0.6251\n",
            "t = 19, avg_loss = 0.4303\n",
            "t = 20, avg_loss = 0.6035\n",
            "t = 21, avg_loss = 0.4598\n",
            "t = 22, avg_loss = 0.5050\n",
            "t = 23, avg_loss = 0.4190\n",
            "t = 24, avg_loss = 0.4892\n",
            "t = 25, avg_loss = 0.5436\n",
            "t = 26, avg_loss = 0.4286\n",
            "t = 27, avg_loss = 0.6073\n",
            "t = 28, avg_loss = 0.5956\n",
            "t = 29, avg_loss = 0.4847\n",
            "t = 30, avg_loss = 0.6598\n",
            "Checking accuracy on test set\n",
            "Got 138 / 240 correct (57.50)\n",
            "acc = 0.575000\n",
            "Starting epoch 19 / 50\n",
            "t = 1, avg_loss = 0.6315\n",
            "t = 2, avg_loss = 0.4965\n",
            "t = 3, avg_loss = 0.5240\n",
            "t = 4, avg_loss = 0.6392\n",
            "t = 5, avg_loss = 0.5063\n",
            "t = 6, avg_loss = 0.4449\n",
            "t = 7, avg_loss = 0.4650\n",
            "t = 8, avg_loss = 0.4816\n",
            "t = 9, avg_loss = 0.6000\n",
            "t = 10, avg_loss = 0.5869\n",
            "t = 11, avg_loss = 0.4670\n",
            "t = 12, avg_loss = 0.6610\n",
            "t = 13, avg_loss = 0.6197\n",
            "t = 14, avg_loss = 0.5236\n",
            "t = 15, avg_loss = 0.4686\n",
            "t = 16, avg_loss = 0.5880\n",
            "t = 17, avg_loss = 0.5785\n",
            "t = 18, avg_loss = 0.5301\n",
            "t = 19, avg_loss = 0.5514\n",
            "t = 20, avg_loss = 0.5167\n",
            "t = 21, avg_loss = 0.4775\n",
            "t = 22, avg_loss = 0.5036\n",
            "t = 23, avg_loss = 0.4593\n",
            "t = 24, avg_loss = 0.6830\n",
            "t = 25, avg_loss = 0.4124\n",
            "t = 26, avg_loss = 0.6572\n",
            "t = 27, avg_loss = 0.5144\n",
            "t = 28, avg_loss = 0.4828\n",
            "t = 29, avg_loss = 0.5170\n",
            "t = 30, avg_loss = 0.4880\n",
            "Checking accuracy on test set\n",
            "Got 134 / 240 correct (55.83)\n",
            "acc = 0.558333\n",
            "Starting epoch 20 / 50\n",
            "t = 1, avg_loss = 0.4526\n",
            "t = 2, avg_loss = 0.4231\n",
            "t = 3, avg_loss = 0.5549\n",
            "t = 4, avg_loss = 0.5535\n",
            "t = 5, avg_loss = 0.4077\n",
            "t = 6, avg_loss = 0.5859\n",
            "t = 7, avg_loss = 0.4954\n",
            "t = 8, avg_loss = 0.4830\n",
            "t = 9, avg_loss = 0.3835\n",
            "t = 10, avg_loss = 0.4923\n",
            "t = 11, avg_loss = 0.6156\n",
            "t = 12, avg_loss = 0.5452\n",
            "t = 13, avg_loss = 0.5943\n",
            "t = 14, avg_loss = 0.6419\n",
            "t = 15, avg_loss = 0.4844\n",
            "t = 16, avg_loss = 0.5437\n",
            "t = 17, avg_loss = 0.4288\n",
            "t = 18, avg_loss = 0.5717\n",
            "t = 19, avg_loss = 0.5037\n",
            "t = 20, avg_loss = 0.5085\n",
            "t = 21, avg_loss = 0.4935\n",
            "t = 22, avg_loss = 0.6455\n",
            "t = 23, avg_loss = 0.6348\n",
            "t = 24, avg_loss = 0.6216\n",
            "t = 25, avg_loss = 0.4365\n",
            "t = 26, avg_loss = 0.5062\n",
            "t = 27, avg_loss = 0.4774\n",
            "t = 28, avg_loss = 0.5984\n",
            "t = 29, avg_loss = 0.4468\n",
            "t = 30, avg_loss = 0.5919\n",
            "Checking accuracy on test set\n",
            "Got 160 / 240 correct (66.67)\n",
            "acc = 0.666667\n",
            "Starting epoch 21 / 50\n",
            "t = 1, avg_loss = 0.3702\n",
            "t = 2, avg_loss = 0.5150\n",
            "t = 3, avg_loss = 0.5023\n",
            "t = 4, avg_loss = 0.4887\n",
            "t = 5, avg_loss = 0.5533\n",
            "t = 6, avg_loss = 0.5001\n",
            "t = 7, avg_loss = 0.4755\n",
            "t = 8, avg_loss = 0.4723\n",
            "t = 9, avg_loss = 0.5257\n",
            "t = 10, avg_loss = 0.4429\n",
            "t = 11, avg_loss = 0.5335\n",
            "t = 12, avg_loss = 0.4579\n",
            "t = 13, avg_loss = 0.6035\n",
            "t = 14, avg_loss = 0.5339\n",
            "t = 15, avg_loss = 0.5022\n",
            "t = 16, avg_loss = 0.5664\n",
            "t = 17, avg_loss = 0.4535\n",
            "t = 18, avg_loss = 0.6525\n",
            "t = 19, avg_loss = 0.5238\n",
            "t = 20, avg_loss = 0.4587\n",
            "t = 21, avg_loss = 0.5610\n",
            "t = 22, avg_loss = 0.4450\n",
            "t = 23, avg_loss = 0.5948\n",
            "t = 24, avg_loss = 0.5256\n",
            "t = 25, avg_loss = 0.6867\n",
            "t = 26, avg_loss = 0.5330\n",
            "t = 27, avg_loss = 0.4668\n",
            "t = 28, avg_loss = 0.4591\n",
            "t = 29, avg_loss = 0.7350\n",
            "t = 30, avg_loss = 0.4443\n",
            "Checking accuracy on test set\n",
            "Got 122 / 240 correct (50.83)\n",
            "acc = 0.508333\n",
            "Starting epoch 22 / 50\n",
            "t = 1, avg_loss = 0.4755\n",
            "t = 2, avg_loss = 0.5094\n",
            "t = 3, avg_loss = 0.6234\n",
            "t = 4, avg_loss = 0.4868\n",
            "t = 5, avg_loss = 0.5581\n",
            "t = 6, avg_loss = 0.4756\n",
            "t = 7, avg_loss = 0.3955\n",
            "t = 8, avg_loss = 0.4217\n",
            "t = 9, avg_loss = 0.5524\n",
            "t = 10, avg_loss = 0.4267\n",
            "t = 11, avg_loss = 0.4407\n",
            "t = 12, avg_loss = 0.5814\n",
            "t = 13, avg_loss = 0.4267\n",
            "t = 14, avg_loss = 0.6711\n",
            "t = 15, avg_loss = 0.4140\n",
            "t = 16, avg_loss = 0.5142\n",
            "t = 17, avg_loss = 0.4368\n",
            "t = 18, avg_loss = 0.6049\n",
            "t = 19, avg_loss = 0.4314\n",
            "t = 20, avg_loss = 0.6538\n",
            "t = 21, avg_loss = 0.7746\n",
            "t = 22, avg_loss = 0.5153\n",
            "t = 23, avg_loss = 0.4201\n",
            "t = 24, avg_loss = 0.4689\n",
            "t = 25, avg_loss = 0.4525\n",
            "t = 26, avg_loss = 0.5513\n",
            "t = 27, avg_loss = 0.5160\n",
            "t = 28, avg_loss = 0.5687\n",
            "t = 29, avg_loss = 0.4093\n",
            "t = 30, avg_loss = 0.4477\n",
            "Checking accuracy on test set\n",
            "Got 124 / 240 correct (51.67)\n",
            "acc = 0.516667\n",
            "Starting epoch 23 / 50\n",
            "t = 1, avg_loss = 0.4122\n",
            "t = 2, avg_loss = 0.6119\n",
            "t = 3, avg_loss = 0.5255\n",
            "t = 4, avg_loss = 0.6621\n",
            "t = 5, avg_loss = 0.4430\n",
            "t = 6, avg_loss = 0.4446\n",
            "t = 7, avg_loss = 0.4803\n",
            "t = 8, avg_loss = 0.4323\n",
            "t = 9, avg_loss = 0.5103\n",
            "t = 10, avg_loss = 0.5340\n",
            "t = 11, avg_loss = 0.5690\n",
            "t = 12, avg_loss = 0.4958\n",
            "t = 13, avg_loss = 0.5285\n",
            "t = 14, avg_loss = 0.4960\n",
            "t = 15, avg_loss = 0.4777\n",
            "t = 16, avg_loss = 0.5474\n",
            "t = 17, avg_loss = 0.5167\n",
            "t = 18, avg_loss = 0.4726\n",
            "t = 19, avg_loss = 0.5511\n",
            "t = 20, avg_loss = 0.4782\n",
            "t = 21, avg_loss = 0.5531\n",
            "t = 22, avg_loss = 0.3842\n",
            "t = 23, avg_loss = 0.5151\n",
            "t = 24, avg_loss = 0.5733\n",
            "t = 25, avg_loss = 0.4341\n",
            "t = 26, avg_loss = 0.5997\n",
            "t = 27, avg_loss = 0.5525\n",
            "t = 28, avg_loss = 0.5596\n",
            "t = 29, avg_loss = 0.5699\n",
            "t = 30, avg_loss = 0.5239\n",
            "Checking accuracy on test set\n",
            "Got 130 / 240 correct (54.17)\n",
            "acc = 0.541667\n",
            "Starting epoch 24 / 50\n",
            "t = 1, avg_loss = 0.6170\n",
            "t = 2, avg_loss = 0.4695\n",
            "t = 3, avg_loss = 0.5125\n",
            "t = 4, avg_loss = 0.6241\n",
            "t = 5, avg_loss = 0.5156\n",
            "t = 6, avg_loss = 0.4234\n",
            "t = 7, avg_loss = 0.5909\n",
            "t = 8, avg_loss = 0.5668\n",
            "t = 9, avg_loss = 0.6126\n",
            "t = 10, avg_loss = 0.6146\n",
            "t = 11, avg_loss = 0.4635\n",
            "t = 12, avg_loss = 0.5115\n",
            "t = 13, avg_loss = 0.6068\n",
            "t = 14, avg_loss = 0.4748\n",
            "t = 15, avg_loss = 0.4416\n",
            "t = 16, avg_loss = 0.5064\n",
            "t = 17, avg_loss = 0.6151\n",
            "t = 18, avg_loss = 0.5355\n",
            "t = 19, avg_loss = 0.5885\n",
            "t = 20, avg_loss = 0.5442\n",
            "t = 21, avg_loss = 0.5662\n",
            "t = 22, avg_loss = 0.5323\n",
            "t = 23, avg_loss = 0.5009\n",
            "t = 24, avg_loss = 0.5122\n",
            "t = 25, avg_loss = 0.4777\n",
            "t = 26, avg_loss = 0.4905\n",
            "t = 27, avg_loss = 0.4746\n",
            "t = 28, avg_loss = 0.6732\n",
            "t = 29, avg_loss = 0.4387\n",
            "t = 30, avg_loss = 0.5303\n",
            "Checking accuracy on test set\n",
            "Got 160 / 240 correct (66.67)\n",
            "acc = 0.666667\n",
            "Starting epoch 25 / 50\n",
            "t = 1, avg_loss = 0.4925\n",
            "t = 2, avg_loss = 0.4094\n",
            "t = 3, avg_loss = 0.7172\n",
            "t = 4, avg_loss = 0.4194\n",
            "t = 5, avg_loss = 0.5258\n",
            "t = 6, avg_loss = 0.5461\n",
            "t = 7, avg_loss = 0.5742\n",
            "t = 8, avg_loss = 0.5206\n",
            "t = 9, avg_loss = 0.5814\n",
            "t = 10, avg_loss = 0.5043\n",
            "t = 11, avg_loss = 0.5305\n",
            "t = 12, avg_loss = 0.4754\n",
            "t = 13, avg_loss = 0.4710\n",
            "t = 14, avg_loss = 0.5739\n",
            "t = 15, avg_loss = 0.7866\n",
            "t = 16, avg_loss = 0.6012\n",
            "t = 17, avg_loss = 0.4767\n",
            "t = 18, avg_loss = 0.6338\n",
            "t = 19, avg_loss = 0.5668\n",
            "t = 20, avg_loss = 0.6082\n",
            "t = 21, avg_loss = 0.4938\n",
            "t = 22, avg_loss = 0.4981\n",
            "t = 23, avg_loss = 0.6169\n",
            "t = 24, avg_loss = 0.4303\n",
            "t = 25, avg_loss = 0.5590\n",
            "t = 26, avg_loss = 0.5298\n",
            "t = 27, avg_loss = 0.4894\n",
            "t = 28, avg_loss = 0.5335\n",
            "t = 29, avg_loss = 0.4975\n",
            "t = 30, avg_loss = 0.4842\n",
            "Checking accuracy on test set\n",
            "Got 140 / 240 correct (58.33)\n",
            "acc = 0.583333\n",
            "Starting epoch 26 / 50\n",
            "t = 1, avg_loss = 0.5525\n",
            "t = 2, avg_loss = 0.4800\n",
            "t = 3, avg_loss = 0.4178\n",
            "t = 4, avg_loss = 0.4668\n",
            "t = 5, avg_loss = 0.5267\n",
            "t = 6, avg_loss = 0.4814\n",
            "t = 7, avg_loss = 0.5591\n",
            "t = 8, avg_loss = 0.6362\n",
            "t = 9, avg_loss = 0.4855\n",
            "t = 10, avg_loss = 0.5110\n",
            "t = 11, avg_loss = 0.6864\n",
            "t = 12, avg_loss = 0.4580\n",
            "t = 13, avg_loss = 0.4742\n",
            "t = 14, avg_loss = 0.5563\n",
            "t = 15, avg_loss = 0.5032\n",
            "t = 16, avg_loss = 0.4727\n",
            "t = 17, avg_loss = 0.4676\n",
            "t = 18, avg_loss = 0.4474\n",
            "t = 19, avg_loss = 0.4780\n",
            "t = 20, avg_loss = 0.4377\n",
            "t = 21, avg_loss = 0.5656\n",
            "t = 22, avg_loss = 0.5587\n",
            "t = 23, avg_loss = 0.5882\n",
            "t = 24, avg_loss = 0.5446\n",
            "t = 25, avg_loss = 0.4654\n",
            "t = 26, avg_loss = 0.4464\n",
            "t = 27, avg_loss = 0.5103\n",
            "t = 28, avg_loss = 0.6316\n",
            "t = 29, avg_loss = 0.4599\n",
            "t = 30, avg_loss = 0.6125\n",
            "Checking accuracy on test set\n",
            "Got 176 / 240 correct (73.33)\n",
            "acc = 0.733333\n",
            "Starting epoch 27 / 50\n",
            "t = 1, avg_loss = 0.5140\n",
            "t = 2, avg_loss = 0.6785\n",
            "t = 3, avg_loss = 0.5715\n",
            "t = 4, avg_loss = 0.4365\n",
            "t = 5, avg_loss = 0.5310\n",
            "t = 6, avg_loss = 0.4542\n",
            "t = 7, avg_loss = 0.4961\n",
            "t = 8, avg_loss = 0.5197\n",
            "t = 9, avg_loss = 0.4731\n",
            "t = 10, avg_loss = 0.5331\n",
            "t = 11, avg_loss = 0.5475\n",
            "t = 12, avg_loss = 0.4746\n",
            "t = 13, avg_loss = 0.4606\n",
            "t = 14, avg_loss = 0.5349\n",
            "t = 15, avg_loss = 0.6034\n",
            "t = 16, avg_loss = 0.4671\n",
            "t = 17, avg_loss = 0.4815\n",
            "t = 18, avg_loss = 0.7147\n",
            "t = 19, avg_loss = 0.4672\n",
            "t = 20, avg_loss = 0.5285\n",
            "t = 21, avg_loss = 0.4329\n",
            "t = 22, avg_loss = 0.5362\n",
            "t = 23, avg_loss = 0.5091\n",
            "t = 24, avg_loss = 0.4742\n",
            "t = 25, avg_loss = 0.5492\n",
            "t = 26, avg_loss = 0.5418\n",
            "t = 27, avg_loss = 0.4421\n",
            "t = 28, avg_loss = 0.5055\n",
            "t = 29, avg_loss = 0.4632\n",
            "t = 30, avg_loss = 0.3984\n",
            "Checking accuracy on test set\n",
            "Got 136 / 240 correct (56.67)\n",
            "acc = 0.566667\n",
            "Starting epoch 28 / 50\n",
            "t = 1, avg_loss = 0.4626\n",
            "t = 2, avg_loss = 0.6589\n",
            "t = 3, avg_loss = 0.5487\n",
            "t = 4, avg_loss = 0.4862\n",
            "t = 5, avg_loss = 0.5053\n",
            "t = 6, avg_loss = 0.5573\n",
            "t = 7, avg_loss = 0.5765\n",
            "t = 8, avg_loss = 0.5412\n",
            "t = 9, avg_loss = 0.5807\n",
            "t = 10, avg_loss = 0.3827\n",
            "t = 11, avg_loss = 0.5661\n",
            "t = 12, avg_loss = 0.4989\n",
            "t = 13, avg_loss = 0.5543\n",
            "t = 14, avg_loss = 0.4164\n",
            "t = 15, avg_loss = 0.5205\n",
            "t = 16, avg_loss = 0.5352\n",
            "t = 17, avg_loss = 0.4247\n",
            "t = 18, avg_loss = 0.5530\n",
            "t = 19, avg_loss = 0.5759\n",
            "t = 20, avg_loss = 0.4824\n",
            "t = 21, avg_loss = 0.4765\n",
            "t = 22, avg_loss = 0.4593\n",
            "t = 23, avg_loss = 0.5278\n",
            "t = 24, avg_loss = 0.6923\n",
            "t = 25, avg_loss = 0.5188\n",
            "t = 26, avg_loss = 0.5651\n",
            "t = 27, avg_loss = 0.4624\n",
            "t = 28, avg_loss = 0.5273\n",
            "t = 29, avg_loss = 0.5786\n",
            "t = 30, avg_loss = 0.6322\n",
            "Checking accuracy on test set\n",
            "Got 134 / 240 correct (55.83)\n",
            "acc = 0.558333\n",
            "Starting epoch 29 / 50\n",
            "t = 1, avg_loss = 0.4400\n",
            "t = 2, avg_loss = 0.5629\n",
            "t = 3, avg_loss = 0.5937\n",
            "t = 4, avg_loss = 0.5307\n",
            "t = 5, avg_loss = 0.6309\n",
            "t = 6, avg_loss = 0.6194\n",
            "t = 7, avg_loss = 0.6644\n",
            "t = 8, avg_loss = 0.5935\n",
            "t = 9, avg_loss = 0.5872\n",
            "t = 10, avg_loss = 0.5321\n",
            "t = 11, avg_loss = 0.5431\n",
            "t = 12, avg_loss = 0.4967\n",
            "t = 13, avg_loss = 0.4428\n",
            "t = 14, avg_loss = 0.5578\n",
            "t = 15, avg_loss = 0.4771\n",
            "t = 16, avg_loss = 0.4644\n",
            "t = 17, avg_loss = 0.5074\n",
            "t = 18, avg_loss = 0.4617\n",
            "t = 19, avg_loss = 0.5304\n",
            "t = 20, avg_loss = 0.5678\n",
            "t = 21, avg_loss = 0.6031\n",
            "t = 22, avg_loss = 0.5639\n",
            "t = 23, avg_loss = 0.6055\n",
            "t = 24, avg_loss = 0.4789\n",
            "t = 25, avg_loss = 0.5012\n",
            "t = 26, avg_loss = 0.5637\n",
            "t = 27, avg_loss = 0.5017\n",
            "t = 28, avg_loss = 0.5921\n",
            "t = 29, avg_loss = 0.5106\n",
            "t = 30, avg_loss = 0.5866\n",
            "Checking accuracy on test set\n",
            "Got 137 / 240 correct (57.08)\n",
            "acc = 0.570833\n",
            "Starting epoch 30 / 50\n",
            "t = 1, avg_loss = 0.5579\n",
            "t = 2, avg_loss = 0.5480\n",
            "t = 3, avg_loss = 0.5125\n",
            "t = 4, avg_loss = 0.5403\n",
            "t = 5, avg_loss = 0.4503\n",
            "t = 6, avg_loss = 0.5166\n",
            "t = 7, avg_loss = 0.6437\n",
            "t = 8, avg_loss = 0.5340\n",
            "t = 9, avg_loss = 0.5082\n",
            "t = 10, avg_loss = 0.4568\n",
            "t = 11, avg_loss = 0.5160\n",
            "t = 12, avg_loss = 0.5660\n",
            "t = 13, avg_loss = 0.5444\n",
            "t = 14, avg_loss = 0.5334\n",
            "t = 15, avg_loss = 0.5860\n",
            "t = 16, avg_loss = 0.5753\n",
            "t = 17, avg_loss = 0.6552\n",
            "t = 18, avg_loss = 0.6357\n",
            "t = 19, avg_loss = 0.4936\n",
            "t = 20, avg_loss = 0.4770\n",
            "t = 21, avg_loss = 0.5391\n",
            "t = 22, avg_loss = 0.4017\n",
            "t = 23, avg_loss = 0.4867\n",
            "t = 24, avg_loss = 0.4859\n",
            "t = 25, avg_loss = 0.5207\n",
            "t = 26, avg_loss = 0.5529\n",
            "t = 27, avg_loss = 0.6622\n",
            "t = 28, avg_loss = 0.4614\n",
            "t = 29, avg_loss = 0.4848\n",
            "t = 30, avg_loss = 0.5630\n",
            "Checking accuracy on test set\n",
            "Got 117 / 240 correct (48.75)\n",
            "acc = 0.487500\n",
            "Starting epoch 31 / 50\n",
            "t = 1, avg_loss = 0.4994\n",
            "t = 2, avg_loss = 0.4947\n",
            "t = 3, avg_loss = 0.5429\n",
            "t = 4, avg_loss = 0.5781\n",
            "t = 5, avg_loss = 0.5568\n",
            "t = 6, avg_loss = 0.5894\n",
            "t = 7, avg_loss = 0.4520\n",
            "t = 8, avg_loss = 0.5642\n",
            "t = 9, avg_loss = 0.5192\n",
            "t = 10, avg_loss = 0.5492\n",
            "t = 11, avg_loss = 0.4730\n",
            "t = 12, avg_loss = 0.6181\n",
            "t = 13, avg_loss = 0.4801\n",
            "t = 14, avg_loss = 0.4576\n",
            "t = 15, avg_loss = 0.4861\n",
            "t = 16, avg_loss = 0.4789\n",
            "t = 17, avg_loss = 0.5743\n",
            "t = 18, avg_loss = 0.5869\n",
            "t = 19, avg_loss = 0.5295\n",
            "t = 20, avg_loss = 0.4798\n",
            "t = 21, avg_loss = 0.5578\n",
            "t = 22, avg_loss = 0.5556\n",
            "t = 23, avg_loss = 0.4127\n",
            "t = 24, avg_loss = 0.4637\n",
            "t = 25, avg_loss = 0.4967\n",
            "t = 26, avg_loss = 0.4693\n",
            "t = 27, avg_loss = 0.4382\n",
            "t = 28, avg_loss = 0.5719\n",
            "t = 29, avg_loss = 0.6576\n",
            "t = 30, avg_loss = 0.4888\n",
            "Checking accuracy on test set\n",
            "Got 136 / 240 correct (56.67)\n",
            "acc = 0.566667\n",
            "Starting epoch 32 / 50\n",
            "t = 1, avg_loss = 0.4435\n",
            "t = 2, avg_loss = 0.5220\n",
            "t = 3, avg_loss = 0.4746\n",
            "t = 4, avg_loss = 0.4978\n",
            "t = 5, avg_loss = 0.5387\n",
            "t = 6, avg_loss = 0.4147\n",
            "t = 7, avg_loss = 0.5446\n",
            "t = 8, avg_loss = 0.4025\n",
            "t = 9, avg_loss = 0.5227\n",
            "t = 10, avg_loss = 0.6131\n",
            "t = 11, avg_loss = 0.5572\n",
            "t = 12, avg_loss = 0.7733\n",
            "t = 13, avg_loss = 0.6100\n",
            "t = 14, avg_loss = 0.4555\n",
            "t = 15, avg_loss = 0.3921\n",
            "t = 16, avg_loss = 0.4565\n",
            "t = 17, avg_loss = 0.5305\n",
            "t = 18, avg_loss = 0.4955\n",
            "t = 19, avg_loss = 0.4862\n",
            "t = 20, avg_loss = 0.6067\n",
            "t = 21, avg_loss = 0.5563\n",
            "t = 22, avg_loss = 0.4746\n",
            "t = 23, avg_loss = 0.4680\n",
            "t = 24, avg_loss = 0.5976\n",
            "t = 25, avg_loss = 0.5132\n",
            "t = 26, avg_loss = 0.4751\n",
            "t = 27, avg_loss = 0.4418\n",
            "t = 28, avg_loss = 0.6083\n",
            "t = 29, avg_loss = 0.6644\n",
            "t = 30, avg_loss = 0.5818\n",
            "Checking accuracy on test set\n",
            "Got 152 / 240 correct (63.33)\n",
            "acc = 0.633333\n",
            "Starting epoch 33 / 50\n",
            "t = 1, avg_loss = 0.5332\n",
            "t = 2, avg_loss = 0.5756\n",
            "t = 3, avg_loss = 0.6169\n",
            "t = 4, avg_loss = 0.5480\n",
            "t = 5, avg_loss = 0.5254\n",
            "t = 6, avg_loss = 0.5558\n",
            "t = 7, avg_loss = 0.5726\n",
            "t = 8, avg_loss = 0.5149\n",
            "t = 9, avg_loss = 0.4850\n",
            "t = 10, avg_loss = 0.4466\n",
            "t = 11, avg_loss = 0.7033\n",
            "t = 12, avg_loss = 0.4866\n",
            "t = 13, avg_loss = 0.5976\n",
            "t = 14, avg_loss = 0.5208\n",
            "t = 15, avg_loss = 0.4366\n",
            "t = 16, avg_loss = 0.6193\n",
            "t = 17, avg_loss = 0.6149\n",
            "t = 18, avg_loss = 0.5807\n",
            "t = 19, avg_loss = 0.6035\n",
            "t = 20, avg_loss = 0.4929\n",
            "t = 21, avg_loss = 0.5773\n",
            "t = 22, avg_loss = 0.4045\n",
            "t = 23, avg_loss = 0.4316\n",
            "t = 24, avg_loss = 0.4786\n",
            "t = 25, avg_loss = 0.4548\n",
            "t = 26, avg_loss = 0.4999\n",
            "t = 27, avg_loss = 0.5782\n",
            "t = 28, avg_loss = 0.4556\n",
            "t = 29, avg_loss = 0.4435\n",
            "t = 30, avg_loss = 0.6162\n",
            "Checking accuracy on test set\n",
            "Got 169 / 240 correct (70.42)\n",
            "acc = 0.704167\n",
            "Starting epoch 34 / 50\n",
            "t = 1, avg_loss = 0.5918\n",
            "t = 2, avg_loss = 0.5970\n",
            "t = 3, avg_loss = 0.4942\n",
            "t = 4, avg_loss = 0.4860\n",
            "t = 5, avg_loss = 0.4116\n",
            "t = 6, avg_loss = 0.5621\n",
            "t = 7, avg_loss = 0.5888\n",
            "t = 8, avg_loss = 0.5129\n",
            "t = 9, avg_loss = 0.4663\n",
            "t = 10, avg_loss = 0.6926\n",
            "t = 11, avg_loss = 0.4657\n",
            "t = 12, avg_loss = 0.5336\n",
            "t = 13, avg_loss = 0.5091\n",
            "t = 14, avg_loss = 0.3515\n",
            "t = 15, avg_loss = 0.5191\n",
            "t = 16, avg_loss = 0.4597\n",
            "t = 17, avg_loss = 0.4832\n",
            "t = 18, avg_loss = 0.5601\n",
            "t = 19, avg_loss = 0.3964\n",
            "t = 20, avg_loss = 0.5184\n",
            "t = 21, avg_loss = 0.5992\n",
            "t = 22, avg_loss = 0.4552\n",
            "t = 23, avg_loss = 0.4729\n",
            "t = 24, avg_loss = 0.6154\n",
            "t = 25, avg_loss = 0.6111\n",
            "t = 26, avg_loss = 0.5328\n",
            "t = 27, avg_loss = 0.5276\n",
            "t = 28, avg_loss = 0.6865\n",
            "t = 29, avg_loss = 0.6450\n",
            "t = 30, avg_loss = 0.6411\n",
            "Checking accuracy on test set\n",
            "Got 154 / 240 correct (64.17)\n",
            "acc = 0.641667\n",
            "Starting epoch 35 / 50\n",
            "t = 1, avg_loss = 0.4609\n",
            "t = 2, avg_loss = 0.5538\n",
            "t = 3, avg_loss = 0.5082\n",
            "t = 4, avg_loss = 0.6308\n",
            "t = 5, avg_loss = 0.5315\n",
            "t = 6, avg_loss = 0.5255\n",
            "t = 7, avg_loss = 0.5046\n",
            "t = 8, avg_loss = 0.5431\n",
            "t = 9, avg_loss = 0.5680\n",
            "t = 10, avg_loss = 0.4857\n",
            "t = 11, avg_loss = 0.4997\n",
            "t = 12, avg_loss = 0.4734\n",
            "t = 13, avg_loss = 0.5388\n",
            "t = 14, avg_loss = 0.4872\n",
            "t = 15, avg_loss = 0.5722\n",
            "t = 16, avg_loss = 0.4884\n",
            "t = 17, avg_loss = 0.6274\n",
            "t = 18, avg_loss = 0.5548\n",
            "t = 19, avg_loss = 0.4846\n",
            "t = 20, avg_loss = 0.6834\n",
            "t = 21, avg_loss = 0.5320\n",
            "t = 22, avg_loss = 0.6499\n",
            "t = 23, avg_loss = 0.5485\n",
            "t = 24, avg_loss = 0.6048\n",
            "t = 25, avg_loss = 0.6593\n",
            "t = 26, avg_loss = 0.5430\n",
            "t = 27, avg_loss = 0.5212\n",
            "t = 28, avg_loss = 0.5079\n",
            "t = 29, avg_loss = 0.5164\n",
            "t = 30, avg_loss = 0.5559\n",
            "Checking accuracy on test set\n",
            "Got 121 / 240 correct (50.42)\n",
            "acc = 0.504167\n",
            "Starting epoch 36 / 50\n",
            "t = 1, avg_loss = 0.5561\n",
            "t = 2, avg_loss = 0.4704\n",
            "t = 3, avg_loss = 0.5414\n",
            "t = 4, avg_loss = 0.5764\n",
            "t = 5, avg_loss = 0.5377\n",
            "t = 6, avg_loss = 0.5295\n",
            "t = 7, avg_loss = 0.7088\n",
            "t = 8, avg_loss = 0.5713\n",
            "t = 9, avg_loss = 0.5121\n",
            "t = 10, avg_loss = 0.4642\n",
            "t = 11, avg_loss = 0.4770\n",
            "t = 12, avg_loss = 0.5639\n",
            "t = 13, avg_loss = 0.5036\n",
            "t = 14, avg_loss = 0.5118\n",
            "t = 15, avg_loss = 0.5615\n",
            "t = 16, avg_loss = 0.5144\n",
            "t = 17, avg_loss = 0.5346\n",
            "t = 18, avg_loss = 0.5882\n",
            "t = 19, avg_loss = 0.4414\n",
            "t = 20, avg_loss = 0.5102\n",
            "t = 21, avg_loss = 0.5486\n",
            "t = 22, avg_loss = 0.5373\n",
            "t = 23, avg_loss = 0.5841\n",
            "t = 24, avg_loss = 0.6910\n",
            "t = 25, avg_loss = 0.4412\n",
            "t = 26, avg_loss = 0.5708\n",
            "t = 27, avg_loss = 0.4668\n",
            "t = 28, avg_loss = 0.4426\n",
            "t = 29, avg_loss = 0.6256\n",
            "t = 30, avg_loss = 0.4708\n",
            "Checking accuracy on test set\n",
            "Got 121 / 240 correct (50.42)\n",
            "acc = 0.504167\n",
            "Starting epoch 37 / 50\n",
            "t = 1, avg_loss = 0.5795\n",
            "t = 2, avg_loss = 0.5764\n",
            "t = 3, avg_loss = 0.5245\n",
            "t = 4, avg_loss = 0.4056\n",
            "t = 5, avg_loss = 0.4410\n",
            "t = 6, avg_loss = 0.4500\n",
            "t = 7, avg_loss = 0.5337\n",
            "t = 8, avg_loss = 0.4724\n",
            "t = 9, avg_loss = 0.4474\n",
            "t = 10, avg_loss = 0.5919\n",
            "t = 11, avg_loss = 0.5187\n",
            "t = 12, avg_loss = 0.6203\n",
            "t = 13, avg_loss = 0.5435\n",
            "t = 14, avg_loss = 0.4728\n",
            "t = 15, avg_loss = 0.4776\n",
            "t = 16, avg_loss = 0.4877\n",
            "t = 17, avg_loss = 0.5275\n",
            "t = 18, avg_loss = 0.5617\n",
            "t = 19, avg_loss = 0.6391\n",
            "t = 20, avg_loss = 0.5767\n",
            "t = 21, avg_loss = 0.4986\n",
            "t = 22, avg_loss = 0.4558\n",
            "t = 23, avg_loss = 0.5994\n",
            "t = 24, avg_loss = 0.5239\n",
            "t = 25, avg_loss = 0.4364\n",
            "t = 26, avg_loss = 0.5550\n",
            "t = 27, avg_loss = 0.5102\n",
            "t = 28, avg_loss = 0.4569\n",
            "t = 29, avg_loss = 0.6819\n",
            "t = 30, avg_loss = 0.4855\n",
            "Checking accuracy on test set\n",
            "Got 123 / 240 correct (51.25)\n",
            "acc = 0.512500\n",
            "Starting epoch 38 / 50\n",
            "t = 1, avg_loss = 0.4731\n",
            "t = 2, avg_loss = 0.6046\n",
            "t = 3, avg_loss = 0.4619\n",
            "t = 4, avg_loss = 0.4505\n",
            "t = 5, avg_loss = 0.6800\n",
            "t = 6, avg_loss = 0.4317\n",
            "t = 7, avg_loss = 0.5211\n",
            "t = 8, avg_loss = 0.5572\n",
            "t = 9, avg_loss = 0.5473\n",
            "t = 10, avg_loss = 0.5115\n",
            "t = 11, avg_loss = 0.4987\n",
            "t = 12, avg_loss = 0.6542\n",
            "t = 13, avg_loss = 0.4415\n",
            "t = 14, avg_loss = 0.5260\n",
            "t = 15, avg_loss = 0.4221\n",
            "t = 16, avg_loss = 0.6632\n",
            "t = 17, avg_loss = 0.4713\n",
            "t = 18, avg_loss = 0.5654\n",
            "t = 19, avg_loss = 0.4481\n",
            "t = 20, avg_loss = 0.4872\n",
            "t = 21, avg_loss = 0.5255\n",
            "t = 22, avg_loss = 0.4757\n",
            "t = 23, avg_loss = 0.6215\n",
            "t = 24, avg_loss = 0.6059\n",
            "t = 25, avg_loss = 0.4924\n",
            "t = 26, avg_loss = 0.4949\n",
            "t = 27, avg_loss = 0.5076\n",
            "t = 28, avg_loss = 0.4862\n",
            "t = 29, avg_loss = 0.4919\n",
            "t = 30, avg_loss = 0.5156\n",
            "Checking accuracy on test set\n",
            "Got 140 / 240 correct (58.33)\n",
            "acc = 0.583333\n",
            "Starting epoch 39 / 50\n",
            "t = 1, avg_loss = 0.4523\n",
            "t = 2, avg_loss = 0.3871\n",
            "t = 3, avg_loss = 0.5400\n",
            "t = 4, avg_loss = 0.5274\n",
            "t = 5, avg_loss = 0.4595\n",
            "t = 6, avg_loss = 0.4444\n",
            "t = 7, avg_loss = 0.4744\n",
            "t = 8, avg_loss = 0.5832\n",
            "t = 9, avg_loss = 0.6910\n",
            "t = 10, avg_loss = 0.5363\n",
            "t = 11, avg_loss = 0.5334\n",
            "t = 12, avg_loss = 0.5482\n",
            "t = 13, avg_loss = 0.4559\n",
            "t = 14, avg_loss = 0.6847\n",
            "t = 15, avg_loss = 0.6247\n",
            "t = 16, avg_loss = 0.5655\n",
            "t = 17, avg_loss = 0.5898\n",
            "t = 18, avg_loss = 0.5461\n",
            "t = 19, avg_loss = 0.5905\n",
            "t = 20, avg_loss = 0.5297\n",
            "t = 21, avg_loss = 0.6133\n",
            "t = 22, avg_loss = 0.5057\n",
            "t = 23, avg_loss = 0.5419\n",
            "t = 24, avg_loss = 0.4867\n",
            "t = 25, avg_loss = 0.5610\n",
            "t = 26, avg_loss = 0.5977\n",
            "t = 27, avg_loss = 0.5384\n",
            "t = 28, avg_loss = 0.5169\n",
            "t = 29, avg_loss = 0.5682\n",
            "t = 30, avg_loss = 0.5132\n",
            "Checking accuracy on test set\n",
            "Got 169 / 240 correct (70.42)\n",
            "acc = 0.704167\n",
            "Starting epoch 40 / 50\n",
            "t = 1, avg_loss = 0.5128\n",
            "t = 2, avg_loss = 0.3734\n",
            "t = 3, avg_loss = 0.6513\n",
            "t = 4, avg_loss = 0.4845\n",
            "t = 5, avg_loss = 0.5137\n",
            "t = 6, avg_loss = 0.4708\n",
            "t = 7, avg_loss = 0.4678\n",
            "t = 8, avg_loss = 0.6196\n",
            "t = 9, avg_loss = 0.6391\n",
            "t = 10, avg_loss = 0.4423\n",
            "t = 11, avg_loss = 0.5253\n",
            "t = 12, avg_loss = 0.5278\n",
            "t = 13, avg_loss = 0.4525\n",
            "t = 14, avg_loss = 0.5012\n",
            "t = 15, avg_loss = 0.6753\n",
            "t = 16, avg_loss = 0.5242\n",
            "t = 17, avg_loss = 0.4827\n",
            "t = 18, avg_loss = 0.5942\n",
            "t = 19, avg_loss = 0.5900\n",
            "t = 20, avg_loss = 0.4991\n",
            "t = 21, avg_loss = 0.4546\n",
            "t = 22, avg_loss = 0.5032\n",
            "t = 23, avg_loss = 0.4951\n",
            "t = 24, avg_loss = 0.5075\n",
            "t = 25, avg_loss = 0.4506\n",
            "t = 26, avg_loss = 0.4289\n",
            "t = 27, avg_loss = 0.4709\n",
            "t = 28, avg_loss = 0.4790\n",
            "t = 29, avg_loss = 0.5751\n",
            "t = 30, avg_loss = 0.5280\n",
            "Checking accuracy on test set\n",
            "Got 119 / 240 correct (49.58)\n",
            "acc = 0.495833\n",
            "Starting epoch 41 / 50\n",
            "t = 1, avg_loss = 0.5758\n",
            "t = 2, avg_loss = 0.4856\n",
            "t = 3, avg_loss = 0.5451\n",
            "t = 4, avg_loss = 0.6017\n",
            "t = 5, avg_loss = 0.5862\n",
            "t = 6, avg_loss = 0.5968\n",
            "t = 7, avg_loss = 0.6192\n",
            "t = 8, avg_loss = 0.4081\n",
            "t = 9, avg_loss = 0.4240\n",
            "t = 10, avg_loss = 0.4430\n",
            "t = 11, avg_loss = 0.4751\n",
            "t = 12, avg_loss = 0.5625\n",
            "t = 13, avg_loss = 0.4048\n",
            "t = 14, avg_loss = 0.4655\n",
            "t = 15, avg_loss = 0.5676\n",
            "t = 16, avg_loss = 0.4722\n",
            "t = 17, avg_loss = 0.4235\n",
            "t = 18, avg_loss = 0.3753\n",
            "t = 19, avg_loss = 0.6600\n",
            "t = 20, avg_loss = 0.4327\n",
            "t = 21, avg_loss = 0.5502\n",
            "t = 22, avg_loss = 0.4756\n",
            "t = 23, avg_loss = 0.4353\n",
            "t = 24, avg_loss = 0.4846\n",
            "t = 25, avg_loss = 0.5293\n",
            "t = 26, avg_loss = 0.6681\n",
            "t = 27, avg_loss = 0.6146\n",
            "t = 28, avg_loss = 0.4941\n",
            "t = 29, avg_loss = 0.6563\n",
            "t = 30, avg_loss = 0.5166\n",
            "Checking accuracy on test set\n",
            "Got 117 / 240 correct (48.75)\n",
            "acc = 0.487500\n",
            "Starting epoch 42 / 50\n",
            "t = 1, avg_loss = 0.5037\n",
            "t = 2, avg_loss = 0.5730\n",
            "t = 3, avg_loss = 0.4945\n",
            "t = 4, avg_loss = 0.5130\n",
            "t = 5, avg_loss = 0.5663\n",
            "t = 6, avg_loss = 0.5605\n",
            "t = 7, avg_loss = 0.6717\n",
            "t = 8, avg_loss = 0.5207\n",
            "t = 9, avg_loss = 0.6354\n",
            "t = 10, avg_loss = 0.4648\n",
            "t = 11, avg_loss = 0.5134\n",
            "t = 12, avg_loss = 0.4283\n",
            "t = 13, avg_loss = 0.5064\n",
            "t = 14, avg_loss = 0.5100\n",
            "t = 15, avg_loss = 0.4725\n",
            "t = 16, avg_loss = 0.6023\n",
            "t = 17, avg_loss = 0.5306\n",
            "t = 18, avg_loss = 0.5080\n",
            "t = 19, avg_loss = 0.6432\n",
            "t = 20, avg_loss = 0.4601\n",
            "t = 21, avg_loss = 0.6176\n",
            "t = 22, avg_loss = 0.5148\n",
            "t = 23, avg_loss = 0.5107\n",
            "t = 24, avg_loss = 0.5607\n",
            "t = 25, avg_loss = 0.6191\n",
            "t = 26, avg_loss = 0.6655\n",
            "t = 27, avg_loss = 0.5023\n",
            "t = 28, avg_loss = 0.4517\n",
            "t = 29, avg_loss = 0.4582\n",
            "t = 30, avg_loss = 0.4832\n",
            "Checking accuracy on test set\n",
            "Got 171 / 240 correct (71.25)\n",
            "acc = 0.712500\n",
            "Starting epoch 43 / 50\n",
            "t = 1, avg_loss = 0.5210\n",
            "t = 2, avg_loss = 0.5329\n",
            "t = 3, avg_loss = 0.5125\n",
            "t = 4, avg_loss = 0.4425\n",
            "t = 5, avg_loss = 0.4951\n",
            "t = 6, avg_loss = 0.4991\n",
            "t = 7, avg_loss = 0.4231\n",
            "t = 8, avg_loss = 0.7331\n",
            "t = 9, avg_loss = 0.6394\n",
            "t = 10, avg_loss = 0.4589\n",
            "t = 11, avg_loss = 0.6082\n",
            "t = 12, avg_loss = 0.5326\n",
            "t = 13, avg_loss = 0.5565\n",
            "t = 14, avg_loss = 0.8278\n",
            "t = 15, avg_loss = 0.6369\n",
            "t = 16, avg_loss = 0.5761\n",
            "t = 17, avg_loss = 0.4448\n",
            "t = 18, avg_loss = 0.5873\n",
            "t = 19, avg_loss = 0.5858\n",
            "t = 20, avg_loss = 0.4353\n",
            "t = 21, avg_loss = 0.4638\n",
            "t = 22, avg_loss = 0.5005\n",
            "t = 23, avg_loss = 0.5907\n",
            "t = 24, avg_loss = 0.4856\n",
            "t = 25, avg_loss = 0.5090\n",
            "t = 26, avg_loss = 0.5314\n",
            "t = 27, avg_loss = 0.5118\n",
            "t = 28, avg_loss = 0.6731\n",
            "t = 29, avg_loss = 0.5727\n",
            "t = 30, avg_loss = 0.4983\n",
            "Checking accuracy on test set\n",
            "Got 128 / 240 correct (53.33)\n",
            "acc = 0.533333\n",
            "Starting epoch 44 / 50\n",
            "t = 1, avg_loss = 0.5720\n",
            "t = 2, avg_loss = 0.4848\n",
            "t = 3, avg_loss = 0.4919\n",
            "t = 4, avg_loss = 0.5295\n",
            "t = 5, avg_loss = 0.5389\n",
            "t = 6, avg_loss = 0.5342\n",
            "t = 7, avg_loss = 0.5574\n",
            "t = 8, avg_loss = 0.4640\n",
            "t = 9, avg_loss = 0.4950\n",
            "t = 10, avg_loss = 0.5234\n",
            "t = 11, avg_loss = 0.5166\n",
            "t = 12, avg_loss = 0.5314\n",
            "t = 13, avg_loss = 0.4982\n",
            "t = 14, avg_loss = 0.5184\n",
            "t = 15, avg_loss = 0.5114\n",
            "t = 16, avg_loss = 0.4809\n",
            "t = 17, avg_loss = 0.5450\n",
            "t = 18, avg_loss = 0.5246\n",
            "t = 19, avg_loss = 0.4859\n",
            "t = 20, avg_loss = 0.6650\n",
            "t = 21, avg_loss = 0.6870\n",
            "t = 22, avg_loss = 0.4873\n",
            "t = 23, avg_loss = 0.5201\n",
            "t = 24, avg_loss = 0.5130\n",
            "t = 25, avg_loss = 0.6346\n",
            "t = 26, avg_loss = 0.5143\n",
            "t = 27, avg_loss = 0.5579\n",
            "t = 28, avg_loss = 0.5017\n",
            "t = 29, avg_loss = 0.5158\n",
            "t = 30, avg_loss = 0.5310\n",
            "Checking accuracy on test set\n",
            "Got 115 / 240 correct (47.92)\n",
            "acc = 0.479167\n",
            "Starting epoch 45 / 50\n",
            "t = 1, avg_loss = 0.5321\n",
            "t = 2, avg_loss = 0.5406\n",
            "t = 3, avg_loss = 0.4543\n",
            "t = 4, avg_loss = 0.5834\n",
            "t = 5, avg_loss = 0.4654\n",
            "t = 6, avg_loss = 0.6482\n",
            "t = 7, avg_loss = 0.5194\n",
            "t = 8, avg_loss = 0.4771\n",
            "t = 9, avg_loss = 0.5247\n",
            "t = 10, avg_loss = 0.5649\n",
            "t = 11, avg_loss = 0.5517\n",
            "t = 12, avg_loss = 0.5382\n",
            "t = 13, avg_loss = 0.4643\n",
            "t = 14, avg_loss = 0.4368\n",
            "t = 15, avg_loss = 0.5819\n",
            "t = 16, avg_loss = 0.6353\n",
            "t = 17, avg_loss = 0.5169\n",
            "t = 18, avg_loss = 0.5004\n",
            "t = 19, avg_loss = 0.4215\n",
            "t = 20, avg_loss = 0.5261\n",
            "t = 21, avg_loss = 0.5284\n",
            "t = 22, avg_loss = 0.5241\n",
            "t = 23, avg_loss = 0.4567\n",
            "t = 24, avg_loss = 0.5088\n",
            "t = 25, avg_loss = 0.5226\n",
            "t = 26, avg_loss = 0.4344\n",
            "t = 27, avg_loss = 0.5049\n",
            "t = 28, avg_loss = 0.5744\n",
            "t = 29, avg_loss = 0.4559\n",
            "t = 30, avg_loss = 0.5333\n",
            "Checking accuracy on test set\n",
            "Got 175 / 240 correct (72.92)\n",
            "acc = 0.729167\n",
            "Starting epoch 46 / 50\n",
            "t = 1, avg_loss = 0.5348\n",
            "t = 2, avg_loss = 0.5229\n",
            "t = 3, avg_loss = 0.4860\n",
            "t = 4, avg_loss = 0.4738\n",
            "t = 5, avg_loss = 0.4566\n",
            "t = 6, avg_loss = 0.4597\n",
            "t = 7, avg_loss = 0.4459\n",
            "t = 8, avg_loss = 0.4952\n",
            "t = 9, avg_loss = 0.4369\n",
            "t = 10, avg_loss = 0.4834\n",
            "t = 11, avg_loss = 0.6116\n",
            "t = 12, avg_loss = 0.5630\n",
            "t = 13, avg_loss = 0.5283\n",
            "t = 14, avg_loss = 0.5072\n",
            "t = 15, avg_loss = 0.4153\n",
            "t = 16, avg_loss = 0.4755\n",
            "t = 17, avg_loss = 0.3898\n",
            "t = 18, avg_loss = 0.5397\n",
            "t = 19, avg_loss = 0.5669\n",
            "t = 20, avg_loss = 0.5501\n",
            "t = 21, avg_loss = 0.4151\n",
            "t = 22, avg_loss = 0.5744\n",
            "t = 23, avg_loss = 0.4504\n",
            "t = 24, avg_loss = 0.4894\n",
            "t = 25, avg_loss = 0.6102\n",
            "t = 26, avg_loss = 0.4210\n",
            "t = 27, avg_loss = 0.5179\n",
            "t = 28, avg_loss = 0.4191\n",
            "t = 29, avg_loss = 0.5973\n",
            "t = 30, avg_loss = 0.5168\n",
            "Checking accuracy on test set\n",
            "Got 161 / 240 correct (67.08)\n",
            "acc = 0.670833\n",
            "Starting epoch 47 / 50\n",
            "t = 1, avg_loss = 0.4003\n",
            "t = 2, avg_loss = 0.4705\n",
            "t = 3, avg_loss = 0.4304\n",
            "t = 4, avg_loss = 0.4297\n",
            "t = 5, avg_loss = 0.4924\n",
            "t = 6, avg_loss = 0.4892\n",
            "t = 7, avg_loss = 0.5181\n",
            "t = 8, avg_loss = 0.5200\n",
            "t = 9, avg_loss = 0.4546\n",
            "t = 10, avg_loss = 0.5268\n",
            "t = 11, avg_loss = 0.3875\n",
            "t = 12, avg_loss = 0.4810\n",
            "t = 13, avg_loss = 0.5392\n",
            "t = 14, avg_loss = 0.4942\n",
            "t = 15, avg_loss = 0.6619\n",
            "t = 16, avg_loss = 0.4990\n",
            "t = 17, avg_loss = 0.7543\n",
            "t = 18, avg_loss = 0.5414\n",
            "t = 19, avg_loss = 0.5566\n",
            "t = 20, avg_loss = 0.4463\n",
            "t = 21, avg_loss = 0.6387\n",
            "t = 22, avg_loss = 0.5748\n",
            "t = 23, avg_loss = 0.3919\n",
            "t = 24, avg_loss = 0.4604\n",
            "t = 25, avg_loss = 0.5309\n",
            "t = 26, avg_loss = 0.5758\n",
            "t = 27, avg_loss = 0.4849\n",
            "t = 28, avg_loss = 0.4524\n",
            "t = 29, avg_loss = 0.6022\n",
            "t = 30, avg_loss = 0.4886\n",
            "Checking accuracy on test set\n",
            "Got 118 / 240 correct (49.17)\n",
            "acc = 0.491667\n",
            "Starting epoch 48 / 50\n",
            "t = 1, avg_loss = 0.4943\n",
            "t = 2, avg_loss = 0.6289\n",
            "t = 3, avg_loss = 0.5007\n",
            "t = 4, avg_loss = 0.5600\n",
            "t = 5, avg_loss = 0.4036\n",
            "t = 6, avg_loss = 0.5310\n",
            "t = 7, avg_loss = 0.5508\n",
            "t = 8, avg_loss = 0.4441\n",
            "t = 9, avg_loss = 0.4565\n",
            "t = 10, avg_loss = 0.6073\n",
            "t = 11, avg_loss = 0.5070\n",
            "t = 12, avg_loss = 0.4452\n",
            "t = 13, avg_loss = 0.4943\n",
            "t = 14, avg_loss = 0.4614\n",
            "t = 15, avg_loss = 0.4943\n",
            "t = 16, avg_loss = 0.4183\n",
            "t = 17, avg_loss = 0.4302\n",
            "t = 18, avg_loss = 0.5358\n",
            "t = 19, avg_loss = 0.5389\n",
            "t = 20, avg_loss = 0.4827\n",
            "t = 21, avg_loss = 0.5184\n",
            "t = 22, avg_loss = 0.4677\n",
            "t = 23, avg_loss = 0.5812\n",
            "t = 24, avg_loss = 0.5355\n",
            "t = 25, avg_loss = 0.4569\n",
            "t = 26, avg_loss = 0.6378\n",
            "t = 27, avg_loss = 0.4343\n",
            "t = 28, avg_loss = 0.5176\n",
            "t = 29, avg_loss = 0.5687\n",
            "t = 30, avg_loss = 0.5127\n",
            "Checking accuracy on test set\n",
            "Got 145 / 240 correct (60.42)\n",
            "acc = 0.604167\n",
            "Starting epoch 49 / 50\n",
            "t = 1, avg_loss = 0.4807\n",
            "t = 2, avg_loss = 0.4298\n",
            "t = 3, avg_loss = 0.7073\n",
            "t = 4, avg_loss = 0.3958\n",
            "t = 5, avg_loss = 0.5238\n",
            "t = 6, avg_loss = 0.4038\n",
            "t = 7, avg_loss = 0.5015\n",
            "t = 8, avg_loss = 0.3945\n",
            "t = 9, avg_loss = 0.5476\n",
            "t = 10, avg_loss = 0.4033\n",
            "t = 11, avg_loss = 0.4574\n",
            "t = 12, avg_loss = 0.4123\n",
            "t = 13, avg_loss = 0.5629\n",
            "t = 14, avg_loss = 0.5283\n",
            "t = 15, avg_loss = 0.4255\n",
            "t = 16, avg_loss = 0.6023\n",
            "t = 17, avg_loss = 0.4877\n",
            "t = 18, avg_loss = 0.4611\n",
            "t = 19, avg_loss = 0.4205\n",
            "t = 20, avg_loss = 0.4660\n",
            "t = 21, avg_loss = 0.5160\n",
            "t = 22, avg_loss = 0.5446\n",
            "t = 23, avg_loss = 0.5085\n",
            "t = 24, avg_loss = 0.4664\n",
            "t = 25, avg_loss = 0.5358\n",
            "t = 26, avg_loss = 0.4709\n",
            "t = 27, avg_loss = 0.4635\n",
            "t = 28, avg_loss = 0.5178\n",
            "t = 29, avg_loss = 0.5541\n",
            "t = 30, avg_loss = 0.3780\n",
            "Checking accuracy on test set\n",
            "Got 147 / 240 correct (61.25)\n",
            "acc = 0.612500\n",
            "Starting epoch 50 / 50\n",
            "t = 1, avg_loss = 0.7691\n",
            "t = 2, avg_loss = 0.4903\n",
            "t = 3, avg_loss = 0.4914\n",
            "t = 4, avg_loss = 0.5161\n",
            "t = 5, avg_loss = 0.5553\n",
            "t = 6, avg_loss = 0.5592\n",
            "t = 7, avg_loss = 0.4715\n",
            "t = 8, avg_loss = 0.6229\n",
            "t = 9, avg_loss = 0.3953\n",
            "t = 10, avg_loss = 0.4679\n",
            "t = 11, avg_loss = 0.4662\n",
            "t = 12, avg_loss = 0.4945\n",
            "t = 13, avg_loss = 0.5090\n",
            "t = 14, avg_loss = 0.4260\n",
            "t = 15, avg_loss = 0.5238\n",
            "t = 16, avg_loss = 0.4673\n",
            "t = 17, avg_loss = 0.4167\n",
            "t = 18, avg_loss = 0.4451\n",
            "t = 19, avg_loss = 0.5341\n",
            "t = 20, avg_loss = 0.5472\n",
            "t = 21, avg_loss = 0.5588\n",
            "t = 22, avg_loss = 0.4472\n",
            "t = 23, avg_loss = 0.6804\n",
            "t = 24, avg_loss = 0.4316\n",
            "t = 25, avg_loss = 0.4483\n",
            "t = 26, avg_loss = 0.5345\n",
            "t = 27, avg_loss = 0.3908\n",
            "t = 28, avg_loss = 0.4394\n",
            "t = 29, avg_loss = 0.4800\n",
            "t = 30, avg_loss = 0.5918\n",
            "Checking accuracy on test set\n",
            "Got 152 / 240 correct (63.33)\n",
            "acc = 0.633333\n",
            "Checking accuracy on test set\n",
            "Got 156 / 240 correct (65.00)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.65"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDn-2g-TLY3k",
        "colab_type": "code",
        "outputId": "c1f9c881-2f9b-49c5-90b8-f0b17a5f1508",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot([print_every*batch_size*(i+1) for i in range((len(avg_loss_list)))],avg_loss_list)\n",
        "print(\"Loss:\")\n",
        "plt.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD5CAYAAAA3Os7hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0BmGYZIEFnSGYZgkgQWdYRgmSfh/PJHHoZ88ky8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGer6hJv7-zR",
        "colab_type": "code",
        "outputId": "c1f93b3a-3cf0-4596-923a-3f14055bf48e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "plt.plot([i+1 for i in range((len(acc_list)))],acc_list)\n",
        "print(\"Accurancy:\")\n",
        "plt.show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accurancy:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9BERNZegj4iouQR9RETN/QWg5aGHBKGKmgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBTReWHxz4Ml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}