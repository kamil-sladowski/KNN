{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KNN3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM8mzbGXDP8KUi0U7W5h6Ek",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kamilo116/KNN/blob/master/KNN3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7ReM7FU-xs4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import os\n",
        "import csv\n",
        "import sys\n",
        "from random import shuffle\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from matplotlib.pyplot import imshow\n",
        "%matplotlib inline\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import sampler\n",
        "import torchvision\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "import timeit\n",
        "\n",
        "np.random.seed(4) \n",
        "torch.manual_seed(4) \n",
        "torch.cuda.manual_seed(4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMOHez_kHZD6",
        "colab_type": "code",
        "outputId": "2bd5aa43-68e4-428b-eb8e-dd893b7c7517",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive, files\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYIMZ8QyYewD",
        "colab_type": "code",
        "outputId": "b82591cc-5883-48fa-b3a5-0c9e1df60aba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "! git clone https://github.com/wang-chen/kervolution.git \n",
        "\n",
        "sys.path.append(\"kervolution/\")\n",
        "from kervolution import Kerv2d\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'kervolution' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b8P1NdXfVdX",
        "colab_type": "code",
        "outputId": "97c7dd1a-b0bb-471d-90b6-b24acf318c7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "MALIGNANT_DATASET = '/content/drive/My Drive/Colab_data/malignant/malignant/'\n",
        "BENIGN_DATASET = '/content/drive/My Drive/Colab_data/benign/benign/'\n",
        "DATA_FOLDER = '/content/drive/My Drive/Colab_data/'\n",
        "LIMIT_IMAGES_NUM = 600\n",
        "\n",
        "benign_file_list = sorted(os.listdir(BENIGN_DATASET))[:LIMIT_IMAGES_NUM]\n",
        "malignant_file_list = sorted(os.listdir(MALIGNANT_DATASET))[:LIMIT_IMAGES_NUM]\n",
        "shuffle(benign_file_list)\n",
        "shuffle(malignant_file_list)\n",
        "\n",
        "print(f\"Number of benign {len(benign_file_list)} images\")\n",
        "print(f\"Number of malignant {len(malignant_file_list)} images\")\n",
        "\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize((100, 100)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of benign 600 images\n",
            "Number of malignant 600 images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGDQfal74BLi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "benign_dict = {filename: 0 for filename in benign_file_list}\n",
        "malignant_dict = {filename: 1 for filename in malignant_file_list}\n",
        "img_class_dict = {**benign_dict , **malignant_dict}\n",
        "labeled_data = pd.Series(img_class_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvAAOqmVfVll",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class IsicDataset(Dataset):\n",
        "    def __init__(self, data_folder, labeled_data, \n",
        "                 transform=transforms.Compose([transforms.ToTensor()])):\n",
        "        self.labeled_data = labeled_data\n",
        "        self.transform = transform\n",
        "        self.data_folder = data_folder\n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.labeled_data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        label = self.labeled_data[index]\n",
        "        if label == 0:\n",
        "          image = Image.open(os.path.join(self.data_folder, \"benign\", \"benign\", index ))\n",
        "        else:\n",
        "          image = Image.open(os.path.join(self.data_folder, \"malignant\", \"malignant\", index ))\n",
        "        image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "    @property\n",
        "    def labels(self):\n",
        "      return self.labeled_data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSuYqjLAYrWA",
        "colab_type": "code",
        "outputId": "6be9f57f-6898-4339-b890-c9a6623b0f1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        }
      },
      "source": [
        " \n",
        "dataset = IsicDataset(DATA_FOLDER, labeled_data, transform=data_transforms)\n",
        "print(dataset.labels)\n",
        "\n",
        "X_train, X_test = train_test_split(dataset.labels, test_size=0.2)\n",
        "print(\"number of training data: \",len(X_train))\n",
        "print(\"number of testing  data: \",len(X_test))\n",
        "\n",
        "train_sampler = SubsetRandomSampler(list(X_train.index))\n",
        "valid_sampler = SubsetRandomSampler(list(X_test.index))\n",
        "batch_size = 32\n",
        "num_workers = 0\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers)\n",
        "valid_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler, num_workers=num_workers)\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ISIC_0000000.jpeg    0\n",
            "ISIC_0000416.jpeg    0\n",
            "ISIC_0000662.jpeg    0\n",
            "ISIC_0000495.jpeg    0\n",
            "ISIC_0000350.jpeg    0\n",
            "                    ..\n",
            "ISIC_0010067.jpeg    1\n",
            "ISIC_0011471.jpeg    1\n",
            "ISIC_0000554.jpeg    1\n",
            "ISIC_0009905.jpeg    1\n",
            "ISIC_0010714.jpeg    1\n",
            "Length: 1200, dtype: int64\n",
            "number of training data:  960\n",
            "number of testing  data:  240\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_e8nFBR6Yug5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "avg_loss_list = []\n",
        "acc_list = []\n",
        "\n",
        "def train(model, train_loader ,loss_fn, optimizer, num_epochs = 1):\n",
        "    total_loss =0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Starting epoch %d / %d' % (epoch + 1, num_epochs))\n",
        "        model.train()\n",
        "\n",
        "        for t, (x, y) in enumerate(train_loader):\n",
        "            x_var = Variable(x.type(gpu_dtype))\n",
        "            y_var = Variable(y.type(gpu_dtype).long())\n",
        "            scores = model(x_var)\n",
        "            loss = loss_fn(scores, y_var)\n",
        "            total_loss += loss.data\n",
        "            \n",
        "            if (t + 1) % print_every == 0:\n",
        "                avg_loss = total_loss/print_every\n",
        "                print('t = %d, avg_loss = %.4f' % (t + 1, avg_loss) )\n",
        "                avg_loss_list.append(avg_loss)\n",
        "                total_loss = 0\n",
        "                \n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        acc = check_accuracy(fixed_model_gpu, valid_loader)\n",
        "        print('acc = %f' %(acc))\n",
        "            \n",
        "def check_accuracy(model, loader):\n",
        "    print('Checking accuracy on test set')   \n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval() \n",
        "    for x, y in loader:\n",
        "        x_var = Variable(x.type(gpu_dtype))\n",
        "\n",
        "        scores = model(x_var)\n",
        "        _, preds = scores.data.cpu().max(1)\n",
        "        num_correct += (preds == y).sum()\n",
        "        num_samples += preds.size(0)\n",
        "    acc = float(num_correct) / num_samples\n",
        "    acc_list.append(acc)\n",
        "    print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
        "    return acc\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggvpMu36Yw2D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Flatten(nn.Module):\n",
        "    def forward(self, x):\n",
        "        N, C, H, W = x.size()\n",
        "        return x.view(N, -1)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgmexhBRZAK6",
        "colab_type": "code",
        "outputId": "f8f51aa9-50ed-45ee-fe5c-36be33c3b11c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "\n",
        "print_every = 1\n",
        "gpu_dtype = torch.cuda.FloatTensor\n",
        "\n",
        "out_0 = 90\n",
        "out_1 = 64\n",
        "out_2 = 48\n",
        "out_3 = 32\n",
        "out_4 = 24\n",
        "#out_5 = 16\n",
        "\n",
        "k_size_1 = 3\n",
        "padding_1 = 1\n",
        "in_channels = 3\n",
        "\n",
        "num_epochs = 50\n",
        "\n",
        "'''\n",
        "Kerv2d\n",
        "kervolution with following options:\n",
        "kernel_type: [linear, polynomial, gaussian, etc.]\n",
        "default is convolution:\n",
        "          kernel_type --> linear,\n",
        "balance, power, gamma is valid only when the kernel_type is specified\n",
        "if learnable_kernel = True,  they just be the initial value of learable parameters\n",
        "if learnable_kernel = False, they are the value of kernel_type's parameter\n",
        "the parameter [power] cannot be learned due to integer limitation\n",
        "dilation (int or tuple, optional): Spacing between kernel\n",
        "elements. Default: 1\n",
        "groups (int, optional): Number of blocked connections from input\n",
        "channels to output channels. Default: 1\n",
        "bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``\n",
        "kernel_type (str), Default: 'linear'\n",
        "learnable_kernel (bool): Learnable kernel parameters.  Default: False \n",
        "balance: 0, 1\n",
        "power: 3, 4, 5\n",
        "gamma:\n",
        "'''\n",
        "\n",
        "\n",
        "fixed_model_base = nn.Sequential( \n",
        "                nn.Kerv2d(in_channels , out_0, padding=padding_1, dilation=1, groups=1, bias=True, \n",
        "                          kernel_type='polynomial', kernel_size=k_size_1, learnable_kernel=True,\n",
        "                          kernel_regularizer=True, stride=1, balance=0, power=3, gamma=1), \n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.BatchNorm2d(out_0),\n",
        "                nn.Conv2d(out_0 , out_1, padding= padding_1, kernel_size=k_size_1, stride=1), \n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.BatchNorm2d(out_1),\n",
        "                nn.MaxPool2d(2, stride=2),\n",
        "                nn.Conv2d(out_1 , out_2, padding= padding_1, kernel_size=k_size_1, stride=1), \n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.BatchNorm2d(out_2),\n",
        "                nn.MaxPool2d(2, stride=2),\n",
        "                nn.Conv2d(out_2 , out_3, padding= padding_1, kernel_size=k_size_1, stride=1),  \n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.BatchNorm2d(out_3),\n",
        "                nn.MaxPool2d(2, stride=2),\n",
        "                nn.Conv2d(out_3 , out_4, padding= padding_1, kernel_size=k_size_1, stride=1), \n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.BatchNorm2d(out_4),\n",
        "                #nn.Conv2d(out_4 , out_5, padding= padding_1, kernel_size=k_size_1, stride=1), \n",
        "                #nn.ReLU(inplace=True),\n",
        "                #nn.BatchNorm2d(out_5),\n",
        "                nn.MaxPool2d(2, stride=2),\n",
        "                nn.Dropout(0.5),\n",
        "                Flatten(),\n",
        "                nn.Linear(864,64),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Linear(64,10),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Linear(10,2)\n",
        "            )\n",
        "fixed_model_gpu = fixed_model_base.type(gpu_dtype)\n",
        "print(fixed_model_gpu)\n",
        "loss_fn = nn.modules.loss.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(fixed_model_gpu.parameters(), lr = 0.00115, weight_decay=0.02) \n",
        "\n",
        "train(fixed_model_gpu, train_loader ,loss_fn, optimizer, num_epochs=num_epochs)\n",
        "check_accuracy(fixed_model_gpu, valid_loader)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Kerv2d(3, 90, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (1): ReLU(inplace=True)\n",
            "  (2): BatchNorm2d(90, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (3): Conv2d(90, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (4): ReLU(inplace=True)\n",
            "  (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (7): Conv2d(64, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (8): ReLU(inplace=True)\n",
            "  (9): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (11): Conv2d(48, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (12): ReLU(inplace=True)\n",
            "  (13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (15): Conv2d(32, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (16): ReLU(inplace=True)\n",
            "  (17): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (19): Dropout(p=0.5, inplace=False)\n",
            "  (20): Flatten()\n",
            "  (21): Linear(in_features=864, out_features=64, bias=True)\n",
            "  (22): ReLU(inplace=True)\n",
            "  (23): Linear(in_features=64, out_features=10, bias=True)\n",
            "  (24): ReLU(inplace=True)\n",
            "  (25): Linear(in_features=10, out_features=2, bias=True)\n",
            ")\n",
            "Starting epoch 1 / 50\n",
            "ERROR! Session/line number was not unique in database. History logging moved to new session 63\n",
            "t = 1, avg_loss = 0.6813\n",
            "t = 2, avg_loss = 0.6625\n",
            "t = 3, avg_loss = 0.7180\n",
            "t = 4, avg_loss = 0.7210\n",
            "t = 5, avg_loss = 0.6076\n",
            "t = 6, avg_loss = 0.6205\n",
            "t = 7, avg_loss = 0.5739\n",
            "t = 8, avg_loss = 0.6211\n",
            "t = 9, avg_loss = 0.5681\n",
            "t = 10, avg_loss = 0.6397\n",
            "t = 11, avg_loss = 0.5724\n",
            "t = 12, avg_loss = 0.6013\n",
            "t = 13, avg_loss = 0.5899\n",
            "t = 14, avg_loss = 0.5307\n",
            "t = 15, avg_loss = 0.4919\n",
            "t = 16, avg_loss = 0.6056\n",
            "t = 17, avg_loss = 0.5898\n",
            "t = 18, avg_loss = 0.6684\n",
            "t = 19, avg_loss = 0.6269\n",
            "t = 20, avg_loss = 0.5342\n",
            "t = 21, avg_loss = 0.5571\n",
            "t = 22, avg_loss = 0.5145\n",
            "t = 23, avg_loss = 0.8580\n",
            "t = 24, avg_loss = 0.5688\n",
            "t = 25, avg_loss = 0.6263\n",
            "t = 26, avg_loss = 0.6098\n",
            "t = 27, avg_loss = 0.6274\n",
            "t = 28, avg_loss = 0.5342\n",
            "t = 29, avg_loss = 0.5438\n",
            "t = 30, avg_loss = 0.4969\n",
            "Checking accuracy on test set\n",
            "Got 118 / 240 correct (49.17)\n",
            "acc = 0.491667\n",
            "Starting epoch 2 / 50\n",
            "t = 1, avg_loss = 0.7412\n",
            "t = 2, avg_loss = 0.6073\n",
            "t = 3, avg_loss = 0.5320\n",
            "t = 4, avg_loss = 0.4478\n",
            "t = 5, avg_loss = 0.4938\n",
            "t = 6, avg_loss = 0.5463\n",
            "t = 7, avg_loss = 0.4281\n",
            "t = 8, avg_loss = 0.5866\n",
            "t = 9, avg_loss = 0.4865\n",
            "t = 10, avg_loss = 0.4761\n",
            "t = 11, avg_loss = 0.3823\n",
            "t = 12, avg_loss = 0.6177\n",
            "t = 13, avg_loss = 0.6426\n",
            "t = 14, avg_loss = 0.5337\n",
            "t = 15, avg_loss = 0.3836\n",
            "t = 16, avg_loss = 0.6692\n",
            "t = 17, avg_loss = 0.4259\n",
            "t = 18, avg_loss = 0.5538\n",
            "t = 19, avg_loss = 0.4760\n",
            "t = 20, avg_loss = 0.3864\n",
            "t = 21, avg_loss = 0.6233\n",
            "t = 22, avg_loss = 0.4728\n",
            "t = 23, avg_loss = 0.5561\n",
            "t = 24, avg_loss = 0.4478\n",
            "t = 25, avg_loss = 0.5501\n",
            "t = 26, avg_loss = 0.4371\n",
            "t = 27, avg_loss = 0.4314\n",
            "t = 28, avg_loss = 0.3855\n",
            "t = 29, avg_loss = 0.4549\n",
            "t = 30, avg_loss = 0.3659\n",
            "Checking accuracy on test set\n",
            "Got 131 / 240 correct (54.58)\n",
            "acc = 0.545833\n",
            "Starting epoch 3 / 50\n",
            "t = 1, avg_loss = 0.5876\n",
            "t = 2, avg_loss = 0.4240\n",
            "t = 3, avg_loss = 0.4029\n",
            "t = 4, avg_loss = 0.3120\n",
            "t = 5, avg_loss = 0.4409\n",
            "t = 6, avg_loss = 0.4978\n",
            "t = 7, avg_loss = 0.3684\n",
            "t = 8, avg_loss = 0.7310\n",
            "t = 9, avg_loss = 0.3732\n",
            "t = 10, avg_loss = 0.8189\n",
            "t = 11, avg_loss = 0.4568\n",
            "t = 12, avg_loss = 0.5426\n",
            "t = 13, avg_loss = 0.4388\n",
            "t = 14, avg_loss = 0.4239\n",
            "t = 15, avg_loss = 0.7211\n",
            "t = 16, avg_loss = 0.4793\n",
            "t = 17, avg_loss = 0.3135\n",
            "t = 18, avg_loss = 0.5342\n",
            "t = 19, avg_loss = 0.5988\n",
            "t = 20, avg_loss = 0.4963\n",
            "t = 21, avg_loss = 0.3702\n",
            "t = 22, avg_loss = 0.4308\n",
            "t = 23, avg_loss = 0.5792\n",
            "t = 24, avg_loss = 0.3700\n",
            "t = 25, avg_loss = 0.3735\n",
            "t = 26, avg_loss = 0.5741\n",
            "t = 27, avg_loss = 0.3676\n",
            "t = 28, avg_loss = 0.4836\n",
            "t = 29, avg_loss = 0.2914\n",
            "t = 30, avg_loss = 0.3761\n",
            "Checking accuracy on test set\n",
            "Got 190 / 240 correct (79.17)\n",
            "acc = 0.791667\n",
            "Starting epoch 4 / 50\n",
            "t = 1, avg_loss = 0.3685\n",
            "t = 2, avg_loss = 0.5127\n",
            "t = 3, avg_loss = 0.4248\n",
            "t = 4, avg_loss = 0.5262\n",
            "t = 5, avg_loss = 0.3771\n",
            "t = 6, avg_loss = 0.4107\n",
            "t = 7, avg_loss = 0.4146\n",
            "t = 8, avg_loss = 0.3721\n",
            "t = 9, avg_loss = 0.3756\n",
            "t = 10, avg_loss = 0.6385\n",
            "t = 11, avg_loss = 0.4068\n",
            "t = 12, avg_loss = 0.5094\n",
            "t = 13, avg_loss = 0.3364\n",
            "t = 14, avg_loss = 0.4432\n",
            "t = 15, avg_loss = 0.3666\n",
            "t = 16, avg_loss = 0.3118\n",
            "t = 17, avg_loss = 0.2784\n",
            "t = 18, avg_loss = 0.2878\n",
            "t = 19, avg_loss = 0.4479\n",
            "t = 20, avg_loss = 0.5565\n",
            "t = 21, avg_loss = 0.6047\n",
            "t = 22, avg_loss = 0.3503\n",
            "t = 23, avg_loss = 0.4243\n",
            "t = 24, avg_loss = 0.3452\n",
            "t = 25, avg_loss = 0.3434\n",
            "t = 26, avg_loss = 0.3166\n",
            "t = 27, avg_loss = 0.3725\n",
            "t = 28, avg_loss = 0.3153\n",
            "t = 29, avg_loss = 0.3570\n",
            "t = 30, avg_loss = 0.3267\n",
            "Checking accuracy on test set\n",
            "Got 174 / 240 correct (72.50)\n",
            "acc = 0.725000\n",
            "Starting epoch 5 / 50\n",
            "t = 1, avg_loss = 0.3668\n",
            "t = 2, avg_loss = 0.3351\n",
            "t = 3, avg_loss = 0.2888\n",
            "t = 4, avg_loss = 0.4203\n",
            "t = 5, avg_loss = 0.3427\n",
            "t = 6, avg_loss = 0.4211\n",
            "t = 7, avg_loss = 0.3348\n",
            "t = 8, avg_loss = 0.3925\n",
            "t = 9, avg_loss = 0.2531\n",
            "t = 10, avg_loss = 0.4377\n",
            "t = 11, avg_loss = 0.8178\n",
            "t = 12, avg_loss = 0.4759\n",
            "t = 13, avg_loss = 0.3948\n",
            "t = 14, avg_loss = 0.4829\n",
            "t = 15, avg_loss = 0.3012\n",
            "t = 16, avg_loss = 0.4067\n",
            "t = 17, avg_loss = 0.3712\n",
            "t = 18, avg_loss = 0.5801\n",
            "t = 19, avg_loss = 0.4175\n",
            "t = 20, avg_loss = 0.3786\n",
            "t = 21, avg_loss = 0.3680\n",
            "t = 22, avg_loss = 0.3494\n",
            "t = 23, avg_loss = 0.6472\n",
            "t = 24, avg_loss = 0.3339\n",
            "t = 25, avg_loss = 0.4624\n",
            "t = 26, avg_loss = 0.5790\n",
            "t = 27, avg_loss = 0.5229\n",
            "t = 28, avg_loss = 0.2912\n",
            "t = 29, avg_loss = 0.4948\n",
            "t = 30, avg_loss = 0.3758\n",
            "Checking accuracy on test set\n",
            "Got 178 / 240 correct (74.17)\n",
            "acc = 0.741667\n",
            "Starting epoch 6 / 50\n",
            "t = 1, avg_loss = 0.4074\n",
            "t = 2, avg_loss = 0.2187\n",
            "t = 3, avg_loss = 0.4890\n",
            "t = 4, avg_loss = 0.3716\n",
            "t = 5, avg_loss = 0.2928\n",
            "t = 6, avg_loss = 0.3964\n",
            "t = 7, avg_loss = 0.3368\n",
            "t = 8, avg_loss = 0.3069\n",
            "t = 9, avg_loss = 0.3866\n",
            "t = 10, avg_loss = 0.3441\n",
            "t = 11, avg_loss = 0.3870\n",
            "t = 12, avg_loss = 0.4105\n",
            "t = 13, avg_loss = 0.4397\n",
            "t = 14, avg_loss = 0.4566\n",
            "t = 15, avg_loss = 0.5822\n",
            "t = 16, avg_loss = 0.3884\n",
            "t = 17, avg_loss = 0.3015\n",
            "t = 18, avg_loss = 0.3671\n",
            "t = 19, avg_loss = 0.4806\n",
            "t = 20, avg_loss = 0.5294\n",
            "t = 21, avg_loss = 0.6054\n",
            "t = 22, avg_loss = 0.4001\n",
            "t = 23, avg_loss = 0.5390\n",
            "t = 24, avg_loss = 0.7297\n",
            "t = 25, avg_loss = 0.8209\n",
            "t = 26, avg_loss = 0.4501\n",
            "t = 27, avg_loss = 0.4972\n",
            "t = 28, avg_loss = 0.4920\n",
            "t = 29, avg_loss = 0.4059\n",
            "t = 30, avg_loss = 0.4600\n",
            "Checking accuracy on test set\n",
            "Got 156 / 240 correct (65.00)\n",
            "acc = 0.650000\n",
            "Starting epoch 7 / 50\n",
            "t = 1, avg_loss = 0.4036\n",
            "t = 2, avg_loss = 0.5309\n",
            "t = 3, avg_loss = 0.5209\n",
            "t = 4, avg_loss = 0.4764\n",
            "t = 5, avg_loss = 0.3906\n",
            "t = 6, avg_loss = 0.4552\n",
            "t = 7, avg_loss = 0.4087\n",
            "t = 8, avg_loss = 0.4211\n",
            "t = 9, avg_loss = 0.3768\n",
            "t = 10, avg_loss = 0.4292\n",
            "t = 11, avg_loss = 0.1965\n",
            "t = 12, avg_loss = 0.3538\n",
            "t = 13, avg_loss = 0.4209\n",
            "t = 14, avg_loss = 0.3262\n",
            "t = 15, avg_loss = 0.6696\n",
            "t = 16, avg_loss = 0.2271\n",
            "t = 17, avg_loss = 0.3651\n",
            "t = 18, avg_loss = 0.3025\n",
            "t = 19, avg_loss = 0.3746\n",
            "t = 20, avg_loss = 0.3134\n",
            "t = 21, avg_loss = 0.2731\n",
            "t = 22, avg_loss = 0.4214\n",
            "t = 23, avg_loss = 0.2038\n",
            "t = 24, avg_loss = 0.4937\n",
            "t = 25, avg_loss = 0.6036\n",
            "t = 26, avg_loss = 0.4280\n",
            "t = 27, avg_loss = 0.4341\n",
            "t = 28, avg_loss = 0.5579\n",
            "t = 29, avg_loss = 0.4361\n",
            "t = 30, avg_loss = 0.4268\n",
            "Checking accuracy on test set\n",
            "Got 183 / 240 correct (76.25)\n",
            "acc = 0.762500\n",
            "Starting epoch 8 / 50\n",
            "t = 1, avg_loss = 0.3286\n",
            "t = 2, avg_loss = 0.4241\n",
            "t = 3, avg_loss = 0.3743\n",
            "t = 4, avg_loss = 0.3477\n",
            "t = 5, avg_loss = 0.4843\n",
            "t = 6, avg_loss = 0.3013\n",
            "t = 7, avg_loss = 0.2917\n",
            "t = 8, avg_loss = 0.3086\n",
            "t = 9, avg_loss = 0.3046\n",
            "t = 10, avg_loss = 0.3085\n",
            "t = 11, avg_loss = 0.2832\n",
            "t = 12, avg_loss = 0.2462\n",
            "t = 13, avg_loss = 0.4695\n",
            "t = 14, avg_loss = 0.4083\n",
            "t = 15, avg_loss = 0.5701\n",
            "t = 16, avg_loss = 0.4460\n",
            "t = 17, avg_loss = 0.2999\n",
            "t = 18, avg_loss = 0.5562\n",
            "t = 19, avg_loss = 0.4348\n",
            "t = 20, avg_loss = 0.2378\n",
            "t = 21, avg_loss = 0.3643\n",
            "t = 22, avg_loss = 0.6967\n",
            "t = 23, avg_loss = 0.5961\n",
            "t = 24, avg_loss = 0.3496\n",
            "t = 25, avg_loss = 0.4292\n",
            "t = 26, avg_loss = 0.6083\n",
            "t = 27, avg_loss = 0.2994\n",
            "t = 28, avg_loss = 0.2918\n",
            "t = 29, avg_loss = 0.3838\n",
            "t = 30, avg_loss = 0.3248\n",
            "Checking accuracy on test set\n",
            "Got 178 / 240 correct (74.17)\n",
            "acc = 0.741667\n",
            "Starting epoch 9 / 50\n",
            "t = 1, avg_loss = 0.5110\n",
            "t = 2, avg_loss = 0.3303\n",
            "t = 3, avg_loss = 0.3942\n",
            "t = 4, avg_loss = 0.3255\n",
            "t = 5, avg_loss = 0.4982\n",
            "t = 6, avg_loss = 0.4795\n",
            "t = 7, avg_loss = 0.5231\n",
            "t = 8, avg_loss = 0.4184\n",
            "t = 9, avg_loss = 0.5218\n",
            "t = 10, avg_loss = 0.3337\n",
            "t = 11, avg_loss = 0.4058\n",
            "t = 12, avg_loss = 0.3086\n",
            "t = 13, avg_loss = 0.3888\n",
            "t = 14, avg_loss = 0.4387\n",
            "t = 15, avg_loss = 0.4475\n",
            "t = 16, avg_loss = 0.3191\n",
            "t = 17, avg_loss = 0.5069\n",
            "t = 18, avg_loss = 0.3875\n",
            "t = 19, avg_loss = 0.4569\n",
            "t = 20, avg_loss = 0.4433\n",
            "t = 21, avg_loss = 0.3387\n",
            "t = 22, avg_loss = 0.2933\n",
            "t = 23, avg_loss = 0.3298\n",
            "t = 24, avg_loss = 0.2839\n",
            "t = 25, avg_loss = 0.3551\n",
            "t = 26, avg_loss = 0.2276\n",
            "t = 27, avg_loss = 0.4311\n",
            "t = 28, avg_loss = 0.3618\n",
            "t = 29, avg_loss = 0.2101\n",
            "t = 30, avg_loss = 0.4368\n",
            "Checking accuracy on test set\n",
            "Got 143 / 240 correct (59.58)\n",
            "acc = 0.595833\n",
            "Starting epoch 10 / 50\n",
            "t = 1, avg_loss = 0.2341\n",
            "t = 2, avg_loss = 0.4722\n",
            "t = 3, avg_loss = 0.3734\n",
            "t = 4, avg_loss = 0.3092\n",
            "t = 5, avg_loss = 0.5415\n",
            "t = 6, avg_loss = 0.3040\n",
            "t = 7, avg_loss = 0.2760\n",
            "t = 8, avg_loss = 0.3920\n",
            "t = 9, avg_loss = 0.3769\n",
            "t = 10, avg_loss = 0.4529\n",
            "t = 11, avg_loss = 0.5264\n",
            "t = 12, avg_loss = 0.3458\n",
            "t = 13, avg_loss = 0.3290\n",
            "t = 14, avg_loss = 0.3750\n",
            "t = 15, avg_loss = 0.4653\n",
            "t = 16, avg_loss = 0.3787\n",
            "t = 17, avg_loss = 0.3826\n",
            "t = 18, avg_loss = 0.3552\n",
            "t = 19, avg_loss = 0.4654\n",
            "t = 20, avg_loss = 0.3991\n",
            "t = 21, avg_loss = 0.3657\n",
            "t = 22, avg_loss = 0.2924\n",
            "t = 23, avg_loss = 0.1904\n",
            "t = 24, avg_loss = 0.5959\n",
            "t = 25, avg_loss = 0.4477\n",
            "t = 26, avg_loss = 0.4039\n",
            "t = 27, avg_loss = 0.3469\n",
            "t = 28, avg_loss = 0.3679\n",
            "t = 29, avg_loss = 0.3621\n",
            "t = 30, avg_loss = 0.3434\n",
            "Checking accuracy on test set\n",
            "Got 142 / 240 correct (59.17)\n",
            "acc = 0.591667\n",
            "Starting epoch 11 / 50\n",
            "t = 1, avg_loss = 0.3533\n",
            "t = 2, avg_loss = 0.2460\n",
            "t = 3, avg_loss = 0.3481\n",
            "t = 4, avg_loss = 0.2915\n",
            "t = 5, avg_loss = 0.3719\n",
            "t = 6, avg_loss = 0.3596\n",
            "t = 7, avg_loss = 0.5021\n",
            "t = 8, avg_loss = 0.5229\n",
            "t = 9, avg_loss = 0.2338\n",
            "t = 10, avg_loss = 0.3506\n",
            "t = 11, avg_loss = 0.3154\n",
            "t = 12, avg_loss = 0.5231\n",
            "t = 13, avg_loss = 0.4972\n",
            "t = 14, avg_loss = 0.2748\n",
            "t = 15, avg_loss = 0.6060\n",
            "t = 16, avg_loss = 0.3086\n",
            "t = 17, avg_loss = 0.5048\n",
            "t = 18, avg_loss = 0.3600\n",
            "t = 19, avg_loss = 0.2773\n",
            "t = 20, avg_loss = 0.2850\n",
            "t = 21, avg_loss = 0.2839\n",
            "t = 22, avg_loss = 0.2470\n",
            "t = 23, avg_loss = 0.3656\n",
            "t = 24, avg_loss = 0.3868\n",
            "t = 25, avg_loss = 0.3563\n",
            "t = 26, avg_loss = 0.3285\n",
            "t = 27, avg_loss = 0.4371\n",
            "t = 28, avg_loss = 0.4921\n",
            "t = 29, avg_loss = 0.3265\n",
            "t = 30, avg_loss = 0.3838\n",
            "Checking accuracy on test set\n",
            "Got 194 / 240 correct (80.83)\n",
            "acc = 0.808333\n",
            "Starting epoch 12 / 50\n",
            "t = 1, avg_loss = 0.2590\n",
            "t = 2, avg_loss = 0.4091\n",
            "t = 3, avg_loss = 0.3234\n",
            "t = 4, avg_loss = 0.2566\n",
            "t = 5, avg_loss = 0.4480\n",
            "t = 6, avg_loss = 0.3876\n",
            "t = 7, avg_loss = 0.3591\n",
            "t = 8, avg_loss = 0.4143\n",
            "t = 9, avg_loss = 0.3626\n",
            "t = 10, avg_loss = 0.3158\n",
            "t = 11, avg_loss = 0.4140\n",
            "t = 12, avg_loss = 0.3395\n",
            "t = 13, avg_loss = 0.3456\n",
            "t = 14, avg_loss = 0.3811\n",
            "t = 15, avg_loss = 0.2171\n",
            "t = 16, avg_loss = 0.3810\n",
            "t = 17, avg_loss = 0.3686\n",
            "t = 18, avg_loss = 0.3208\n",
            "t = 19, avg_loss = 0.4734\n",
            "t = 20, avg_loss = 0.2317\n",
            "t = 21, avg_loss = 0.5218\n",
            "t = 22, avg_loss = 0.4509\n",
            "t = 23, avg_loss = 0.5566\n",
            "t = 24, avg_loss = 0.3763\n",
            "t = 25, avg_loss = 0.3799\n",
            "t = 26, avg_loss = 0.4743\n",
            "t = 27, avg_loss = 0.4744\n",
            "t = 28, avg_loss = 0.3518\n",
            "t = 29, avg_loss = 0.6573\n",
            "t = 30, avg_loss = 0.3332\n",
            "Checking accuracy on test set\n",
            "Got 185 / 240 correct (77.08)\n",
            "acc = 0.770833\n",
            "Starting epoch 13 / 50\n",
            "t = 1, avg_loss = 0.4345\n",
            "t = 2, avg_loss = 0.3873\n",
            "t = 3, avg_loss = 0.3693\n",
            "t = 4, avg_loss = 0.4624\n",
            "t = 5, avg_loss = 0.3230\n",
            "t = 6, avg_loss = 0.4115\n",
            "t = 7, avg_loss = 0.3730\n",
            "t = 8, avg_loss = 0.4015\n",
            "t = 9, avg_loss = 0.4368\n",
            "t = 10, avg_loss = 0.6815\n",
            "t = 11, avg_loss = 0.3435\n",
            "t = 12, avg_loss = 0.2954\n",
            "t = 13, avg_loss = 0.5188\n",
            "t = 14, avg_loss = 0.3527\n",
            "t = 15, avg_loss = 0.2842\n",
            "t = 16, avg_loss = 0.2680\n",
            "t = 17, avg_loss = 0.4093\n",
            "t = 18, avg_loss = 0.3302\n",
            "t = 19, avg_loss = 0.2833\n",
            "t = 20, avg_loss = 0.3861\n",
            "t = 21, avg_loss = 0.3513\n",
            "t = 22, avg_loss = 0.3012\n",
            "t = 23, avg_loss = 0.3579\n",
            "t = 24, avg_loss = 0.3640\n",
            "t = 25, avg_loss = 0.5338\n",
            "t = 26, avg_loss = 0.2850\n",
            "t = 27, avg_loss = 0.4663\n",
            "t = 28, avg_loss = 0.3763\n",
            "t = 29, avg_loss = 0.3216\n",
            "t = 30, avg_loss = 0.6181\n",
            "Checking accuracy on test set\n",
            "Got 194 / 240 correct (80.83)\n",
            "acc = 0.808333\n",
            "Starting epoch 14 / 50\n",
            "t = 1, avg_loss = 0.1967\n",
            "t = 2, avg_loss = 0.4713\n",
            "t = 3, avg_loss = 0.4295\n",
            "t = 4, avg_loss = 0.5200\n",
            "t = 5, avg_loss = 0.3879\n",
            "t = 6, avg_loss = 0.4681\n",
            "t = 7, avg_loss = 0.3435\n",
            "t = 8, avg_loss = 0.3359\n",
            "t = 9, avg_loss = 0.3977\n",
            "t = 10, avg_loss = 0.3227\n",
            "t = 11, avg_loss = 0.3355\n",
            "t = 12, avg_loss = 0.2501\n",
            "t = 13, avg_loss = 0.3971\n",
            "t = 14, avg_loss = 0.3544\n",
            "t = 15, avg_loss = 0.4297\n",
            "t = 16, avg_loss = 0.4309\n",
            "t = 17, avg_loss = 0.3783\n",
            "t = 18, avg_loss = 0.2295\n",
            "t = 19, avg_loss = 0.4048\n",
            "t = 20, avg_loss = 0.3138\n",
            "t = 21, avg_loss = 0.4875\n",
            "t = 22, avg_loss = 0.3313\n",
            "t = 23, avg_loss = 0.3759\n",
            "t = 24, avg_loss = 0.4015\n",
            "t = 25, avg_loss = 0.4424\n",
            "t = 26, avg_loss = 0.3954\n",
            "t = 27, avg_loss = 0.3671\n",
            "t = 28, avg_loss = 0.3495\n",
            "t = 29, avg_loss = 0.3451\n",
            "t = 30, avg_loss = 0.2673\n",
            "Checking accuracy on test set\n",
            "Got 141 / 240 correct (58.75)\n",
            "acc = 0.587500\n",
            "Starting epoch 15 / 50\n",
            "t = 1, avg_loss = 0.2892\n",
            "t = 2, avg_loss = 0.2091\n",
            "t = 3, avg_loss = 0.3734\n",
            "t = 4, avg_loss = 0.4286\n",
            "t = 5, avg_loss = 0.3384\n",
            "t = 6, avg_loss = 0.6836\n",
            "t = 7, avg_loss = 0.2457\n",
            "t = 8, avg_loss = 0.3033\n",
            "t = 9, avg_loss = 0.3566\n",
            "t = 10, avg_loss = 0.4331\n",
            "t = 11, avg_loss = 0.3130\n",
            "t = 12, avg_loss = 0.3603\n",
            "t = 13, avg_loss = 0.4573\n",
            "t = 14, avg_loss = 0.4108\n",
            "t = 15, avg_loss = 0.2389\n",
            "t = 16, avg_loss = 0.2544\n",
            "t = 17, avg_loss = 0.3040\n",
            "t = 18, avg_loss = 0.3678\n",
            "t = 19, avg_loss = 0.2992\n",
            "t = 20, avg_loss = 0.4399\n",
            "t = 21, avg_loss = 0.4618\n",
            "t = 22, avg_loss = 0.2958\n",
            "t = 23, avg_loss = 0.1991\n",
            "t = 24, avg_loss = 0.3277\n",
            "t = 25, avg_loss = 0.5370\n",
            "t = 26, avg_loss = 0.4203\n",
            "t = 27, avg_loss = 0.4062\n",
            "t = 28, avg_loss = 0.4216\n",
            "t = 29, avg_loss = 0.5822\n",
            "t = 30, avg_loss = 0.4234\n",
            "Checking accuracy on test set\n",
            "Got 165 / 240 correct (68.75)\n",
            "acc = 0.687500\n",
            "Starting epoch 16 / 50\n",
            "t = 1, avg_loss = 0.5270\n",
            "t = 2, avg_loss = 0.1502\n",
            "t = 3, avg_loss = 0.2858\n",
            "t = 4, avg_loss = 0.2831\n",
            "t = 5, avg_loss = 0.2730\n",
            "t = 6, avg_loss = 0.2625\n",
            "t = 7, avg_loss = 0.4259\n",
            "t = 8, avg_loss = 0.3324\n",
            "t = 9, avg_loss = 0.2400\n",
            "t = 10, avg_loss = 0.3255\n",
            "t = 11, avg_loss = 0.2131\n",
            "t = 12, avg_loss = 0.2866\n",
            "t = 13, avg_loss = 0.3131\n",
            "t = 14, avg_loss = 0.3708\n",
            "t = 15, avg_loss = 0.3723\n",
            "t = 16, avg_loss = 0.3351\n",
            "t = 17, avg_loss = 0.5958\n",
            "t = 18, avg_loss = 0.3456\n",
            "t = 19, avg_loss = 0.2619\n",
            "t = 20, avg_loss = 0.3174\n",
            "t = 21, avg_loss = 0.2298\n",
            "t = 22, avg_loss = 0.4568\n",
            "t = 23, avg_loss = 0.3104\n",
            "t = 24, avg_loss = 0.3187\n",
            "t = 25, avg_loss = 0.5113\n",
            "t = 26, avg_loss = 0.5814\n",
            "t = 27, avg_loss = 0.3707\n",
            "t = 28, avg_loss = 0.3532\n",
            "t = 29, avg_loss = 0.6302\n",
            "t = 30, avg_loss = 0.4290\n",
            "Checking accuracy on test set\n",
            "Got 186 / 240 correct (77.50)\n",
            "acc = 0.775000\n",
            "Starting epoch 17 / 50\n",
            "t = 1, avg_loss = 0.3442\n",
            "t = 2, avg_loss = 0.2548\n",
            "t = 3, avg_loss = 0.3543\n",
            "t = 4, avg_loss = 0.2970\n",
            "t = 5, avg_loss = 0.3562\n",
            "t = 6, avg_loss = 0.2493\n",
            "t = 7, avg_loss = 0.4809\n",
            "t = 8, avg_loss = 0.2231\n",
            "t = 9, avg_loss = 0.2991\n",
            "t = 10, avg_loss = 0.3774\n",
            "t = 11, avg_loss = 0.5270\n",
            "t = 12, avg_loss = 0.3509\n",
            "t = 13, avg_loss = 0.5568\n",
            "t = 14, avg_loss = 0.3007\n",
            "t = 15, avg_loss = 0.4477\n",
            "t = 16, avg_loss = 0.2788\n",
            "t = 17, avg_loss = 0.3359\n",
            "t = 18, avg_loss = 0.3914\n",
            "t = 19, avg_loss = 0.2935\n",
            "t = 20, avg_loss = 0.2752\n",
            "t = 21, avg_loss = 0.2828\n",
            "t = 22, avg_loss = 0.4638\n",
            "t = 23, avg_loss = 0.4896\n",
            "t = 24, avg_loss = 0.2388\n",
            "t = 25, avg_loss = 0.3879\n",
            "t = 26, avg_loss = 0.3386\n",
            "t = 27, avg_loss = 0.3383\n",
            "t = 28, avg_loss = 0.5112\n",
            "t = 29, avg_loss = 0.3875\n",
            "t = 30, avg_loss = 0.3626\n",
            "Checking accuracy on test set\n",
            "Got 162 / 240 correct (67.50)\n",
            "acc = 0.675000\n",
            "Starting epoch 18 / 50\n",
            "t = 1, avg_loss = 0.4106\n",
            "t = 2, avg_loss = 0.2542\n",
            "t = 3, avg_loss = 0.4812\n",
            "t = 4, avg_loss = 0.3402\n",
            "t = 5, avg_loss = 0.3160\n",
            "t = 6, avg_loss = 0.3098\n",
            "t = 7, avg_loss = 0.2973\n",
            "t = 8, avg_loss = 0.2913\n",
            "t = 9, avg_loss = 0.3768\n",
            "t = 10, avg_loss = 0.3150\n",
            "t = 11, avg_loss = 0.2824\n",
            "t = 12, avg_loss = 0.3345\n",
            "t = 13, avg_loss = 0.3417\n",
            "t = 14, avg_loss = 0.3731\n",
            "t = 15, avg_loss = 0.3049\n",
            "t = 16, avg_loss = 0.2925\n",
            "t = 17, avg_loss = 0.5272\n",
            "t = 18, avg_loss = 0.3007\n",
            "t = 19, avg_loss = 0.2317\n",
            "t = 20, avg_loss = 0.1906\n",
            "t = 21, avg_loss = 0.4075\n",
            "t = 22, avg_loss = 0.3115\n",
            "t = 23, avg_loss = 0.2980\n",
            "t = 24, avg_loss = 0.3899\n",
            "t = 25, avg_loss = 0.6081\n",
            "t = 26, avg_loss = 0.4983\n",
            "t = 27, avg_loss = 0.3334\n",
            "t = 28, avg_loss = 0.4289\n",
            "t = 29, avg_loss = 0.3971\n",
            "t = 30, avg_loss = 0.3970\n",
            "Checking accuracy on test set\n",
            "Got 181 / 240 correct (75.42)\n",
            "acc = 0.754167\n",
            "Starting epoch 19 / 50\n",
            "t = 1, avg_loss = 0.2787\n",
            "t = 2, avg_loss = 0.2677\n",
            "t = 3, avg_loss = 0.3444\n",
            "t = 4, avg_loss = 0.3911\n",
            "t = 5, avg_loss = 0.3474\n",
            "t = 6, avg_loss = 0.2607\n",
            "t = 7, avg_loss = 0.2206\n",
            "t = 8, avg_loss = 0.4127\n",
            "t = 9, avg_loss = 0.3657\n",
            "t = 10, avg_loss = 0.5007\n",
            "t = 11, avg_loss = 0.2416\n",
            "t = 12, avg_loss = 0.5685\n",
            "t = 13, avg_loss = 0.2863\n",
            "t = 14, avg_loss = 0.4052\n",
            "t = 15, avg_loss = 0.4266\n",
            "t = 16, avg_loss = 0.5497\n",
            "t = 17, avg_loss = 0.3231\n",
            "t = 18, avg_loss = 0.5373\n",
            "t = 19, avg_loss = 0.2055\n",
            "t = 20, avg_loss = 0.2732\n",
            "t = 21, avg_loss = 0.3447\n",
            "t = 22, avg_loss = 0.3752\n",
            "t = 23, avg_loss = 0.2158\n",
            "t = 24, avg_loss = 0.4462\n",
            "t = 25, avg_loss = 0.5259\n",
            "t = 26, avg_loss = 0.3605\n",
            "t = 27, avg_loss = 0.4326\n",
            "t = 28, avg_loss = 0.3419\n",
            "t = 29, avg_loss = 0.4587\n",
            "t = 30, avg_loss = 0.4368\n",
            "Checking accuracy on test set\n",
            "Got 164 / 240 correct (68.33)\n",
            "acc = 0.683333\n",
            "Starting epoch 20 / 50\n",
            "t = 1, avg_loss = 0.4312\n",
            "t = 2, avg_loss = 0.2360\n",
            "t = 3, avg_loss = 0.2778\n",
            "t = 4, avg_loss = 0.3137\n",
            "t = 5, avg_loss = 0.2035\n",
            "t = 6, avg_loss = 0.2516\n",
            "t = 7, avg_loss = 0.3919\n",
            "t = 8, avg_loss = 0.3112\n",
            "t = 9, avg_loss = 0.4165\n",
            "t = 10, avg_loss = 0.3448\n",
            "t = 11, avg_loss = 0.3708\n",
            "t = 12, avg_loss = 0.2724\n",
            "t = 13, avg_loss = 0.3154\n",
            "t = 14, avg_loss = 0.3497\n",
            "t = 15, avg_loss = 0.3537\n",
            "t = 16, avg_loss = 0.2339\n",
            "t = 17, avg_loss = 0.3963\n",
            "t = 18, avg_loss = 0.2782\n",
            "t = 19, avg_loss = 0.2846\n",
            "t = 20, avg_loss = 0.3190\n",
            "t = 21, avg_loss = 0.5017\n",
            "t = 22, avg_loss = 0.3263\n",
            "t = 23, avg_loss = 0.2497\n",
            "t = 24, avg_loss = 0.3517\n",
            "t = 25, avg_loss = 0.4076\n",
            "t = 26, avg_loss = 0.5362\n",
            "t = 27, avg_loss = 0.4812\n",
            "t = 28, avg_loss = 0.2875\n",
            "t = 29, avg_loss = 0.5109\n",
            "t = 30, avg_loss = 0.4071\n",
            "Checking accuracy on test set\n",
            "Got 166 / 240 correct (69.17)\n",
            "acc = 0.691667\n",
            "Starting epoch 21 / 50\n",
            "t = 1, avg_loss = 0.3683\n",
            "t = 2, avg_loss = 0.4034\n",
            "t = 3, avg_loss = 0.3459\n",
            "t = 4, avg_loss = 0.3145\n",
            "t = 5, avg_loss = 0.3249\n",
            "t = 6, avg_loss = 0.3124\n",
            "t = 7, avg_loss = 0.2686\n",
            "t = 8, avg_loss = 0.2385\n",
            "t = 9, avg_loss = 0.3570\n",
            "t = 10, avg_loss = 0.3154\n",
            "t = 11, avg_loss = 0.3563\n",
            "t = 12, avg_loss = 0.3589\n",
            "t = 13, avg_loss = 0.3106\n",
            "t = 14, avg_loss = 0.2376\n",
            "t = 15, avg_loss = 0.3490\n",
            "t = 16, avg_loss = 0.3136\n",
            "t = 17, avg_loss = 0.3601\n",
            "t = 18, avg_loss = 0.4083\n",
            "t = 19, avg_loss = 0.5103\n",
            "t = 20, avg_loss = 0.8934\n",
            "t = 21, avg_loss = 0.3977\n",
            "t = 22, avg_loss = 0.3235\n",
            "t = 23, avg_loss = 0.3710\n",
            "t = 24, avg_loss = 0.3964\n",
            "t = 25, avg_loss = 0.4456\n",
            "t = 26, avg_loss = 0.4056\n",
            "t = 27, avg_loss = 0.3503\n",
            "t = 28, avg_loss = 0.2378\n",
            "t = 29, avg_loss = 0.3030\n",
            "t = 30, avg_loss = 0.4208\n",
            "Checking accuracy on test set\n",
            "Got 184 / 240 correct (76.67)\n",
            "acc = 0.766667\n",
            "Starting epoch 22 / 50\n",
            "t = 1, avg_loss = 0.3172\n",
            "t = 2, avg_loss = 0.3555\n",
            "t = 3, avg_loss = 0.3139\n",
            "t = 4, avg_loss = 0.2631\n",
            "t = 5, avg_loss = 0.4551\n",
            "t = 6, avg_loss = 0.2225\n",
            "t = 7, avg_loss = 0.4024\n",
            "t = 8, avg_loss = 0.3225\n",
            "t = 9, avg_loss = 0.2852\n",
            "t = 10, avg_loss = 0.3142\n",
            "t = 11, avg_loss = 0.3696\n",
            "t = 12, avg_loss = 0.5239\n",
            "t = 13, avg_loss = 0.3059\n",
            "t = 14, avg_loss = 0.2155\n",
            "t = 15, avg_loss = 0.4499\n",
            "t = 16, avg_loss = 0.2687\n",
            "t = 17, avg_loss = 0.4001\n",
            "t = 18, avg_loss = 0.4156\n",
            "t = 19, avg_loss = 0.3291\n",
            "t = 20, avg_loss = 0.3058\n",
            "t = 21, avg_loss = 0.2319\n",
            "t = 22, avg_loss = 0.3442\n",
            "t = 23, avg_loss = 0.2228\n",
            "t = 24, avg_loss = 0.3208\n",
            "t = 25, avg_loss = 0.2700\n",
            "t = 26, avg_loss = 0.2051\n",
            "t = 27, avg_loss = 0.3541\n",
            "t = 28, avg_loss = 0.2151\n",
            "t = 29, avg_loss = 0.2793\n",
            "t = 30, avg_loss = 0.4439\n",
            "Checking accuracy on test set\n",
            "Got 193 / 240 correct (80.42)\n",
            "acc = 0.804167\n",
            "Starting epoch 23 / 50\n",
            "t = 1, avg_loss = 0.3287\n",
            "t = 2, avg_loss = 0.4281\n",
            "t = 3, avg_loss = 0.4914\n",
            "t = 4, avg_loss = 0.6297\n",
            "t = 5, avg_loss = 0.4877\n",
            "t = 6, avg_loss = 0.2934\n",
            "t = 7, avg_loss = 0.3457\n",
            "t = 8, avg_loss = 0.2049\n",
            "t = 9, avg_loss = 0.4344\n",
            "t = 10, avg_loss = 0.3862\n",
            "t = 11, avg_loss = 0.3362\n",
            "t = 12, avg_loss = 0.5059\n",
            "t = 13, avg_loss = 0.3093\n",
            "t = 14, avg_loss = 0.3923\n",
            "t = 15, avg_loss = 0.3131\n",
            "t = 16, avg_loss = 0.3420\n",
            "t = 17, avg_loss = 0.4651\n",
            "t = 18, avg_loss = 0.2446\n",
            "t = 19, avg_loss = 0.3843\n",
            "t = 20, avg_loss = 0.4809\n",
            "t = 21, avg_loss = 0.2431\n",
            "t = 22, avg_loss = 0.2530\n",
            "t = 23, avg_loss = 0.4618\n",
            "t = 24, avg_loss = 0.3255\n",
            "t = 25, avg_loss = 0.1880\n",
            "t = 26, avg_loss = 0.4080\n",
            "t = 27, avg_loss = 0.3441\n",
            "t = 28, avg_loss = 0.4768\n",
            "t = 29, avg_loss = 0.4117\n",
            "t = 30, avg_loss = 0.2922\n",
            "Checking accuracy on test set\n",
            "Got 187 / 240 correct (77.92)\n",
            "acc = 0.779167\n",
            "Starting epoch 24 / 50\n",
            "t = 1, avg_loss = 0.3009\n",
            "t = 2, avg_loss = 0.2951\n",
            "t = 3, avg_loss = 0.4573\n",
            "t = 4, avg_loss = 0.2836\n",
            "t = 5, avg_loss = 0.3890\n",
            "t = 6, avg_loss = 0.3624\n",
            "t = 7, avg_loss = 0.3226\n",
            "t = 8, avg_loss = 0.3769\n",
            "t = 9, avg_loss = 0.2740\n",
            "t = 10, avg_loss = 0.2426\n",
            "t = 11, avg_loss = 0.3863\n",
            "t = 12, avg_loss = 0.3858\n",
            "t = 13, avg_loss = 0.2990\n",
            "t = 14, avg_loss = 0.3832\n",
            "t = 15, avg_loss = 0.3477\n",
            "t = 16, avg_loss = 0.3387\n",
            "t = 17, avg_loss = 0.3732\n",
            "t = 18, avg_loss = 0.2763\n",
            "t = 19, avg_loss = 0.3423\n",
            "t = 20, avg_loss = 0.2746\n",
            "t = 21, avg_loss = 0.4281\n",
            "t = 22, avg_loss = 0.3525\n",
            "t = 23, avg_loss = 0.6181\n",
            "t = 24, avg_loss = 0.4032\n",
            "t = 25, avg_loss = 0.2849\n",
            "t = 26, avg_loss = 0.4082\n",
            "t = 27, avg_loss = 0.3228\n",
            "t = 28, avg_loss = 0.3918\n",
            "t = 29, avg_loss = 0.5678\n",
            "t = 30, avg_loss = 0.2794\n",
            "Checking accuracy on test set\n",
            "Got 169 / 240 correct (70.42)\n",
            "acc = 0.704167\n",
            "Starting epoch 25 / 50\n",
            "t = 1, avg_loss = 0.4542\n",
            "t = 2, avg_loss = 0.4273\n",
            "t = 3, avg_loss = 0.2913\n",
            "t = 4, avg_loss = 0.3348\n",
            "t = 5, avg_loss = 0.3263\n",
            "t = 6, avg_loss = 0.2757\n",
            "t = 7, avg_loss = 0.3449\n",
            "t = 8, avg_loss = 0.3222\n",
            "t = 9, avg_loss = 0.3780\n",
            "t = 10, avg_loss = 0.2535\n",
            "t = 11, avg_loss = 0.2437\n",
            "t = 12, avg_loss = 0.4518\n",
            "t = 13, avg_loss = 0.3080\n",
            "t = 14, avg_loss = 0.2789\n",
            "t = 15, avg_loss = 0.3993\n",
            "t = 16, avg_loss = 0.2836\n",
            "t = 17, avg_loss = 0.2834\n",
            "t = 18, avg_loss = 0.2299\n",
            "t = 19, avg_loss = 0.3745\n",
            "t = 20, avg_loss = 0.2623\n",
            "t = 21, avg_loss = 0.5480\n",
            "t = 22, avg_loss = 0.2911\n",
            "t = 23, avg_loss = 0.3762\n",
            "t = 24, avg_loss = 0.2533\n",
            "t = 25, avg_loss = 0.3075\n",
            "t = 26, avg_loss = 0.3236\n",
            "t = 27, avg_loss = 0.2678\n",
            "t = 28, avg_loss = 0.3081\n",
            "t = 29, avg_loss = 0.3873\n",
            "t = 30, avg_loss = 0.3711\n",
            "Checking accuracy on test set\n",
            "Got 164 / 240 correct (68.33)\n",
            "acc = 0.683333\n",
            "Starting epoch 26 / 50\n",
            "t = 1, avg_loss = 0.3990\n",
            "t = 2, avg_loss = 0.3658\n",
            "t = 3, avg_loss = 0.3494\n",
            "t = 4, avg_loss = 0.4228\n",
            "t = 5, avg_loss = 0.5068\n",
            "t = 6, avg_loss = 0.4951\n",
            "t = 7, avg_loss = 0.2735\n",
            "t = 8, avg_loss = 0.2911\n",
            "t = 9, avg_loss = 0.1736\n",
            "t = 10, avg_loss = 0.3194\n",
            "t = 11, avg_loss = 0.2265\n",
            "t = 12, avg_loss = 0.2693\n",
            "t = 13, avg_loss = 0.4083\n",
            "t = 14, avg_loss = 0.2254\n",
            "t = 15, avg_loss = 0.5391\n",
            "t = 16, avg_loss = 0.3719\n",
            "t = 17, avg_loss = 0.2987\n",
            "t = 18, avg_loss = 0.3133\n",
            "t = 19, avg_loss = 0.4346\n",
            "t = 20, avg_loss = 0.2525\n",
            "t = 21, avg_loss = 0.4066\n",
            "t = 22, avg_loss = 0.2053\n",
            "t = 23, avg_loss = 0.4776\n",
            "t = 24, avg_loss = 0.2812\n",
            "t = 25, avg_loss = 0.3907\n",
            "t = 26, avg_loss = 0.3630\n",
            "t = 27, avg_loss = 0.2765\n",
            "t = 28, avg_loss = 0.5535\n",
            "t = 29, avg_loss = 0.2933\n",
            "t = 30, avg_loss = 0.4656\n",
            "Checking accuracy on test set\n",
            "Got 180 / 240 correct (75.00)\n",
            "acc = 0.750000\n",
            "Starting epoch 27 / 50\n",
            "t = 1, avg_loss = 0.3700\n",
            "t = 2, avg_loss = 0.2454\n",
            "t = 3, avg_loss = 0.2728\n",
            "t = 4, avg_loss = 0.3319\n",
            "t = 5, avg_loss = 0.4122\n",
            "t = 6, avg_loss = 0.2400\n",
            "t = 7, avg_loss = 0.3835\n",
            "t = 8, avg_loss = 0.2216\n",
            "t = 9, avg_loss = 0.5702\n",
            "t = 10, avg_loss = 0.3306\n",
            "t = 11, avg_loss = 0.3240\n",
            "t = 12, avg_loss = 0.2056\n",
            "t = 13, avg_loss = 0.3779\n",
            "t = 14, avg_loss = 0.2642\n",
            "t = 15, avg_loss = 0.2134\n",
            "t = 16, avg_loss = 0.2095\n",
            "t = 17, avg_loss = 0.5787\n",
            "t = 18, avg_loss = 0.3618\n",
            "t = 19, avg_loss = 0.5324\n",
            "t = 20, avg_loss = 0.3593\n",
            "t = 21, avg_loss = 0.3842\n",
            "t = 22, avg_loss = 0.5472\n",
            "t = 23, avg_loss = 0.5517\n",
            "t = 24, avg_loss = 0.3682\n",
            "t = 25, avg_loss = 0.4329\n",
            "t = 26, avg_loss = 0.3800\n",
            "t = 27, avg_loss = 0.4715\n",
            "t = 28, avg_loss = 0.3138\n",
            "t = 29, avg_loss = 0.2820\n",
            "t = 30, avg_loss = 0.8316\n",
            "Checking accuracy on test set\n",
            "Got 179 / 240 correct (74.58)\n",
            "acc = 0.745833\n",
            "Starting epoch 28 / 50\n",
            "t = 1, avg_loss = 0.3302\n",
            "t = 2, avg_loss = 0.4999\n",
            "t = 3, avg_loss = 0.4133\n",
            "t = 4, avg_loss = 0.4239\n",
            "t = 5, avg_loss = 0.2377\n",
            "t = 6, avg_loss = 0.3417\n",
            "t = 7, avg_loss = 0.2627\n",
            "t = 8, avg_loss = 0.4446\n",
            "t = 9, avg_loss = 0.3004\n",
            "t = 10, avg_loss = 0.4705\n",
            "t = 11, avg_loss = 0.3943\n",
            "t = 12, avg_loss = 0.4284\n",
            "t = 13, avg_loss = 0.3297\n",
            "t = 14, avg_loss = 0.3025\n",
            "t = 15, avg_loss = 0.3577\n",
            "t = 16, avg_loss = 0.2040\n",
            "t = 17, avg_loss = 0.3444\n",
            "t = 18, avg_loss = 0.3002\n",
            "t = 19, avg_loss = 0.5465\n",
            "t = 20, avg_loss = 0.4252\n",
            "t = 21, avg_loss = 0.3127\n",
            "t = 22, avg_loss = 0.4424\n",
            "t = 23, avg_loss = 0.4372\n",
            "t = 24, avg_loss = 0.3290\n",
            "t = 25, avg_loss = 0.2408\n",
            "t = 26, avg_loss = 0.2320\n",
            "t = 27, avg_loss = 0.4380\n",
            "t = 28, avg_loss = 0.2789\n",
            "t = 29, avg_loss = 0.6281\n",
            "t = 30, avg_loss = 0.5592\n",
            "Checking accuracy on test set\n",
            "Got 179 / 240 correct (74.58)\n",
            "acc = 0.745833\n",
            "Starting epoch 29 / 50\n",
            "t = 1, avg_loss = 0.4158\n",
            "t = 2, avg_loss = 0.2217\n",
            "t = 3, avg_loss = 0.4832\n",
            "t = 4, avg_loss = 0.2847\n",
            "t = 5, avg_loss = 0.2942\n",
            "t = 6, avg_loss = 0.3250\n",
            "t = 7, avg_loss = 0.3413\n",
            "t = 8, avg_loss = 0.3441\n",
            "t = 9, avg_loss = 0.4451\n",
            "t = 10, avg_loss = 0.3100\n",
            "t = 11, avg_loss = 0.3431\n",
            "t = 12, avg_loss = 0.2627\n",
            "t = 13, avg_loss = 0.4189\n",
            "t = 14, avg_loss = 0.3479\n",
            "t = 15, avg_loss = 0.2423\n",
            "t = 16, avg_loss = 0.2658\n",
            "t = 17, avg_loss = 0.2402\n",
            "t = 18, avg_loss = 0.3623\n",
            "t = 19, avg_loss = 0.5031\n",
            "t = 20, avg_loss = 0.4386\n",
            "t = 21, avg_loss = 0.3883\n",
            "t = 22, avg_loss = 0.4786\n",
            "t = 23, avg_loss = 0.2696\n",
            "t = 24, avg_loss = 0.2188\n",
            "t = 25, avg_loss = 0.4549\n",
            "t = 26, avg_loss = 0.4071\n",
            "t = 27, avg_loss = 0.3659\n",
            "t = 28, avg_loss = 0.2624\n",
            "t = 29, avg_loss = 0.1962\n",
            "t = 30, avg_loss = 0.3369\n",
            "Checking accuracy on test set\n",
            "Got 188 / 240 correct (78.33)\n",
            "acc = 0.783333\n",
            "Starting epoch 30 / 50\n",
            "t = 1, avg_loss = 0.3789\n",
            "t = 2, avg_loss = 0.2960\n",
            "t = 3, avg_loss = 0.3178\n",
            "t = 4, avg_loss = 0.3241\n",
            "t = 5, avg_loss = 0.5106\n",
            "t = 6, avg_loss = 0.2581\n",
            "t = 7, avg_loss = 0.2201\n",
            "t = 8, avg_loss = 0.4096\n",
            "t = 9, avg_loss = 0.3531\n",
            "t = 10, avg_loss = 0.3911\n",
            "t = 11, avg_loss = 0.2651\n",
            "t = 12, avg_loss = 0.1952\n",
            "t = 13, avg_loss = 0.3545\n",
            "t = 14, avg_loss = 0.3222\n",
            "t = 15, avg_loss = 0.4752\n",
            "t = 16, avg_loss = 0.4283\n",
            "t = 17, avg_loss = 0.1770\n",
            "t = 18, avg_loss = 0.6774\n",
            "t = 19, avg_loss = 0.4588\n",
            "t = 20, avg_loss = 0.2867\n",
            "t = 21, avg_loss = 0.2734\n",
            "t = 22, avg_loss = 0.2419\n",
            "t = 23, avg_loss = 0.4641\n",
            "t = 24, avg_loss = 0.2842\n",
            "t = 25, avg_loss = 0.2770\n",
            "t = 26, avg_loss = 0.3473\n",
            "t = 27, avg_loss = 0.3301\n",
            "t = 28, avg_loss = 0.2452\n",
            "t = 29, avg_loss = 0.3614\n",
            "t = 30, avg_loss = 0.6745\n",
            "Checking accuracy on test set\n",
            "Got 158 / 240 correct (65.83)\n",
            "acc = 0.658333\n",
            "Starting epoch 31 / 50\n",
            "t = 1, avg_loss = 0.3813\n",
            "t = 2, avg_loss = 0.4254\n",
            "t = 3, avg_loss = 0.4121\n",
            "t = 4, avg_loss = 0.2819\n",
            "t = 5, avg_loss = 0.3355\n",
            "t = 6, avg_loss = 0.4018\n",
            "t = 7, avg_loss = 0.4492\n",
            "t = 8, avg_loss = 0.2681\n",
            "t = 9, avg_loss = 0.3666\n",
            "t = 10, avg_loss = 0.6550\n",
            "t = 11, avg_loss = 0.2605\n",
            "t = 12, avg_loss = 0.4391\n",
            "t = 13, avg_loss = 0.3429\n",
            "t = 14, avg_loss = 0.2907\n",
            "t = 15, avg_loss = 0.3793\n",
            "t = 16, avg_loss = 0.3325\n",
            "t = 17, avg_loss = 0.2609\n",
            "t = 18, avg_loss = 0.3394\n",
            "t = 19, avg_loss = 0.4218\n",
            "t = 20, avg_loss = 0.3968\n",
            "t = 21, avg_loss = 0.3724\n",
            "t = 22, avg_loss = 0.3810\n",
            "t = 23, avg_loss = 0.3271\n",
            "t = 24, avg_loss = 0.2136\n",
            "t = 25, avg_loss = 0.3400\n",
            "t = 26, avg_loss = 0.3401\n",
            "t = 27, avg_loss = 0.3182\n",
            "t = 28, avg_loss = 0.3899\n",
            "t = 29, avg_loss = 0.2443\n",
            "t = 30, avg_loss = 0.3180\n",
            "Checking accuracy on test set\n",
            "Got 195 / 240 correct (81.25)\n",
            "acc = 0.812500\n",
            "Starting epoch 32 / 50\n",
            "t = 1, avg_loss = 0.5003\n",
            "t = 2, avg_loss = 0.2928\n",
            "t = 3, avg_loss = 0.2488\n",
            "t = 4, avg_loss = 0.4995\n",
            "t = 5, avg_loss = 0.3989\n",
            "t = 6, avg_loss = 0.2729\n",
            "t = 7, avg_loss = 0.2818\n",
            "t = 8, avg_loss = 0.2326\n",
            "t = 9, avg_loss = 0.4324\n",
            "t = 10, avg_loss = 0.3534\n",
            "t = 11, avg_loss = 0.2140\n",
            "t = 12, avg_loss = 0.4603\n",
            "t = 13, avg_loss = 0.3446\n",
            "t = 14, avg_loss = 0.2191\n",
            "t = 15, avg_loss = 0.3333\n",
            "t = 16, avg_loss = 0.2608\n",
            "t = 17, avg_loss = 0.2743\n",
            "t = 18, avg_loss = 0.4795\n",
            "t = 19, avg_loss = 0.3424\n",
            "t = 20, avg_loss = 0.2226\n",
            "t = 21, avg_loss = 0.3374\n",
            "t = 22, avg_loss = 0.4397\n",
            "t = 23, avg_loss = 0.1956\n",
            "t = 24, avg_loss = 0.2977\n",
            "t = 25, avg_loss = 0.3099\n",
            "t = 26, avg_loss = 0.3482\n",
            "t = 27, avg_loss = 0.2698\n",
            "t = 28, avg_loss = 0.2243\n",
            "t = 29, avg_loss = 0.2338\n",
            "t = 30, avg_loss = 0.6344\n",
            "Checking accuracy on test set\n",
            "Got 138 / 240 correct (57.50)\n",
            "acc = 0.575000\n",
            "Starting epoch 33 / 50\n",
            "t = 1, avg_loss = 0.2773\n",
            "t = 2, avg_loss = 0.2825\n",
            "t = 3, avg_loss = 0.3734\n",
            "t = 4, avg_loss = 0.3489\n",
            "t = 5, avg_loss = 0.4029\n",
            "t = 6, avg_loss = 0.3107\n",
            "t = 7, avg_loss = 0.6618\n",
            "t = 8, avg_loss = 0.3693\n",
            "t = 9, avg_loss = 0.2760\n",
            "t = 10, avg_loss = 0.3268\n",
            "t = 11, avg_loss = 0.3702\n",
            "t = 12, avg_loss = 0.2864\n",
            "t = 13, avg_loss = 0.4361\n",
            "t = 14, avg_loss = 0.5361\n",
            "t = 15, avg_loss = 0.2670\n",
            "t = 16, avg_loss = 0.3836\n",
            "t = 17, avg_loss = 0.3769\n",
            "t = 18, avg_loss = 0.4547\n",
            "t = 19, avg_loss = 0.3255\n",
            "t = 20, avg_loss = 0.2681\n",
            "t = 21, avg_loss = 0.2574\n",
            "t = 22, avg_loss = 0.3186\n",
            "t = 23, avg_loss = 0.3757\n",
            "t = 24, avg_loss = 0.2644\n",
            "t = 25, avg_loss = 0.3011\n",
            "t = 26, avg_loss = 0.2433\n",
            "t = 27, avg_loss = 0.3535\n",
            "t = 28, avg_loss = 0.2667\n",
            "t = 29, avg_loss = 0.3768\n",
            "t = 30, avg_loss = 0.3147\n",
            "Checking accuracy on test set\n",
            "Got 176 / 240 correct (73.33)\n",
            "acc = 0.733333\n",
            "Starting epoch 34 / 50\n",
            "t = 1, avg_loss = 0.4749\n",
            "t = 2, avg_loss = 0.5344\n",
            "t = 3, avg_loss = 0.5424\n",
            "t = 4, avg_loss = 0.3709\n",
            "t = 5, avg_loss = 0.4550\n",
            "t = 6, avg_loss = 0.4161\n",
            "t = 7, avg_loss = 0.4093\n",
            "t = 8, avg_loss = 0.4772\n",
            "t = 9, avg_loss = 0.2060\n",
            "t = 10, avg_loss = 0.3535\n",
            "t = 11, avg_loss = 0.2188\n",
            "t = 12, avg_loss = 0.4433\n",
            "t = 13, avg_loss = 0.2871\n",
            "t = 14, avg_loss = 0.3024\n",
            "t = 15, avg_loss = 0.2272\n",
            "t = 16, avg_loss = 0.5801\n",
            "t = 17, avg_loss = 0.3676\n",
            "t = 18, avg_loss = 0.2945\n",
            "t = 19, avg_loss = 0.3943\n",
            "t = 20, avg_loss = 0.2365\n",
            "t = 21, avg_loss = 0.2489\n",
            "t = 22, avg_loss = 0.2396\n",
            "t = 23, avg_loss = 0.2395\n",
            "t = 24, avg_loss = 0.2269\n",
            "t = 25, avg_loss = 0.3298\n",
            "t = 26, avg_loss = 0.3943\n",
            "t = 27, avg_loss = 0.4170\n",
            "t = 28, avg_loss = 0.3540\n",
            "t = 29, avg_loss = 0.3354\n",
            "t = 30, avg_loss = 0.6023\n",
            "Checking accuracy on test set\n",
            "Got 166 / 240 correct (69.17)\n",
            "acc = 0.691667\n",
            "Starting epoch 35 / 50\n",
            "t = 1, avg_loss = 0.3270\n",
            "t = 2, avg_loss = 0.2110\n",
            "t = 3, avg_loss = 0.1825\n",
            "t = 4, avg_loss = 0.3877\n",
            "t = 5, avg_loss = 0.3023\n",
            "t = 6, avg_loss = 0.3941\n",
            "t = 7, avg_loss = 0.2534\n",
            "t = 8, avg_loss = 0.2954\n",
            "t = 9, avg_loss = 0.2804\n",
            "t = 10, avg_loss = 0.4354\n",
            "t = 11, avg_loss = 0.4279\n",
            "t = 12, avg_loss = 0.2627\n",
            "t = 13, avg_loss = 0.4277\n",
            "t = 14, avg_loss = 0.2495\n",
            "t = 15, avg_loss = 0.3352\n",
            "t = 16, avg_loss = 0.1574\n",
            "t = 17, avg_loss = 0.2828\n",
            "t = 18, avg_loss = 0.3225\n",
            "t = 19, avg_loss = 0.4414\n",
            "t = 20, avg_loss = 0.4126\n",
            "t = 21, avg_loss = 0.3822\n",
            "t = 22, avg_loss = 0.4060\n",
            "t = 23, avg_loss = 0.4073\n",
            "t = 24, avg_loss = 0.2680\n",
            "t = 25, avg_loss = 0.2835\n",
            "t = 26, avg_loss = 0.3052\n",
            "t = 27, avg_loss = 0.3952\n",
            "t = 28, avg_loss = 0.3296\n",
            "t = 29, avg_loss = 0.3547\n",
            "t = 30, avg_loss = 0.4600\n",
            "Checking accuracy on test set\n",
            "Got 157 / 240 correct (65.42)\n",
            "acc = 0.654167\n",
            "Starting epoch 36 / 50\n",
            "t = 1, avg_loss = 0.2814\n",
            "t = 2, avg_loss = 0.3613\n",
            "t = 3, avg_loss = 0.4846\n",
            "t = 4, avg_loss = 0.3049\n",
            "t = 5, avg_loss = 0.3744\n",
            "t = 6, avg_loss = 0.5601\n",
            "t = 7, avg_loss = 0.1516\n",
            "t = 8, avg_loss = 0.2358\n",
            "t = 9, avg_loss = 0.3779\n",
            "t = 10, avg_loss = 0.4137\n",
            "t = 11, avg_loss = 0.2247\n",
            "t = 12, avg_loss = 0.2727\n",
            "t = 13, avg_loss = 0.3805\n",
            "t = 14, avg_loss = 0.4683\n",
            "t = 15, avg_loss = 0.1990\n",
            "t = 16, avg_loss = 0.3192\n",
            "t = 17, avg_loss = 0.3749\n",
            "t = 18, avg_loss = 0.4672\n",
            "t = 19, avg_loss = 0.4534\n",
            "t = 20, avg_loss = 0.3355\n",
            "t = 21, avg_loss = 0.1986\n",
            "t = 22, avg_loss = 0.4277\n",
            "t = 23, avg_loss = 0.2747\n",
            "t = 24, avg_loss = 0.5876\n",
            "t = 25, avg_loss = 0.2600\n",
            "t = 26, avg_loss = 0.5833\n",
            "t = 27, avg_loss = 0.2997\n",
            "t = 28, avg_loss = 0.2951\n",
            "t = 29, avg_loss = 0.3615\n",
            "t = 30, avg_loss = 0.2567\n",
            "Checking accuracy on test set\n",
            "Got 200 / 240 correct (83.33)\n",
            "acc = 0.833333\n",
            "Starting epoch 37 / 50\n",
            "t = 1, avg_loss = 0.3312\n",
            "t = 2, avg_loss = 0.3917\n",
            "t = 3, avg_loss = 0.3190\n",
            "t = 4, avg_loss = 0.5261\n",
            "t = 5, avg_loss = 0.3673\n",
            "t = 6, avg_loss = 0.2439\n",
            "t = 7, avg_loss = 0.3652\n",
            "t = 8, avg_loss = 0.4209\n",
            "t = 9, avg_loss = 0.3061\n",
            "t = 10, avg_loss = 0.2219\n",
            "t = 11, avg_loss = 0.4581\n",
            "t = 12, avg_loss = 0.2906\n",
            "t = 13, avg_loss = 0.3232\n",
            "t = 14, avg_loss = 0.2617\n",
            "t = 15, avg_loss = 0.3487\n",
            "t = 16, avg_loss = 0.2660\n",
            "t = 17, avg_loss = 0.4385\n",
            "t = 18, avg_loss = 0.3903\n",
            "t = 19, avg_loss = 0.3417\n",
            "t = 20, avg_loss = 0.2979\n",
            "t = 21, avg_loss = 0.7156\n",
            "t = 22, avg_loss = 0.3861\n",
            "t = 23, avg_loss = 0.3439\n",
            "t = 24, avg_loss = 0.5317\n",
            "t = 25, avg_loss = 0.3678\n",
            "t = 26, avg_loss = 0.2906\n",
            "t = 27, avg_loss = 0.2004\n",
            "t = 28, avg_loss = 0.3572\n",
            "t = 29, avg_loss = 0.2260\n",
            "t = 30, avg_loss = 0.4071\n",
            "Checking accuracy on test set\n",
            "Got 153 / 240 correct (63.75)\n",
            "acc = 0.637500\n",
            "Starting epoch 38 / 50\n",
            "t = 1, avg_loss = 0.3756\n",
            "t = 2, avg_loss = 0.3428\n",
            "t = 3, avg_loss = 0.3755\n",
            "t = 4, avg_loss = 0.3047\n",
            "t = 5, avg_loss = 0.2728\n",
            "t = 6, avg_loss = 0.2983\n",
            "t = 7, avg_loss = 0.3912\n",
            "t = 8, avg_loss = 0.2915\n",
            "t = 9, avg_loss = 0.3459\n",
            "t = 10, avg_loss = 0.3558\n",
            "t = 11, avg_loss = 0.2972\n",
            "t = 12, avg_loss = 0.3376\n",
            "t = 13, avg_loss = 0.3923\n",
            "t = 14, avg_loss = 0.2768\n",
            "t = 15, avg_loss = 0.3851\n",
            "t = 16, avg_loss = 0.3126\n",
            "t = 17, avg_loss = 0.2427\n",
            "t = 18, avg_loss = 0.6171\n",
            "t = 19, avg_loss = 0.3319\n",
            "t = 20, avg_loss = 0.2975\n",
            "t = 21, avg_loss = 0.1902\n",
            "t = 22, avg_loss = 0.2945\n",
            "t = 23, avg_loss = 0.4296\n",
            "t = 24, avg_loss = 0.3922\n",
            "t = 25, avg_loss = 0.3160\n",
            "t = 26, avg_loss = 0.1851\n",
            "t = 27, avg_loss = 0.3135\n",
            "t = 28, avg_loss = 0.2810\n",
            "t = 29, avg_loss = 0.2571\n",
            "t = 30, avg_loss = 0.4033\n",
            "Checking accuracy on test set\n",
            "Got 179 / 240 correct (74.58)\n",
            "acc = 0.745833\n",
            "Starting epoch 39 / 50\n",
            "t = 1, avg_loss = 0.2983\n",
            "t = 2, avg_loss = 0.2610\n",
            "t = 3, avg_loss = 0.2551\n",
            "t = 4, avg_loss = 0.3727\n",
            "t = 5, avg_loss = 0.4791\n",
            "t = 6, avg_loss = 0.2698\n",
            "t = 7, avg_loss = 0.2856\n",
            "t = 8, avg_loss = 0.4880\n",
            "t = 9, avg_loss = 0.5315\n",
            "t = 10, avg_loss = 0.6199\n",
            "t = 11, avg_loss = 0.2763\n",
            "t = 12, avg_loss = 0.3514\n",
            "t = 13, avg_loss = 0.2948\n",
            "t = 14, avg_loss = 0.3494\n",
            "t = 15, avg_loss = 0.2520\n",
            "t = 16, avg_loss = 0.3080\n",
            "t = 17, avg_loss = 0.3345\n",
            "t = 18, avg_loss = 0.2287\n",
            "t = 19, avg_loss = 0.3160\n",
            "t = 20, avg_loss = 0.3519\n",
            "t = 21, avg_loss = 0.3158\n",
            "t = 22, avg_loss = 0.3656\n",
            "t = 23, avg_loss = 0.2141\n",
            "t = 24, avg_loss = 0.4073\n",
            "t = 25, avg_loss = 0.3249\n",
            "t = 26, avg_loss = 0.3432\n",
            "t = 27, avg_loss = 0.3294\n",
            "t = 28, avg_loss = 0.4394\n",
            "t = 29, avg_loss = 0.2481\n",
            "t = 30, avg_loss = 0.2765\n",
            "Checking accuracy on test set\n",
            "Got 199 / 240 correct (82.92)\n",
            "acc = 0.829167\n",
            "Starting epoch 40 / 50\n",
            "t = 1, avg_loss = 0.1926\n",
            "t = 2, avg_loss = 0.3064\n",
            "t = 3, avg_loss = 0.1673\n",
            "t = 4, avg_loss = 0.2148\n",
            "t = 5, avg_loss = 0.5370\n",
            "t = 6, avg_loss = 0.2417\n",
            "t = 7, avg_loss = 0.3893\n",
            "t = 8, avg_loss = 0.1507\n",
            "t = 9, avg_loss = 0.4842\n",
            "t = 10, avg_loss = 0.3014\n",
            "t = 11, avg_loss = 0.1899\n",
            "t = 12, avg_loss = 0.3766\n",
            "t = 13, avg_loss = 0.3535\n",
            "t = 14, avg_loss = 0.4650\n",
            "t = 15, avg_loss = 0.4592\n",
            "t = 16, avg_loss = 0.4323\n",
            "t = 17, avg_loss = 0.2480\n",
            "t = 18, avg_loss = 0.3452\n",
            "t = 19, avg_loss = 0.4578\n",
            "t = 20, avg_loss = 0.3539\n",
            "t = 21, avg_loss = 0.3447\n",
            "t = 22, avg_loss = 0.2573\n",
            "t = 23, avg_loss = 0.4990\n",
            "t = 24, avg_loss = 0.3773\n",
            "t = 25, avg_loss = 0.3267\n",
            "t = 26, avg_loss = 0.2341\n",
            "t = 27, avg_loss = 0.2883\n",
            "t = 28, avg_loss = 0.2273\n",
            "t = 29, avg_loss = 0.3400\n",
            "t = 30, avg_loss = 0.4437\n",
            "Checking accuracy on test set\n",
            "Got 185 / 240 correct (77.08)\n",
            "acc = 0.770833\n",
            "Starting epoch 41 / 50\n",
            "t = 1, avg_loss = 0.3743\n",
            "t = 2, avg_loss = 0.3372\n",
            "t = 3, avg_loss = 0.2236\n",
            "t = 4, avg_loss = 0.3186\n",
            "t = 5, avg_loss = 0.3136\n",
            "t = 6, avg_loss = 0.3425\n",
            "t = 7, avg_loss = 0.3861\n",
            "t = 8, avg_loss = 0.2564\n",
            "t = 9, avg_loss = 0.2602\n",
            "t = 10, avg_loss = 0.3248\n",
            "t = 11, avg_loss = 0.4585\n",
            "t = 12, avg_loss = 0.2114\n",
            "t = 13, avg_loss = 0.4287\n",
            "t = 14, avg_loss = 0.3772\n",
            "t = 15, avg_loss = 0.2357\n",
            "t = 16, avg_loss = 0.3052\n",
            "t = 17, avg_loss = 0.3541\n",
            "t = 18, avg_loss = 0.4490\n",
            "t = 19, avg_loss = 0.4363\n",
            "t = 20, avg_loss = 0.3614\n",
            "t = 21, avg_loss = 0.4589\n",
            "t = 22, avg_loss = 0.4164\n",
            "t = 23, avg_loss = 0.4152\n",
            "t = 24, avg_loss = 0.2128\n",
            "t = 25, avg_loss = 0.3322\n",
            "t = 26, avg_loss = 0.3636\n",
            "t = 27, avg_loss = 0.3584\n",
            "t = 28, avg_loss = 0.2720\n",
            "t = 29, avg_loss = 0.3083\n",
            "t = 30, avg_loss = 0.4397\n",
            "Checking accuracy on test set\n",
            "Got 128 / 240 correct (53.33)\n",
            "acc = 0.533333\n",
            "Starting epoch 42 / 50\n",
            "t = 1, avg_loss = 0.2828\n",
            "t = 2, avg_loss = 0.1732\n",
            "t = 3, avg_loss = 0.4163\n",
            "t = 4, avg_loss = 0.3483\n",
            "t = 5, avg_loss = 0.4286\n",
            "t = 6, avg_loss = 0.5459\n",
            "t = 7, avg_loss = 0.2676\n",
            "t = 8, avg_loss = 0.2530\n",
            "t = 9, avg_loss = 0.4306\n",
            "t = 10, avg_loss = 0.4502\n",
            "t = 11, avg_loss = 0.2209\n",
            "t = 12, avg_loss = 0.2475\n",
            "t = 13, avg_loss = 0.3582\n",
            "t = 14, avg_loss = 0.2549\n",
            "t = 15, avg_loss = 0.3243\n",
            "t = 16, avg_loss = 0.2196\n",
            "t = 17, avg_loss = 0.2632\n",
            "t = 18, avg_loss = 0.4862\n",
            "t = 19, avg_loss = 0.3183\n",
            "t = 20, avg_loss = 0.2319\n",
            "t = 21, avg_loss = 0.3348\n",
            "t = 22, avg_loss = 0.5419\n",
            "t = 23, avg_loss = 0.2447\n",
            "t = 24, avg_loss = 0.3395\n",
            "t = 25, avg_loss = 0.3292\n",
            "t = 26, avg_loss = 0.2524\n",
            "t = 27, avg_loss = 0.4633\n",
            "t = 28, avg_loss = 0.4334\n",
            "t = 29, avg_loss = 0.4211\n",
            "t = 30, avg_loss = 0.3583\n",
            "Checking accuracy on test set\n",
            "Got 186 / 240 correct (77.50)\n",
            "acc = 0.775000\n",
            "Starting epoch 43 / 50\n",
            "t = 1, avg_loss = 0.4304\n",
            "t = 2, avg_loss = 0.2860\n",
            "t = 3, avg_loss = 0.3034\n",
            "t = 4, avg_loss = 0.1974\n",
            "t = 5, avg_loss = 0.3862\n",
            "t = 6, avg_loss = 0.4911\n",
            "t = 7, avg_loss = 0.3643\n",
            "t = 8, avg_loss = 0.5092\n",
            "t = 9, avg_loss = 0.2233\n",
            "t = 10, avg_loss = 0.2160\n",
            "t = 11, avg_loss = 0.2874\n",
            "t = 12, avg_loss = 0.5087\n",
            "t = 13, avg_loss = 0.3631\n",
            "t = 14, avg_loss = 0.3321\n",
            "t = 15, avg_loss = 0.4259\n",
            "t = 16, avg_loss = 0.4217\n",
            "t = 17, avg_loss = 0.3380\n",
            "t = 18, avg_loss = 0.3306\n",
            "t = 19, avg_loss = 0.2476\n",
            "t = 20, avg_loss = 0.3882\n",
            "t = 21, avg_loss = 0.2078\n",
            "t = 22, avg_loss = 0.3861\n",
            "t = 23, avg_loss = 0.3988\n",
            "t = 24, avg_loss = 0.2076\n",
            "t = 25, avg_loss = 0.2929\n",
            "t = 26, avg_loss = 0.4746\n",
            "t = 27, avg_loss = 0.3241\n",
            "t = 28, avg_loss = 0.4384\n",
            "t = 29, avg_loss = 0.2749\n",
            "t = 30, avg_loss = 0.3558\n",
            "Checking accuracy on test set\n",
            "Got 173 / 240 correct (72.08)\n",
            "acc = 0.720833\n",
            "Starting epoch 44 / 50\n",
            "t = 1, avg_loss = 0.4915\n",
            "t = 2, avg_loss = 0.3770\n",
            "t = 3, avg_loss = 0.2153\n",
            "t = 4, avg_loss = 0.3697\n",
            "t = 5, avg_loss = 0.2032\n",
            "t = 6, avg_loss = 0.2939\n",
            "t = 7, avg_loss = 0.3308\n",
            "t = 8, avg_loss = 0.2514\n",
            "t = 9, avg_loss = 0.3074\n",
            "t = 10, avg_loss = 0.4362\n",
            "t = 11, avg_loss = 0.3670\n",
            "t = 12, avg_loss = 0.2006\n",
            "t = 13, avg_loss = 0.3391\n",
            "t = 14, avg_loss = 0.2040\n",
            "t = 15, avg_loss = 0.3409\n",
            "t = 16, avg_loss = 0.3766\n",
            "t = 17, avg_loss = 0.2715\n",
            "t = 18, avg_loss = 0.4169\n",
            "t = 19, avg_loss = 0.3337\n",
            "t = 20, avg_loss = 0.2598\n",
            "t = 21, avg_loss = 0.2545\n",
            "t = 22, avg_loss = 0.3579\n",
            "t = 23, avg_loss = 0.4620\n",
            "t = 24, avg_loss = 0.3797\n",
            "t = 25, avg_loss = 0.2778\n",
            "t = 26, avg_loss = 0.3845\n",
            "t = 27, avg_loss = 0.2629\n",
            "t = 28, avg_loss = 0.2552\n",
            "t = 29, avg_loss = 0.2702\n",
            "t = 30, avg_loss = 0.3949\n",
            "Checking accuracy on test set\n",
            "Got 185 / 240 correct (77.08)\n",
            "acc = 0.770833\n",
            "Starting epoch 45 / 50\n",
            "t = 1, avg_loss = 0.5074\n",
            "t = 2, avg_loss = 0.3026\n",
            "t = 3, avg_loss = 0.3398\n",
            "t = 4, avg_loss = 0.3191\n",
            "t = 5, avg_loss = 0.2300\n",
            "t = 6, avg_loss = 0.3409\n",
            "t = 7, avg_loss = 0.2594\n",
            "t = 8, avg_loss = 0.2280\n",
            "t = 9, avg_loss = 0.2861\n",
            "t = 10, avg_loss = 0.4429\n",
            "t = 11, avg_loss = 0.3128\n",
            "t = 12, avg_loss = 0.3250\n",
            "t = 13, avg_loss = 0.6207\n",
            "t = 14, avg_loss = 0.2178\n",
            "t = 15, avg_loss = 0.2935\n",
            "t = 16, avg_loss = 0.4360\n",
            "t = 17, avg_loss = 0.3129\n",
            "t = 18, avg_loss = 0.3524\n",
            "t = 19, avg_loss = 0.2526\n",
            "t = 20, avg_loss = 0.3086\n",
            "t = 21, avg_loss = 0.3269\n",
            "t = 22, avg_loss = 0.4122\n",
            "t = 23, avg_loss = 0.3905\n",
            "t = 24, avg_loss = 0.3197\n",
            "t = 25, avg_loss = 0.4126\n",
            "t = 26, avg_loss = 0.2792\n",
            "t = 27, avg_loss = 0.4245\n",
            "t = 28, avg_loss = 0.1833\n",
            "t = 29, avg_loss = 0.3146\n",
            "t = 30, avg_loss = 0.4272\n",
            "Checking accuracy on test set\n",
            "Got 185 / 240 correct (77.08)\n",
            "acc = 0.770833\n",
            "Starting epoch 46 / 50\n",
            "t = 1, avg_loss = 0.2996\n",
            "t = 2, avg_loss = 0.2940\n",
            "t = 3, avg_loss = 0.3070\n",
            "t = 4, avg_loss = 0.3437\n",
            "t = 5, avg_loss = 0.3101\n",
            "t = 6, avg_loss = 0.3287\n",
            "t = 7, avg_loss = 0.4699\n",
            "t = 8, avg_loss = 0.2605\n",
            "t = 9, avg_loss = 0.3226\n",
            "t = 10, avg_loss = 0.3319\n",
            "t = 11, avg_loss = 0.3412\n",
            "t = 12, avg_loss = 0.2651\n",
            "t = 13, avg_loss = 0.3327\n",
            "t = 14, avg_loss = 0.3673\n",
            "t = 15, avg_loss = 0.2639\n",
            "t = 16, avg_loss = 0.2293\n",
            "t = 17, avg_loss = 0.3294\n",
            "t = 18, avg_loss = 0.2989\n",
            "t = 19, avg_loss = 0.2014\n",
            "t = 20, avg_loss = 0.2424\n",
            "t = 21, avg_loss = 0.2814\n",
            "t = 22, avg_loss = 0.3510\n",
            "t = 23, avg_loss = 0.2683\n",
            "t = 24, avg_loss = 0.3249\n",
            "t = 25, avg_loss = 0.4283\n",
            "t = 26, avg_loss = 0.3174\n",
            "t = 27, avg_loss = 0.6502\n",
            "t = 28, avg_loss = 0.3571\n",
            "t = 29, avg_loss = 0.2165\n",
            "t = 30, avg_loss = 0.3087\n",
            "Checking accuracy on test set\n",
            "Got 153 / 240 correct (63.75)\n",
            "acc = 0.637500\n",
            "Starting epoch 47 / 50\n",
            "t = 1, avg_loss = 0.2472\n",
            "t = 2, avg_loss = 0.1597\n",
            "t = 3, avg_loss = 0.3628\n",
            "t = 4, avg_loss = 0.3284\n",
            "t = 5, avg_loss = 0.2156\n",
            "t = 6, avg_loss = 0.3515\n",
            "t = 7, avg_loss = 0.3223\n",
            "t = 8, avg_loss = 0.3191\n",
            "t = 9, avg_loss = 0.3978\n",
            "t = 10, avg_loss = 0.3204\n",
            "t = 11, avg_loss = 0.2778\n",
            "t = 12, avg_loss = 0.3345\n",
            "t = 13, avg_loss = 0.2721\n",
            "t = 14, avg_loss = 0.4533\n",
            "t = 15, avg_loss = 0.2766\n",
            "t = 16, avg_loss = 0.3718\n",
            "t = 17, avg_loss = 0.3081\n",
            "t = 18, avg_loss = 0.2567\n",
            "t = 19, avg_loss = 0.4216\n",
            "t = 20, avg_loss = 0.2626\n",
            "t = 21, avg_loss = 0.2521\n",
            "t = 22, avg_loss = 0.2858\n",
            "t = 23, avg_loss = 0.4895\n",
            "t = 24, avg_loss = 0.4025\n",
            "t = 25, avg_loss = 0.4863\n",
            "t = 26, avg_loss = 0.3307\n",
            "t = 27, avg_loss = 0.3293\n",
            "t = 28, avg_loss = 0.2874\n",
            "t = 29, avg_loss = 0.3159\n",
            "t = 30, avg_loss = 0.2976\n",
            "Checking accuracy on test set\n",
            "Got 184 / 240 correct (76.67)\n",
            "acc = 0.766667\n",
            "Starting epoch 48 / 50\n",
            "t = 1, avg_loss = 0.3502\n",
            "t = 2, avg_loss = 0.2346\n",
            "t = 3, avg_loss = 0.3373\n",
            "t = 4, avg_loss = 0.3018\n",
            "t = 5, avg_loss = 0.3072\n",
            "t = 6, avg_loss = 0.3592\n",
            "t = 7, avg_loss = 0.1305\n",
            "t = 8, avg_loss = 0.2395\n",
            "t = 9, avg_loss = 0.3477\n",
            "t = 10, avg_loss = 0.5138\n",
            "t = 11, avg_loss = 0.2593\n",
            "t = 12, avg_loss = 0.2392\n",
            "t = 13, avg_loss = 0.3803\n",
            "t = 14, avg_loss = 0.2295\n",
            "t = 15, avg_loss = 0.4558\n",
            "t = 16, avg_loss = 0.2901\n",
            "t = 17, avg_loss = 0.2236\n",
            "t = 18, avg_loss = 0.4787\n",
            "t = 19, avg_loss = 0.4188\n",
            "t = 20, avg_loss = 0.3707\n",
            "t = 21, avg_loss = 0.3955\n",
            "t = 22, avg_loss = 0.1894\n",
            "t = 23, avg_loss = 0.3339\n",
            "t = 24, avg_loss = 0.3047\n",
            "t = 25, avg_loss = 0.3525\n",
            "t = 26, avg_loss = 0.2267\n",
            "t = 27, avg_loss = 0.4441\n",
            "t = 28, avg_loss = 0.4372\n",
            "t = 29, avg_loss = 0.2289\n",
            "t = 30, avg_loss = 0.4144\n",
            "Checking accuracy on test set\n",
            "Got 166 / 240 correct (69.17)\n",
            "acc = 0.691667\n",
            "Starting epoch 49 / 50\n",
            "t = 1, avg_loss = 0.2495\n",
            "t = 2, avg_loss = 0.2848\n",
            "t = 3, avg_loss = 0.3070\n",
            "t = 4, avg_loss = 0.3081\n",
            "t = 5, avg_loss = 0.3342\n",
            "t = 6, avg_loss = 0.2679\n",
            "t = 7, avg_loss = 0.2395\n",
            "t = 8, avg_loss = 0.3341\n",
            "t = 9, avg_loss = 0.4023\n",
            "t = 10, avg_loss = 0.4351\n",
            "t = 11, avg_loss = 0.2909\n",
            "t = 12, avg_loss = 0.4537\n",
            "t = 13, avg_loss = 0.3939\n",
            "t = 14, avg_loss = 0.3600\n",
            "t = 15, avg_loss = 0.2897\n",
            "t = 16, avg_loss = 0.2772\n",
            "t = 17, avg_loss = 0.3135\n",
            "t = 18, avg_loss = 0.3404\n",
            "t = 19, avg_loss = 0.4045\n",
            "t = 20, avg_loss = 0.2031\n",
            "t = 21, avg_loss = 0.2831\n",
            "t = 22, avg_loss = 0.4286\n",
            "t = 23, avg_loss = 0.3136\n",
            "t = 24, avg_loss = 0.2111\n",
            "t = 25, avg_loss = 0.3224\n",
            "t = 26, avg_loss = 0.5554\n",
            "t = 27, avg_loss = 0.2865\n",
            "t = 28, avg_loss = 0.2688\n",
            "t = 29, avg_loss = 0.2889\n",
            "t = 30, avg_loss = 0.3118\n",
            "Checking accuracy on test set\n",
            "Got 171 / 240 correct (71.25)\n",
            "acc = 0.712500\n",
            "Starting epoch 50 / 50\n",
            "t = 1, avg_loss = 0.1305\n",
            "t = 2, avg_loss = 0.3682\n",
            "t = 3, avg_loss = 0.2836\n",
            "t = 4, avg_loss = 0.1393\n",
            "t = 5, avg_loss = 0.3324\n",
            "t = 6, avg_loss = 0.5756\n",
            "t = 7, avg_loss = 0.2845\n",
            "t = 8, avg_loss = 0.2458\n",
            "t = 9, avg_loss = 0.2328\n",
            "t = 10, avg_loss = 0.4796\n",
            "t = 11, avg_loss = 0.2306\n",
            "t = 12, avg_loss = 0.3319\n",
            "t = 13, avg_loss = 0.2145\n",
            "t = 14, avg_loss = 0.2332\n",
            "t = 15, avg_loss = 0.1715\n",
            "t = 16, avg_loss = 0.2237\n",
            "t = 17, avg_loss = 0.3658\n",
            "t = 18, avg_loss = 0.3292\n",
            "t = 19, avg_loss = 0.4560\n",
            "t = 20, avg_loss = 0.3532\n",
            "t = 21, avg_loss = 0.3498\n",
            "t = 22, avg_loss = 0.2441\n",
            "t = 23, avg_loss = 0.3151\n",
            "t = 24, avg_loss = 0.2407\n",
            "t = 25, avg_loss = 0.1813\n",
            "t = 26, avg_loss = 0.3896\n",
            "t = 27, avg_loss = 0.4863\n",
            "t = 28, avg_loss = 0.4462\n",
            "t = 29, avg_loss = 0.2719\n",
            "t = 30, avg_loss = 0.5984\n",
            "Checking accuracy on test set\n",
            "Got 193 / 240 correct (80.42)\n",
            "acc = 0.804167\n",
            "Checking accuracy on test set\n",
            "Got 188 / 240 correct (78.33)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7833333333333333"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDn-2g-TLY3k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "2ee533fd-46f1-4a5f-cbe8-7441fde23c2c"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot([print_every*batch_size*(i+1) for i in range((len(avg_loss_list)))],avg_loss_list)\n",
        "print(\"Loss:\")\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFTrDMExEYIXOMAwTEf4/TfACuu36Em4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGer6hJv7-zR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "a73ea5bd-2662-4eee-dd13-29c069fb8dca"
      },
      "source": [
        "plt.plot([i+1 for i in range((len(acc_list)))],acc_list)\n",
        "print(\"Accurancy:\")\n",
        "plt.show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accurancy:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTbjhm5nVxH9s+yNkC+BXPAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBTReWHxz4Ml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}