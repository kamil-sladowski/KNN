{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of KNN_Avg_No_Relu2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kamilo116/KNN/blob/master/Copy_of_KNN_Avg_No_Relu2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7ReM7FU-xs4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import os\n",
        "import csv\n",
        "import sys\n",
        "from random import shuffle\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from matplotlib.pyplot import imshow\n",
        "%matplotlib inline\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import sampler\n",
        "import torchvision\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms as transforms\n",
        "import timeit\n",
        "\n",
        "np.random.seed(4) \n",
        "torch.manual_seed(4) \n",
        "torch.cuda.manual_seed(4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYIMZ8QyYewD",
        "colab_type": "code",
        "outputId": "d7bee2f7-1de6-4406-8503-5ea9e0505988",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "! git clone https://github.com/wang-chen/kervolution.git \n",
        "\n",
        "sys.path.append(\"kervolution/\")\n",
        "from kervolution import Kerv2d\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'kervolution'...\n",
            "remote: Enumerating objects: 6, done.\u001b[K\n",
            "remote: Counting objects:  16% (1/6)\u001b[K\rremote: Counting objects:  33% (2/6)\u001b[K\rremote: Counting objects:  50% (3/6)\u001b[K\rremote: Counting objects:  66% (4/6)\u001b[K\rremote: Counting objects:  83% (5/6)\u001b[K\rremote: Counting objects: 100% (6/6)\u001b[K\rremote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects:  16% (1/6)\u001b[K\rremote: Compressing objects:  33% (2/6)\u001b[K\rremote: Compressing objects:  50% (3/6)\u001b[K\rremote: Compressing objects:  66% (4/6)\u001b[K\rremote: Compressing objects:  83% (5/6)\u001b[K\rremote: Compressing objects: 100% (6/6)\u001b[K\rremote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 53 (delta 2), reused 0 (delta 0), pack-reused 47\u001b[K\n",
            "Unpacking objects:   1% (1/53)   \rUnpacking objects:   3% (2/53)   \rUnpacking objects:   5% (3/53)   \rUnpacking objects:   7% (4/53)   \rUnpacking objects:   9% (5/53)   \rUnpacking objects:  11% (6/53)   \rUnpacking objects:  13% (7/53)   \rUnpacking objects:  15% (8/53)   \rUnpacking objects:  16% (9/53)   \rUnpacking objects:  18% (10/53)   \rUnpacking objects:  20% (11/53)   \rUnpacking objects:  22% (12/53)   \rUnpacking objects:  24% (13/53)   \rUnpacking objects:  26% (14/53)   \rUnpacking objects:  28% (15/53)   \rUnpacking objects:  30% (16/53)   \rUnpacking objects:  32% (17/53)   \rUnpacking objects:  33% (18/53)   \rUnpacking objects:  35% (19/53)   \rUnpacking objects:  37% (20/53)   \rUnpacking objects:  39% (21/53)   \rUnpacking objects:  41% (22/53)   \rUnpacking objects:  43% (23/53)   \rUnpacking objects:  45% (24/53)   \rUnpacking objects:  47% (25/53)   \rUnpacking objects:  49% (26/53)   \rUnpacking objects:  50% (27/53)   \rUnpacking objects:  52% (28/53)   \rUnpacking objects:  54% (29/53)   \rUnpacking objects:  56% (30/53)   \rUnpacking objects:  58% (31/53)   \rUnpacking objects:  60% (32/53)   \rUnpacking objects:  62% (33/53)   \rUnpacking objects:  64% (34/53)   \rUnpacking objects:  66% (35/53)   \rUnpacking objects:  67% (36/53)   \rUnpacking objects:  69% (37/53)   \rUnpacking objects:  71% (38/53)   \rUnpacking objects:  73% (39/53)   \rUnpacking objects:  75% (40/53)   \rUnpacking objects:  77% (41/53)   \rUnpacking objects:  79% (42/53)   \rUnpacking objects:  81% (43/53)   \rUnpacking objects:  83% (44/53)   \rUnpacking objects:  84% (45/53)   \rUnpacking objects:  86% (46/53)   \rUnpacking objects:  88% (47/53)   \rUnpacking objects:  90% (48/53)   \rUnpacking objects:  92% (49/53)   \rUnpacking objects:  94% (50/53)   \rUnpacking objects:  96% (51/53)   \rUnpacking objects:  98% (52/53)   \rUnpacking objects: 100% (53/53)   \rUnpacking objects: 100% (53/53), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvbKg4VJXKY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LIMIT_IMAGES_NUM = 500\n",
        "train_test_split_size = 0.2\n",
        "image_resize = (100, 100)\n",
        "batch_size = 64\n",
        "num_workers = 0\n",
        "\n",
        "in_channels = 3\n",
        "out_channels_1 = 16\n",
        "out_channels_2 = 32\n",
        "out_channels_3 = 64\n",
        "out_channels_4 = 128\n",
        "\n",
        "kernel_size = 3\n",
        "padding_1 = 1\n",
        "num_epochs = 80\n",
        "\n",
        "learning_rate = 0.001\n",
        "weight_decay = 0.1\n",
        "\n",
        "MALIGNANT_DATASET = '/content/drive/My Drive/Colab_data/malignant/malignant/'\n",
        "BENIGN_DATASET = '/content/drive/My Drive/Colab_data/benign/benign/'\n",
        "DATA_FOLDER = '/content/drive/My Drive/Colab_data/'\n",
        "model_backup_path = os.path.join(DATA_FOLDER, 'backup_model') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMOHez_kHZD6",
        "colab_type": "code",
        "outputId": "717188c7-ccc0-4267-c56c-e66fa83ec6ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive, files\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b8P1NdXfVdX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "690080ae-f650-4a21-9948-31cf3b612051"
      },
      "source": [
        "benign_file_list = sorted(os.listdir(BENIGN_DATASET))\n",
        "malignant_file_list = sorted(os.listdir(MALIGNANT_DATASET))\n",
        "shuffle(benign_file_list)\n",
        "shuffle(malignant_file_list)\n",
        "benign_file_list = benign_file_list[:LIMIT_IMAGES_NUM]\n",
        "malignant_file_list = malignant_file_list[:LIMIT_IMAGES_NUM]\n",
        "\n",
        "print(f\"Number of benign {len(benign_file_list)} images\")\n",
        "print(f\"Number of malignant {len(malignant_file_list)} images\")\n",
        "\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize(image_resize),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of benign 500 images\n",
            "Number of malignant 500 images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGDQfal74BLi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "benign_dict = {filename: 0 for filename in benign_file_list}\n",
        "malignant_dict = {filename: 1 for filename in malignant_file_list}\n",
        "img_class_dict = {**benign_dict , **malignant_dict}\n",
        "labeled_data = pd.Series(img_class_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvAAOqmVfVll",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class IsicDataset(Dataset):\n",
        "    def __init__(self, data_folder, labeled_data, \n",
        "                 transform=transforms.Compose([transforms.ToTensor()])):\n",
        "        self.labeled_data = labeled_data\n",
        "        self.transform = transform\n",
        "        self.data_folder = data_folder\n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.labeled_data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        label = self.labeled_data[index]\n",
        "        if label == 0:\n",
        "          image = Image.open(os.path.join(self.data_folder, \"benign\", \"benign\", index ))\n",
        "        else:\n",
        "          image = Image.open(os.path.join(self.data_folder, \"malignant\", \"malignant\", index ))\n",
        "        image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "    @property\n",
        "    def labels(self):\n",
        "      return self.labeled_data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSuYqjLAYrWA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "960633fe-50be-4998-bf9c-cd769c3ab4ec"
      },
      "source": [
        " \n",
        "dataset = IsicDataset(DATA_FOLDER, labeled_data, transform=data_transforms)\n",
        "print(dataset.labels)\n",
        "\n",
        "X_train, X_test = train_test_split(dataset.labels, test_size=train_test_split_size)\n",
        "print(\"number of training data: \",len(X_train))\n",
        "print(\"number of testing  data: \",len(X_test))\n",
        "\n",
        "train_sampler = SubsetRandomSampler(list(X_train.index))\n",
        "valid_sampler = SubsetRandomSampler(list(X_test.index))\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers)\n",
        "valid_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler, num_workers=num_workers)\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ISIC_0000093.jpeg    0\n",
            "ISIC_0001833.jpeg    0\n",
            "ISIC_0000921.jpeg    0\n",
            "ISIC_0001324.jpeg    0\n",
            "ISIC_0001300.jpeg    0\n",
            "                    ..\n",
            "ISIC_0011520.jpeg    1\n",
            "ISIC_0000553.jpeg    1\n",
            "ISIC_0011121.jpeg    1\n",
            "ISIC_0010834.jpeg    1\n",
            "ISIC_0010629.jpeg    1\n",
            "Length: 1000, dtype: int64\n",
            "number of training data:  800\n",
            "number of testing  data:  200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_e8nFBR6Yug5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "avg_loss_list = []\n",
        "acc_list = []\n",
        "\n",
        "def train(model, train_loader ,loss_fn, optimizer, num_epochs=1, starting_from_epoch=1):\n",
        "    total_loss =0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Starting epoch %d / %d' % (epoch + 1, num_epochs))\n",
        "        model.train()\n",
        "\n",
        "        for t, (x, y) in enumerate(train_loader):\n",
        "            x_var = Variable(x.type(gpu_dtype))\n",
        "            y_var = Variable(y.type(gpu_dtype).long())\n",
        "            scores = model(x_var)\n",
        "            loss = loss_fn(scores, y_var)\n",
        "            total_loss += loss.data\n",
        "            \n",
        "            if (t + 1) % print_every == 0:\n",
        "                avg_loss = total_loss/print_every\n",
        "                print('t = %d, avg_loss = %.4f' % (t + 1, avg_loss) )\n",
        "                avg_loss_list.append(avg_loss)\n",
        "                total_loss = 0\n",
        "                \n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        acc = check_accuracy(model_gpu, valid_loader)\n",
        "        print('acc = %f' %(acc))\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model_gpu.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            }, model_backup_path)\n",
        "            \n",
        "def check_accuracy(model, loader):\n",
        "    print('Checking accuracy on test set')   \n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval() \n",
        "    for x, y in loader:\n",
        "        x_var = Variable(x.type(gpu_dtype))\n",
        "\n",
        "        scores = model(x_var)\n",
        "        _, preds = scores.data.cpu().max(1)\n",
        "        num_correct += (preds == y).sum()\n",
        "        num_samples += preds.size(0)\n",
        "    acc = float(num_correct) / num_samples\n",
        "    acc_list.append(acc)\n",
        "    print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
        "    return acc\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggvpMu36Yw2D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Flatten(nn.Module):\n",
        "    def forward(self, x):\n",
        "        N, C, H, W = x.size()\n",
        "        return x.view(N, -1)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgmexhBRZAK6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "141f6714-9d29-4b76-ea2d-561737a29d82"
      },
      "source": [
        "gpu_dtype = torch.cuda.FloatTensor\n",
        "\n",
        "print_every = 1\n",
        "\n",
        "\n",
        "'''\n",
        "Kerv2d\n",
        "kervolution with following options:\n",
        "kernel_type: [linear, polynomial, gaussian, etc.]\n",
        "default is convolution:\n",
        "          kernel_type --> linear,\n",
        "balance, power, gamma is valid only when the kernel_type is specified\n",
        "if learnable_kernel = True,  they just be the initial value of learable parameters\n",
        "if learnable_kernel = False, they are the value of kernel_type's parameter\n",
        "the parameter [power] cannot be learned due to integer limitation\n",
        "dilation (int or tuple, optional): Spacing between kernel\n",
        "elements. Default: 1\n",
        "groups (int, optional): Number of blocked connections from input\n",
        "channels to output channels. Default: 1\n",
        "bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``\n",
        "kernel_type (str), Default: 'linear'\n",
        "learnable_kernel (bool): Learnable kernel parameters.  Default: False \n",
        "balance: 0, 1\n",
        "power: 3, 4, 5\n",
        "gamma:\n",
        "'''\n",
        "\n",
        "\n",
        "model_base = nn.Sequential( \n",
        "                nn.Kerv2d(in_channels , out_channels_1, padding=padding_1, dilation=1, groups=1, bias=True, \n",
        "                          kernel_type='polynomial', kernel_size=kernel_size, learnable_kernel=True,\n",
        "                          kernel_regularizer=True, stride=1, balance=1, power=3, gamma=1\n",
        "                          ),\n",
        "                nn.BatchNorm2d(out_channels_1),\n",
        "                nn.AvgPool2d(2, stride=2),\n",
        "                nn.Conv2d(out_channels_1 , out_channels_2, padding= padding_1, kernel_size=kernel_size, stride=1), \n",
        "                nn.BatchNorm2d(out_channels_2),\n",
        "                nn.AvgPool2d(2, stride=2),\n",
        "                nn.Conv2d(out_channels_2 , out_channels_3, padding= padding_1, kernel_size=kernel_size, stride=1),  \n",
        "                nn.BatchNorm2d(out_channels_3),\n",
        "                nn.AvgPool2d(2, stride=2),\n",
        "                nn.Conv2d(out_channels_3 , out_channels_4, padding= padding_1, kernel_size=kernel_size, stride=1),  \n",
        "                nn.BatchNorm2d(out_channels_4),\n",
        "                nn.AvgPool2d(2, stride=2),\n",
        "                nn.Dropout(0.5),\n",
        "                Flatten(),\n",
        "                nn.Linear(4608,64),\n",
        "                nn.Linear(64,2)\n",
        "            )\n",
        "model_gpu = model_base.type(gpu_dtype)\n",
        "print(model_gpu)\n",
        "loss_fn = nn.modules.loss.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_gpu.parameters(), lr = learning_rate, weight_decay=weight_decay) \n",
        "\n",
        "train(model_gpu, train_loader ,loss_fn, optimizer, num_epochs=num_epochs)\n",
        "check_accuracy(model_gpu, valid_loader)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Kerv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "  (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "  (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (8): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "  (9): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (11): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "  (12): Dropout(p=0.5, inplace=False)\n",
            "  (13): Flatten()\n",
            "  (14): Linear(in_features=4608, out_features=64, bias=True)\n",
            "  (15): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "Starting epoch 1 / 80\n",
            "t = 1, avg_loss = 0.7466\n",
            "t = 2, avg_loss = 1.1396\n",
            "t = 3, avg_loss = 1.4875\n",
            "t = 4, avg_loss = 1.2475\n",
            "t = 5, avg_loss = 0.9391\n",
            "t = 6, avg_loss = 1.1145\n",
            "t = 7, avg_loss = 1.1243\n",
            "t = 8, avg_loss = 0.9142\n",
            "t = 9, avg_loss = 1.3462\n",
            "t = 10, avg_loss = 0.8831\n",
            "t = 11, avg_loss = 0.8528\n",
            "t = 12, avg_loss = 0.9705\n",
            "t = 13, avg_loss = 0.8377\n",
            "Checking accuracy on test set\n",
            "Got 126 / 200 correct (63.00)\n",
            "acc = 0.630000\n",
            "Starting epoch 2 / 80\n",
            "t = 1, avg_loss = 0.9682\n",
            "t = 2, avg_loss = 0.8517\n",
            "t = 3, avg_loss = 0.9360\n",
            "t = 4, avg_loss = 1.0894\n",
            "t = 5, avg_loss = 1.4650\n",
            "t = 6, avg_loss = 0.8477\n",
            "t = 7, avg_loss = 0.6585\n",
            "t = 8, avg_loss = 1.0963\n",
            "t = 9, avg_loss = 1.3018\n",
            "t = 10, avg_loss = 1.0252\n",
            "t = 11, avg_loss = 0.8328\n",
            "t = 12, avg_loss = 0.9209\n",
            "t = 13, avg_loss = 0.7760\n",
            "Checking accuracy on test set\n",
            "Got 106 / 200 correct (53.00)\n",
            "acc = 0.530000\n",
            "Starting epoch 3 / 80\n",
            "t = 1, avg_loss = 1.1275\n",
            "t = 2, avg_loss = 0.9103\n",
            "t = 3, avg_loss = 0.7487\n",
            "t = 4, avg_loss = 0.9889\n",
            "t = 5, avg_loss = 0.7832\n",
            "t = 6, avg_loss = 0.7999\n",
            "t = 7, avg_loss = 0.7347\n",
            "t = 8, avg_loss = 0.8077\n",
            "t = 9, avg_loss = 0.7716\n",
            "t = 10, avg_loss = 0.6322\n",
            "t = 11, avg_loss = 0.7150\n",
            "t = 12, avg_loss = 0.7474\n",
            "t = 13, avg_loss = 0.7954\n",
            "Checking accuracy on test set\n",
            "Got 143 / 200 correct (71.50)\n",
            "acc = 0.715000\n",
            "Starting epoch 4 / 80\n",
            "t = 1, avg_loss = 0.6457\n",
            "t = 2, avg_loss = 0.6630\n",
            "t = 3, avg_loss = 0.9249\n",
            "t = 4, avg_loss = 0.7443\n",
            "t = 5, avg_loss = 0.6748\n",
            "t = 6, avg_loss = 0.6335\n",
            "t = 7, avg_loss = 0.6491\n",
            "t = 8, avg_loss = 0.7101\n",
            "t = 9, avg_loss = 0.7043\n",
            "t = 10, avg_loss = 0.6883\n",
            "t = 11, avg_loss = 0.6404\n",
            "t = 12, avg_loss = 0.6509\n",
            "t = 13, avg_loss = 0.8975\n",
            "Checking accuracy on test set\n",
            "Got 134 / 200 correct (67.00)\n",
            "acc = 0.670000\n",
            "Starting epoch 5 / 80\n",
            "t = 1, avg_loss = 0.8242\n",
            "t = 2, avg_loss = 0.5849\n",
            "t = 3, avg_loss = 0.6447\n",
            "t = 4, avg_loss = 0.6379\n",
            "t = 5, avg_loss = 0.6715\n",
            "t = 6, avg_loss = 0.6318\n",
            "t = 7, avg_loss = 0.6829\n",
            "t = 8, avg_loss = 0.6091\n",
            "t = 9, avg_loss = 0.7549\n",
            "t = 10, avg_loss = 0.6424\n",
            "t = 11, avg_loss = 0.6435\n",
            "t = 12, avg_loss = 0.6653\n",
            "t = 13, avg_loss = 0.5924\n",
            "Checking accuracy on test set\n",
            "Got 130 / 200 correct (65.00)\n",
            "acc = 0.650000\n",
            "Starting epoch 6 / 80\n",
            "t = 1, avg_loss = 0.7000\n",
            "t = 2, avg_loss = 0.6343\n",
            "t = 3, avg_loss = 0.7626\n",
            "t = 4, avg_loss = 0.7675\n",
            "t = 5, avg_loss = 0.6251\n",
            "t = 6, avg_loss = 0.7166\n",
            "t = 7, avg_loss = 0.6016\n",
            "t = 8, avg_loss = 0.6728\n",
            "t = 9, avg_loss = 0.6169\n",
            "t = 10, avg_loss = 0.7515\n",
            "t = 11, avg_loss = 0.6631\n",
            "t = 12, avg_loss = 0.6422\n",
            "t = 13, avg_loss = 0.7281\n",
            "Checking accuracy on test set\n",
            "Got 130 / 200 correct (65.00)\n",
            "acc = 0.650000\n",
            "Starting epoch 7 / 80\n",
            "t = 1, avg_loss = 0.6762\n",
            "t = 2, avg_loss = 0.6403\n",
            "t = 3, avg_loss = 0.6267\n",
            "t = 4, avg_loss = 0.7851\n",
            "t = 5, avg_loss = 0.7431\n",
            "t = 6, avg_loss = 0.6556\n",
            "t = 7, avg_loss = 0.6828\n",
            "t = 8, avg_loss = 0.6542\n",
            "t = 9, avg_loss = 0.6589\n",
            "t = 10, avg_loss = 0.6354\n",
            "t = 11, avg_loss = 0.7146\n",
            "t = 12, avg_loss = 0.6123\n",
            "t = 13, avg_loss = 0.5198\n",
            "Checking accuracy on test set\n",
            "Got 134 / 200 correct (67.00)\n",
            "acc = 0.670000\n",
            "Starting epoch 8 / 80\n",
            "t = 1, avg_loss = 0.6506\n",
            "t = 2, avg_loss = 0.6574\n",
            "t = 3, avg_loss = 0.6490\n",
            "t = 4, avg_loss = 0.5345\n",
            "t = 5, avg_loss = 0.5591\n",
            "t = 6, avg_loss = 0.5918\n",
            "t = 7, avg_loss = 0.5518\n",
            "t = 8, avg_loss = 0.7572\n",
            "t = 9, avg_loss = 0.5409\n",
            "t = 10, avg_loss = 0.6322\n",
            "t = 11, avg_loss = 0.7384\n",
            "t = 12, avg_loss = 0.6328\n",
            "t = 13, avg_loss = 0.7098\n",
            "Checking accuracy on test set\n",
            "Got 114 / 200 correct (57.00)\n",
            "acc = 0.570000\n",
            "Starting epoch 9 / 80\n",
            "t = 1, avg_loss = 0.5605\n",
            "t = 2, avg_loss = 0.5913\n",
            "t = 3, avg_loss = 0.6645\n",
            "t = 4, avg_loss = 0.5993\n",
            "t = 5, avg_loss = 0.5960\n",
            "t = 6, avg_loss = 0.6059\n",
            "t = 7, avg_loss = 0.5886\n",
            "t = 8, avg_loss = 0.5869\n",
            "t = 9, avg_loss = 0.6547\n",
            "t = 10, avg_loss = 0.7366\n",
            "t = 11, avg_loss = 0.7439\n",
            "t = 12, avg_loss = 0.6713\n",
            "t = 13, avg_loss = 0.5815\n",
            "Checking accuracy on test set\n",
            "Got 120 / 200 correct (60.00)\n",
            "acc = 0.600000\n",
            "Starting epoch 10 / 80\n",
            "t = 1, avg_loss = 0.6438\n",
            "t = 2, avg_loss = 0.6586\n",
            "t = 3, avg_loss = 0.6834\n",
            "t = 4, avg_loss = 0.5765\n",
            "t = 5, avg_loss = 0.6297\n",
            "t = 6, avg_loss = 0.6529\n",
            "t = 7, avg_loss = 0.6844\n",
            "t = 8, avg_loss = 0.5747\n",
            "t = 9, avg_loss = 0.6148\n",
            "t = 10, avg_loss = 0.5732\n",
            "t = 11, avg_loss = 0.6479\n",
            "t = 12, avg_loss = 0.6368\n",
            "t = 13, avg_loss = 0.5931\n",
            "Checking accuracy on test set\n",
            "Got 123 / 200 correct (61.50)\n",
            "acc = 0.615000\n",
            "Starting epoch 11 / 80\n",
            "t = 1, avg_loss = 0.5785\n",
            "t = 2, avg_loss = 0.6499\n",
            "t = 3, avg_loss = 0.5313\n",
            "t = 4, avg_loss = 0.6469\n",
            "t = 5, avg_loss = 0.6577\n",
            "t = 6, avg_loss = 0.6787\n",
            "t = 7, avg_loss = 0.5978\n",
            "t = 8, avg_loss = 0.6189\n",
            "t = 9, avg_loss = 0.6310\n",
            "t = 10, avg_loss = 0.6670\n",
            "t = 11, avg_loss = 0.6546\n",
            "t = 12, avg_loss = 0.5519\n",
            "t = 13, avg_loss = 0.6537\n",
            "Checking accuracy on test set\n",
            "Got 133 / 200 correct (66.50)\n",
            "acc = 0.665000\n",
            "Starting epoch 12 / 80\n",
            "t = 1, avg_loss = 0.6648\n",
            "t = 2, avg_loss = 0.6506\n",
            "t = 3, avg_loss = 0.5794\n",
            "t = 4, avg_loss = 0.6296\n",
            "t = 5, avg_loss = 0.5816\n",
            "t = 6, avg_loss = 0.6723\n",
            "t = 7, avg_loss = 0.6543\n",
            "t = 8, avg_loss = 0.6545\n",
            "t = 9, avg_loss = 0.6958\n",
            "t = 10, avg_loss = 0.6420\n",
            "t = 11, avg_loss = 0.6252\n",
            "t = 12, avg_loss = 0.6658\n",
            "t = 13, avg_loss = 0.6001\n",
            "Checking accuracy on test set\n",
            "Got 139 / 200 correct (69.50)\n",
            "acc = 0.695000\n",
            "Starting epoch 13 / 80\n",
            "t = 1, avg_loss = 0.5966\n",
            "t = 2, avg_loss = 0.6295\n",
            "t = 3, avg_loss = 0.6483\n",
            "t = 4, avg_loss = 0.5772\n",
            "t = 5, avg_loss = 0.6099\n",
            "t = 6, avg_loss = 0.6462\n",
            "t = 7, avg_loss = 0.6599\n",
            "t = 8, avg_loss = 0.6484\n",
            "t = 9, avg_loss = 0.6131\n",
            "t = 10, avg_loss = 0.7072\n",
            "t = 11, avg_loss = 0.6264\n",
            "t = 12, avg_loss = 0.6676\n",
            "t = 13, avg_loss = 0.5614\n",
            "Checking accuracy on test set\n",
            "Got 139 / 200 correct (69.50)\n",
            "acc = 0.695000\n",
            "Starting epoch 14 / 80\n",
            "t = 1, avg_loss = 0.7406\n",
            "t = 2, avg_loss = 0.6926\n",
            "t = 3, avg_loss = 0.6092\n",
            "t = 4, avg_loss = 0.6356\n",
            "t = 5, avg_loss = 0.5880\n",
            "t = 6, avg_loss = 0.6632\n",
            "t = 7, avg_loss = 0.6240\n",
            "t = 8, avg_loss = 0.5817\n",
            "t = 9, avg_loss = 0.6629\n",
            "t = 10, avg_loss = 0.6486\n",
            "t = 11, avg_loss = 0.6730\n",
            "t = 12, avg_loss = 0.5924\n",
            "t = 13, avg_loss = 0.7057\n",
            "Checking accuracy on test set\n",
            "Got 142 / 200 correct (71.00)\n",
            "acc = 0.710000\n",
            "Starting epoch 15 / 80\n",
            "t = 1, avg_loss = 0.6194\n",
            "t = 2, avg_loss = 0.6436\n",
            "t = 3, avg_loss = 0.6470\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzuX5gLEeJfE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def retry_from_backup()\n",
        "  model_gpu = model_base.type(gpu_dtype)\n",
        "  print(model_gpu)\n",
        "  loss_fn = nn.modules.loss.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(model_gpu.parameters(), lr = learning_rate, weight_decay=weight_decay) \n",
        "\n",
        "\n",
        "\n",
        "  checkpoint = torch.load(model_backup_path)\n",
        "  model_gpu.load_state_dict(checkpoint['model_state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "  epoch = checkpoint['epoch']\n",
        "  loss = checkpoint['loss']\n",
        "  print(\"starting from epoch: \" + str(epoch))\n",
        "  print(\"starting from loss: \" + str(loss))\n",
        "  print(checkpoint['model_state_dict'])\n",
        "  print(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "  train(model_gpu, train_loader ,loss_fn, optimizer, num_epochs=num_epochs, starting_from_epoch=epoch)\n",
        "  check_accuracy(model_gpu, valid_loader)\n",
        "#retry_from_backup()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDn-2g-TLY3k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot([print_every*batch_size*(i+1) for i in range((len(avg_loss_list)))],avg_loss_list)\n",
        "print(\"Loss:\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGer6hJv7-zR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot([i+1 for i in range((len(acc_list)))],acc_list)\n",
        "print(\"Accurancy:\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBTReWHxz4Ml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%tensorboard --logdir {logs_base_dir}"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}